The Convention on Certain Conventional Weapons (CCW) Informal Meeting of Experts on Lethal Autonomous Weapons Systems U.S. Delegation Opening Statement As Delivered by Michael W. Meier Geneva, April 13, 2015 Thank you Mr. Chairman. First, the United States Delegation would like to congratulate you on your assumption of the chairmanship of this meeting and we can assure you of our full support. We would also like to express our appreciation for the work you have done preparing us for this important informal meeting, especially with your “Food for Thought” paper, on the challenges associated with lethal autonomous weapon systems; we look forward to productive discussions this week that build on the substantive discussions held last year under the able leadership of France. The U.S. delegation is prepared to participate fully in this week’s discussions and while we will make specific comments during the upcoming sessions, we want to take this opportunity to provide some initial thoughts about the work ahead of us. First, we remain in the beginning stages of this important discussion. The first informal meeting of experts on LAWS held last May provided a useful forum for discussing the various technical, legal, operational and ethical issues surrounding increased autonomy in weapon systems. However, it remains clear from that meeting and the subsequent discussions in which the United States has participated along with other states, civil society, scientists, roboticists, lawyers, and ethicists, that more work still needs to be done to establish a common understanding of lethal autonomous weapons systems. We also want to be clear that we are here to talk about future weapons, or in the words of our mandate, “emerging technologies.” Therefore, we are not referring to remotely piloted aircraft, which as their name implies are not autonomous weapons, or other existing weapons systems. We believe our discussion here in CCW, a forum focused on international humanitarian law, remains the relevant framework for this discussion. Second, we expect this week’s discussion to deepen our understanding of the complex issues surrounding LAWS as our program of work will allow us to more fully explore this topic. That said, we believe that it is important to focus on increasing our understanding versus trying to decide possible outcomes. It remains our view that it is premature to try and determine where these discussions might or should lead. Finally, as we highlighted last year, the United States believes that a robust policy process and methodology can help mitigate risk when developing new weapon systems. The United States has a process in place, applicable to all weapon systems, which is designed to ensure weapons operate safely, reliably and are understood by their human operators. Throughout the week the United States intends to elaborate on the review processes and standards we utilize to ensure these standards are met. The United States has established, through our Department of Defense Directive 3000.09, an additional framework for how the United States would consider proposals to develop lethal autonomous weapon systems. We would like to make clear that the Directive does not establish a U.S. position on the potential future development of lethal autonomous weapon systems – it neither encourages nor prohibits the development of such future systems. The framework establishes a deliberative approval process by senior officials, sets out the technical criteria that would need to be satisfied in order to develop autonomous weapon systems, and then assigns responsibility within our Defense Department for overseeing the development of autonomous weapons systems. The Directive imposes additional requirements beyond what is normally required during our weapons acquisition process. These additional requirements are designed to minimize the probability and consequences of failure in autonomous and semi-autonomous weapons systems that could lead to unintended engagements and ensure appropriate levels of human judgment over the use of force. Mr. Chairman, as we have said previously, issues surrounding LAWS are complex. We look forward to sharing our thoughts in what we hope is a robust discussion this week, but more importantly we are looking forward to hearing and learning from other delegations as well as the important contributions on this topic that civil society provides us here in CCW. Thank you, Mr. Chairman. The convention on certain conventional weapons (ccw) informal meeting of experts on lethal autonomous weapons systems u s. Delegation opening statement (checlt against delivery) by michael w. Meier geneva, april 11, 2016 mr. Coordinator, the united states looks forward to continuing our discussions of lethal autonomous weapons systems (laws). We appreciate your efforts to focus our world this week on key issues that will continue helping us to understand better the complex issues related to laws and to identify recommendations for possible future work. During our discussions over the past two sessions, the united states has acknowledged that the issues surrounding lethal autonomous weapon systems are complex; we can see that others appreciate that fact as well. Despite the complexity of the issues and the relatively brief amount of time that we have to discuss them, the united states believes that we are making progress. We sense that there is a much better understanding of the complex issues raised by weapon systems that use autonomy. Mr. Coordinator, i wish to assure you of our delegation's full support; we are prepared to continue to contribute to the robust discussions about these issues so that our collective understanding can grow further. For our part, the united states has explained our national process, applicable to all weapon systems, which is designed to ensure weapons operate legally, safely, and reliably, and are understood by their human operators. In addition, the united states has established, through department of defense directive 3000.09, autonomy in weapons systems, a process as to how the united states would consider proposals to develop lethal autonomous weapon systems. As we have noted before, the directive does not establish a u.s. Position on the potential future development of laws — it neither encourages nor prohibits the development of future systems. Rather, the directive sets out additional requirements beyond what is normally required during our weapons acquisition process that would be applicable to weapon systems that have certain autonomous functions. During this weelc, we would welcome more states coming forward with their national views and policies about the appropriateness of using of technology to improve the performance of weapon systems. In this way, our discussions could significantly develop and mature. We also continue to welcome contributions from civil society and technical experts to inform our discussion. We appreciate that you have provided ample time in the schedule to work toward establishing a common understanding of laws. There remain divergent views in the ccw on what weapon systems we are trying to discuss and with identifying the relevant challenges associated with the potential use of such weapon systems. For the united states, we want to be clear that we are here to talk about future weapons, or in the words of our mandate, “emerging technologies.” remotely piloted aircraft, homing munitions (such as torpedoes), or other existing weapons, including systems that operate in cyberspace and/or operate as defensive systems (such as patriot/aegis), are not “lethal autonomous weapons.” in the course of our discussions on laws, views on what would constitute laws have varied greatly. For example, we have heard some view laws as seemingly synonymous with artificial intelligence, in the sense that they contemplate machines that possess cognitive architectures capable of self-learning. Others' views on what would constitute laws are so broad that they would include existing and more rudimentary computer-enabled weapons, which as we have noted, are not the subject of these discussions. Although the united states does not believe that a definition of laws is required at this stage of our work, we believe a clearer common understanding of the scope of what we mean by laws will help improve the quality of our discussions. The u.s. Delegation also looks forward to a more in depth discussions with respect to human-machine interaction and about the phrase “meaningful human control.” turning first to the phrase “meaningful human control,” we have heard many delegations and experts note that the term is subjective and thus difficult to understand. We have expressed these same concerns about whether “meaningful human control” is a helpful way to advance our discussions. We view the optimization of the human/machine relationship as a primary technical challenge to developing lethal autonomous weapon systems and a key point that needs to be reviewed from the start of any weapon system development. Because this human/machine relationship extends throughout the development and employment of a system and is not limited to the moment of a decision to engage a target, we consider it more useful to talk about “appropriate levels of human judgment.” the united states delegation intends to provide further clarification of our thoughts on this subject during the appropriate session. Finally, we have consistently heard in the ccw interest expressed on the weapons review process and about the requirement to conduct a legal review of all new weapon systems, including laws. We believe that this is an area on which we should focus as an interim step as we continue our consideration of laws in ccw. The united states would like to see the fifth review conference agree to begin worm, as part of the overall mandate on laws, on a non-legally binding outcome document that describes a comprehensive weapons review process, including the policy, technical, legal, and operational best practices that states could consider using if they decide to develop laws or any other weapon system that uses advanced technology. To be clear, the united states believes that the existence of such a document would not endorse the development of laws; it would assist a state in conducting a thorough weapons review if that state is considering developing laws or any new weapon system. It would also help ensure consistency and quality in the weapons review process by all states, regardless of the particular weapon being reviewed. It is also an opportunity for the ccw to take a concrete step related to laws in the near term, even while we continue to develop our common understanding of what constitutes laws. The united states remains very supportive of discussing laws in the ccw. Like we said earlier, we think we are making good, incremental progress, but it is important to continue increasing our understanding about laws rather than trying to decide possible outcomes. We believe that laws is a complex subject that requires in-depth, substantive discussions. Our expectation is that this week's discussion will lead to a better understanding of the various issues related to laws and that will help to identify recommendations for possible future work. Group of governmental experts (gge) on lethal autonomous weapons systems (laws) november 13-17, 2016 opening statement thank you mr. Chairman • the united states welcomes the establishment of this gge. We found the panel presentations informative and look forward to the discussion among states this week. The ccw is uniquely suited to hold these discussions, given its focus on international humanitarian law (ihl) and given that delegations of high contracting parties routinely include members with military, technical, and policy experience. • mr. Chairman we wish to thank you for your efforts to arrange and guide this gge. We believe that this substantive review of the technological, military, and legal/ethical considerations associated with emerging technologies relevant to laws is a valuable contribution to the work of the ccw. Our discussions have provided plenty of food for thought and it is clear that many governments, including that of the united states, are still trying to understand more fully the ways that autonomy will be used by their societies, including by their militaries. These are complex issues, and we need to continue to educate ourselves. • one thing is clear: any development or use of laws must be fully consistent with ihl, including the principles of humanity, distinction, and proportionality. For this reason, the united states places great importance on the weapon review process in the development and acquisition of new weapon systems. This is a critical measure in ensuring that weapon systems can dependably be used in a manner that is consistent with ihl. We continue to believe that best practices for reviewing weapon systems that use autonomy are an especially productive area for continued discussions, as a number of other delegations have also suggested. • the united states also continues to believe that advances in autonomy and machine learning can facilitate and enhance the implementation of ihl, including the principles of distinction and proportionality. One of our goals is thus to understand better how this technology can continue to be used to reduce the risk to civilians and friendly forces in armed conflict. On this issue, we refer other delegations to the united states working paper. • the united states is committed to playing an active and constructive role in this gge, including by sharing our experience in addressing issues related to autonomy in weapon systems. We expect this conversation will greatly contribute to states’ and the public’s understandings of the challenges and benefits that could be presented by laws. We share the frustration expressed by other delegations that a week of meetings was canceled this year. Given the importance of these discussions, the united states would support renewing the current discussion mandate in 2018. • it remains premature, however, to consider where these discussions might or should ultimately lead. For this reason, we do not support the negotiation of a political or legally binding document at this time. The issues presented by laws are complex and evolving, as new technologies and their applications continue to be developed. We must be cautious not to make hasty judgments about the value or likely effects of emerging or future technologies. As history shows, our views of new technologies may change over time as we find new uses and ways to benefit from advances in technology. In particular, we want to encourage innovation and progress in furthering the objects and purposes of the convention. We should therefore proceed with deliberation and patience.
Geneva, 13–17 november 2017 item 6 of the revised provisional agenda examination of various dimensions of emerging technologies in the area of lethal autonomous weapons systems, in the context of the objectives and purposes of the convention autonomy in weapon systems submitted by the united states of america i. Introduction 1. The united states acknowledges both the challenges and opportunities presented by emerging technologies in the area of lethal autonomous weapons systems (laws). As an overall matter, we believe that the law of war (also called international humanitarian law) provides a robust and appropriate framework for the regulation of all weapons in relation to armed conflict. 2. This working paper seeks to contribute to the meetings of the group of governmental experts (gge) on lethal autonomous weapons systems (laws) in geneva between november 13-17, 2017, by providing the views of the united states on: (i) the legal review of weapons with autonomous functions in acquisition or development; (ii) the potential of weapon systems with autonomous functions to improve the implementation of law of war principles in military operations; and (iii) legal accountability regarding weapons with autonomous functions. Ii. Legal review of weapons with autonomous functions in acquisition or development 3. The united states views the review of the legality of weapons as a best practice for implementing customary and treaty law relating to weapons and their use in armed conflict. The united states is not a party to the 1977 additional protocol i to the 1949 geneva conventions and therefore is not bound by that instrument, but we note that article 36 of that protocol creates an obligation for its parties with respect to the study, development, acquisition, or adoption of a "new" weapon, means, or method of warfare. 4. Under united states department of defense (dod) policies, legal reviews are conducted as part of the broader acquisition processes. In particular, a dod policy (dod directive 5000.01) requires that the acquisition and procurement of dod weapons and weapon systems be consistent with all applicable domestic and international law, including the law of war. To implement this requirement, dod directive 5000.01 provides that "a]n attorney authorized to conduct such legal reviews in the department shall conduct the legal review of the intended acquisition of weapons or weapons systems." ccw/gge.1/2017/wp.6 group of governmental experts of the high contracting parties to the convention on prohibitions or restrictions on the use of certain conventional weapons which may be deemed to be excessively injurious or to have indiscriminate effects 10 november 2017 original: english ccw/gge.1/2017/wp.6 2 5. Legal advice on the intended acquisition of weapons ordinarily may be provided at different stages in the process of acquiring such weapons. For example, under dod policy on the use of autonomy in weapon systems (dod directive 3000.09), legal review and advice are provided before formal development and again before fielding with regard to certain types of weapon systems that involve the use of autonomy. 6. For the uniteed states department of defense, the legal review of the acquisition or procurement of a weapon generally focuses on whether the weapon is illegal per se — whether a treaty to which the united states is a party or customary international law has prohibited its use in all circumstances. The use of autonomy to aid in the operation of weapons is not illegal per se. 7. Law of war issues related to targeting generally are not determinative of the lawfulness of a weapon. A legal review of a weapon should consider whether the weapon is "inherently indiscriminate," i.e., whether the weapon is capable, under any set of circumstances and in particular the intended concept of employment, of being used in accordance with the principles of distinction and proportionality. Nevertheless, most targeting issues (e.g., whether a weapon would be used consistent with the requirement that attacks may only be directed against military objectives) are only capable of determination when presented with the facts of a particular military operation. 8. Weapons that use autonomy in target selection and engagement seem unique in the degree to which they would allow consideration of targeting issues during the weapon’s development. For example, if it is possible to program how a weapon will function in a potential combat situation, it may be appropriate to consider the law of war implications of that programming. In particular, it may be appropriate for weapon designers and engineers to consider measures to reduce the likelihood that use of the weapon will cause civilian casualties. 9. Under dod policy, autonomous and semi-autonomous weapons systems go through "rigorous hardware and software verification and validation (v&v) and realistic system developmental and operational test and evaluation (t&e)." although rigorous testing and sound development of weapons are not required by the law of war as such, these good practices can support the implementation of law of war requirements. Rigorous and realistic testing standards and procedures can ensure that commanders and national security policy makers can have a reasonable expectation of the likely effects of employing the weapon in different operational contexts. In addition, such practices can help reduce the risk of unintended combat engagements, such as weapons malfunctions that could inadvertently cause harm to civilians. Iii. Compliance with the law of war in using weapon systems with autonomous functions 10. The law of war rules on conducting attacks (e.g., the rules relating to distinction and proportionality) impose obligations on states and other parties to a conflict, and it is for individual human beings, commensurate with their role within the state or party to the conflict, to ensure compliance with those obligations when employing any weapon or weapons system, including autonomous or semi-autonomous weapons systems. For example, dod policy recognizes that "p]ersons who authorize the use of, direct the use of, or operate autonomous and semi-autonomous weapon systems must," among other requirements, "do so … in accordance with the law of war." 11. It is not the case that the law of war requires that a weapon, even a semi-autonomous or autonomous weapon, make legal determinations. For example, the law of war does not require that a weapon determine whether its target is a military objective, but rather that the weapon be capable of being employed consistent with the principle of distinction. Similarly, the law of war does not require that a weapon make proportionality determinations, such as whether an attack is expected to result in incidental harm to civilians or civilian objects that is excessive in relation to the concrete and direct military advantage expected to be gained. Ccw/gge.1/2017/wp.6 3 12. The law of war does not require weapons to make legal determinations, even if the weapon (e.g., through computers, software, and sensors) may be characterized as capable of taking some form of action or decision in a given moment in the absence of direction by a human being, such as whether to fire the weapon or to select and engage a target. Relatively rudimentary autonomous weapons, such as homing missiles, have been employed for many years, and there has never been a requirement that such weapons themselves determine that legal requirements are met. 13. Rather, it is persons who must comply with the law of war by employing weapons in a discriminate and proportionate manner. For example, even if the weapon autonomously selects and engages targets, its use would be precluded when the use of the weapon would be expected to result in incidental harm to civilians or civilian objects that is excessive in relation to the concrete and direct military advantage expected to be gained. 14. In addition, the obligation to take feasible precautions in order to reduce the risk of harm to civilians and other persons or objects protected from being made the object of attack must be considered when using weapon systems with advanced autonomous functions. For example, depending on the circumstances, it might be feasible to monitor the operation of the weapon system and to stop its operation in the event that it malfunctioned or the circumstances change. As another example, it might be appropriate to consider whether it is possible to program or build mechanisms into the weapon that would reduce the risk of civilian casualties while in no way decreasing the military advantages offered by the weapon. A best practice in this regard may be found in the requirements in dod policy for the interface between people and machines for autonomous and semi-autonomous weapons to: (1) be readily understandable to trained operators; (2) provide traceable feedback on system status; and (3) provide clear procedures for trained operators to activate and deactivate system functions. These requirements to improve human-machine interfaces assist operators in making accurate judgments regarding the use of force. 15. The ability of weapons to make decisions or assessments of issues that would be considered under law of war can be viewed as an additional feature that improves the ability of human beings to implement legal requirements rather than as an effort to replace a human being’s responsibility and judgment under the law. Iv. Potential for autonomy in weapon systems to improve the implementation of law of war principles in military operations 16. In many cases, the use of autonomy in weapon systems could enhance the way law of war principles are implemented in military operations. 17. For example, very basic applications of autonomy allow some munitions to self-deactivate or to self-destruct, which helps reduce the risk these weapons may pose to the civilian population after the munitions have served their military purpose. 18. More advanced applications of autonomy may facilitate greater precision in guidance of bombs and missiles against military objectives, reducing the likelihood of inadvertently striking civilians and civilian objects as compared to the use of unguided bombs and missiles to achieve the same desired result. 19. Similarly, autonomous functions allow defensive systems to select and engage incoming enemy projectiles, such as mortars, artillery shells, and rockets. These defensive systems can provide military commanders more time to decide on how to respond to the threat. For example, directing "counter-battery fire" against the origin of the enemy projectiles has been a common response to such attacks, and the additional time afforded by autonomous defensive systems could allow military commanders more time to consider and execute a more deliberate and precise response. 20. These applications of autonomy illustrate a fundamental feature of the law of war — the law of war often reflects the convergence of military and humanitarian interests. Ccw/gge.1/2017/wp.6 4 21. Autonomy can be used in weapon systems to create more capabilities. Commanders can use additional capabilities to increase the efficiency of military operations — more precisely applying force and causing less unintended destruction. Improving efficiency is done for sound military reasons — to allow fewer resources to accomplish more military purposes. But the same capabilities that reduce wasteful or incorrect applications of military force, such as incidents of "friendly fire," can also reduce the risk of civilian casualties. 22. For example, militaries might develop weapons with advanced technologies, such as smart grenade launchers, to give their soldiers new advantages in countering the use of cover by enemy fighters to avoid small arms fire. By reducing the need for even greater applications of force such as artillery or air bombardments, these weapons have potentially long-term benefits by reducing the effects of larger explosive weapons in populated areas or the presence of explosive remnants of war. 23. These types of "smart" weapons might create additional options for commanders — allowing attacks to be conducted in circumstances where the use of "dumb" weapons would cause significant or excessive civilian casualties. This, however, should not be construed as necessarily requiring states to use "smart" weapons when available rather than "dumb" weapons. 24. It is expected that further developments in autonomous and semi-autonomous weapon systems will allow military forces to apply force more precisely and with less collateral damage than would be possible with existing systems. V. Legal accountability and weapons with autonomous functions 25. Machines are not states or persons under the law. Questions of legal accountability are questions of how existing and well-established principles of state and individual responsibility apply to states and persons who use weapon systems with autonomous functions. 26. As a general principle, states are responsible for the acts of persons forming part of their armed forces. It follows that states are responsible for the uses of weapons with autonomous functions by persons forming part of their armed forces as well as other such acts that may be attributable to a state under the law of state responsibility. States, in ensuring accountability for such conduct, may use a variety of mechanisms, including investigations, individual criminal liability, civil liability, and internal disciplinary measures. 27. As with all decisions to employ weapon systems, persons are responsible for their individual decisions to use weapons with autonomous functions. For example, persons who use weapons with autonomous functions to violate the prohibition on targeting the civilian population may be held responsible for such violations. 28. The responsibilities of any particular individual belonging to a state or a party to the conflict may depend on that person’s role in the organization or military operations. As a general matter, the persons who are responsible for implementing a party to a conflict’s obligation are those persons with the authority to make the necessary decisions and judgments required by that international obligation. For example, a party to a conflict has the obligation to take feasible precautions to reduce the risk to civilians, such as providing warnings before attacks. The determination of whether it is feasible to provide such a warning would be made by the relevant commander in charge of the attack. 29. As noted above, advanced applications of autonomy in weapon systems can allow for issues that would normally only be presented in the context of the use of the weapon system to be presented in the context of the development of the weapon system. Persons who engage in wrongdoing in the development and testing of a weapon could be held accountable, at least under principles and rules of accountability in domestic law. 30. Intentional wrongdoing involving weapons is clearly prohibited. In the absence of intentional wrongdoing, assessments of accountability may be more complex. Mere accidents or equipment malfunctions are not violations of the law of war, even if civilians are killed or injured as a result of those malfunctions. The standard of care or regard that is due in ccw/gge.1/2017/wp.6 5 conducting military operations with regard to the protection of civilians is a complex question to which the law of war does not provide a simple answer. This standard must be assessed based on the general practice of states and common standards of the military profession in conducting operations. 31. A general principle of accountability, which is reflected in the law of war, is that decision-makers must be judged based on the information available to them at the time and not on the basis of information that subsequently comes to light. Thus, for example, in assessing whether a commander’s decision to use weapons with autonomous functions was reasonable in a particular context, whether the commander acted in good faith based on the information available to him or her at the time would need to be considered. In this regard, training on the weapon system and rigorous testing of the weapon system can help commanders be advised of the likely effects of employing the weapon system. These measures, found in dod policy, can help promote good decision-making and accountability.
Geneva, 13–17 november 2017 item 6 of the revised provisional agenda examination of various dimensions of emerging technologies in the area of lethal autonomous weapons systems, in the context of the objectives and purposes of the convention characteristics of lethal autonomous weapons systems submitted by the united states of america i. Introduction 1. This working paper seeks to contribute to the meetings of the group of governmental experts (gge) on lethal autonomous weapons systems (laws) in geneva between november 13–17, 2017, by providing the views of the united states on: (i) identifying characteristics of "laws" rather than negotiating a "definition" in these gge discussion; (ii) recommendations regarding characteristics of laws; and (iii) definitions used by the united states department of defense in internal policies on the use of autonomy in weapon systems. Ii. Identifying characteristics of laws rather than negotiating a definition of "laws" in these gge discussions 2. The united states believes that it is unnecessary for the gge to adopt a specific working definition of laws. Instead, we support promoting a general understanding of the characteristics of laws. We believe that the absence of a specific working definition is no impediment to the gge’s work in understanding the potential issues posed by laws. Given that the law of war provides a robust and coherent system of regulation for the use of weapons, the gge can discuss the issues potentially posed by "laws" under the object and purpose of the ccw without needing to agree on a specific working definition of laws. For example, as explained in the united states working paper on legal issues, the law of war’s existing rules of general applicability apply with respect to the use of all weapons, including any weapons deemed to be "laws." 3. A legal definition is generally developed for the specific purposes of a legal rule and not in the abstract. Often legal definitions determine the scope of a legal rule, i.e., the matters to which the rule would apply. For example, the definition of “remotely-delivered mine” in the ccw amended protocol ii identifies what types of mines are subject to a set of restrictions in that protocol. 4. A working definition should not be drafted with a view toward describing weapons that should be banned. This would be premature and counterproductive because it would divert time and effort from understanding the issues to negotiating what would be covered. As the high contracting parties have not decided to negotiate or adopt a new protocol ccw/gge.1/2017/wp.7 group of governmental experts of the high contracting parties to the convention on prohibitions or restrictions on the use of certain conventional weapons which may be deemed to be excessively injurious or to have indiscriminate effects 10 november 2017 original: english ccw/gge.1/2017/wp.7 2 specifically to ban or regulate laws, any common understanding of laws must not prejudice future decisions regarding potential outcomes. 5. In identifying characteristics of laws, we must be cautious not to make hasty judgments about the value or likely effects of emerging or future technologies. Frequently, we may change our views of technologies over time as we gain more experience with them. In particular, we want to encourage innovation and progress in addressing the objects and purposes of the convention. Iii. Recommendations regarding characteristics of laws 6. Although we believe it unnecessary for the gge to seek to negotiate a single working definition, we support identifying general characteristics of laws in order to promote our understanding of the relevant concepts or issues in these gge discussions. Identifying general characteristics of laws will help us understand what is generally referred to by this term, without providing a definition that would establish the parameters or what is, or is not, included. This flexibility in approach is important given that scientists and engineers continue to develop new technological advancements and that our understanding continues to improve. In light of that purpose, we offer the following recommendations regarding characteristics of laws. 7. The characteristics of laws should be intelligible to all relevant audiences, including roboticists, engineers, scientists, lawyers, military personnel, and ethicists. The characteristics of laws should not be identified based on specific technological assumptions such that the characteristic would be rendered obsolete by technological developments. In this regard, we should not articulate specific levels of autonomy or types of machine reasoning. Our sense is that creating technical categories like this or seeking to define "artificial intelligence" would be especially ill-advised because there are already diverse taxonomies along these lines and because scientists and engineers continue to develop technological advancements. 8. Seeking to define the sophistication of the machine intelligence would incorrectly focus on the machine, rather than understanding what is important for the law — how human beings are using the weapon and what they expect it to do. For example, it is irrelevant under the law of war whether a rocket engine is powered by a solid fuel or a liquid propellant. Rather, the law of war is concerned with how that power is used in combat. Similarly, focusing on the sophistication of the "analytical engine" powering a weapon (e.g., what type of algorithm or method of machine learning is employed) risks ignoring the focus of the law — how humans will use that weapon (e.g., using the machine to select and engage targets without further intervention by a human operator). 9. Lastly, focusing on the machine also could stimulate unwarranted fears that are more the product of science fiction and popular imagination than fact. Iv. Definitions used by the united states department of defense in internal policies on the use of autonomy in weapon systems 10. In light of the above considerations and to further the gge’s understanding of some of the relevant concepts and issues related to the characteristics of laws, we offer for consideration definitions that the department of defense has specifically developed for use in its internal policies relating to the use of autonomy in weapon systems. 11. Although the gge’s purpose may be different than the purposes for which these definitions were created, we believe these definitions help demonstrate the careful thought that should be applied when identifying the relevant characteristics of laws. For example, these definitions were developed after considering existing weapon systems. These definitions do not depend on a technical characterization of the sophistication of the machine reasoning. Instead, these definitions focus on what we believe to be the most important issues posed by the use of autonomy in weapon systems — people who employ these weapons can rely on the weapon systems to select and engage targets. Ccw/gge.1/2017/wp.7 3 12. Department of defense policy includes the following definitions: (a) "autonomous weapon system. A weapon system that, once activated, can select and engage targets without further intervention by a human operator. This includes human-supervised autonomous weapon systems that are designed to allow human operators to override operation of the weapon system, but can select and engage targets without further human input after activation." (b) "semi-autonomous weapon system. A weapon system that, once activated, is intended to only engage individual targets or specific target groups that have been selected by a human operator. This includes: semi-autonomous weapon systems that employ autonomy for engagement-related functions including, but not limited to, acquiring, tracking, and identifying potential targets; cueing potential targets to human operators; prioritizing selected targets; timing of when to fire; or providing terminal guidance to home in on selected targets, provided that human control is retained over the decision to select individual targets and specific target groups for engagement. ‘fire and forget’ or lock-on-after-launch homing munitions that rely on ttps tactics, techniques, and procedures] to maximize the probability that the only targets within the seeker’s acquisition basket when the seeker activates are those individual targets or specific target groups that have been selected by a human operator." 13. The united states department of defense directive establishing these definitions "d]oes not apply to autonomous or semi-autonomous cyberspace systems for cyberspace operations; unarmed, unmanned platforms; unguided munitions; munitions manually guided by the operator (e.g., laser- or wire-guided munitions); mines; or unexploded explosive ordnance." therefore, these types of systems, platforms, weapons, or devices, would not be considered to fall within the rules established by this department of defense directive for "autonomous" or "semi-autonomous" weapons.
Ge.18-14117(e)  second session geneva, 27 - 31 august 2018 item 6 of the provisional agenda other matters human-machine interaction in the development, deployment and use of emerging technologies in the area of lethal autonomous weapons systems submitted by the united states ensuring that machines effectuate human intent in using force 1. In our view, the key issue for human-machine interaction in emerging technologies in the area of laws is ensuring that machines help effectuate the intention of commanders and the operators of weapons systems. This is done by, inter alia, taking practical steps to reduce the risk of unintended engagements and to enable personnel to exercise appropriate levels of human judgment over the use of force. 2. This approach supports compliance with the law of war. Weapons that do what commanders and operators intend can effectuate their intentions to conduct operations in compliance with the law of war and to minimize harm to civilians and civilian objects. 3. This paper discusses a number of measures the united states is taking to ensure that new weapons help effectuate the commander’s intent. These measures and policies are set forth in u.s. Department of defense directive 3000.09, autonomy in weapon systems (dod directive 3000.09). Dod directive 3000.09 was initially issued in 2012 after a dod working group considered dod’s past practice in using autonomy in weapon systems, including lessons learned, and potential future applications of autonomy in weapon systems. Minimizing unintended engagements 4. Dod directive 3000.09 states that one of its purposes is to establish “guidelines designed to minimize the probability and consequences of failures in autonomous and semi-autonomous weapon systems that could lead to unintended engagements.”1 5. Dod directive 3000.09 defines “unintended engagement” as “t]he use of force resulting in damage to persons or objects that human operators did not intend to be the 1 dod directive 3000.09, 1.a. Ccw/gge.2/2018/wp.4 group of governmental experts of the high contracting parties to the convention on prohibitions or restrictions on the use of certain conventional weapons which may be deemed to be excessively injurious or to have indiscriminate effects 28 august 2018 english only ccw/gge.2/2018/wp.4 2 targets of u.s. Military operations, including unacceptable levels of collateral damage beyond those consistent with the law of war, roe, and commander’s intent.”2 6. For example, accidental attacks that killed civilians or friendly forces would be “unintended engagements” under dod directive 3000.09. 7. Similarly, even an attack against authorized targets could be “unintended” if there are significant changes to the factual context between the time of authorization and the engagement (for example, if a cease-fire agreement is negotiated). In this regard, dod directive 3000.09 requires that autonomous and semi-autonomous weapon systems be designed to “c]omplete engagements in a timeframe consistent with commander and operator intentions and, if unable to do so, to terminate engagements or seek additional human operator input before continuing the engagement.”3 ensuring appropriate levels of human judgment over the use of force 8. Dod directive 3000.09 requires that autonomous and semi-autonomous weapon systems “be designed to allow commanders and operators to exercise appropriate levels of human judgment over the use of force.”4 9. “appropriate” is a flexible term that reflects the fact that there is not a fixed, one-size-fits-all level of human judgment that should be applied to every context. What is “appropriate” can differ across weapon systems, domains of warfare, types of warfare, operational contexts, and even across different functions in a weapon system. Some functions might be better performed by a computer than a human being, while other functions should be performed by humans. 10. In some cases, less human involvement might be more appropriate. For example, in certain defensive autonomous weapon systems, such as the phalanx close-in weapon system, the aegis weapon system, and patriot air and missile defense system, the weapon system has autonomous functions that assist in targeting incoming missiles or other projectiles. The machine can strike incoming projectiles with much greater speed and accuracy than a human gunner could achieve manually. As weapons engineers improve the effectiveness of autonomous functions, more situations will likely arise in which the use of autonomous functions is more appropriate than manual control. 11. “human judgment over the use of force” is distinct from human control over the weapon. For example, an operator might be able to exercise meaningful control over every aspect of a weapon system, but if the operator is only reflexively pressing a button to approve strikes recommended by the weapon system, the operator would be exercising little, if any, judgment over the use of force. On the other hand, judgment can be implemented through the use of automation. For example, the extensive automation of functions in a weapon system could allow the operator to exercise better judgment over the use of force by removing the need to focus on basic tasks and to give him or her more time to understand the broader situation. Similarly, the use of algorithms or even autonomous functions that take control away from human operators can better effect human intentions and avoid accidents. A useful case to consider may be the automatic ground collision avoidance system developed by the u.s. Air force that has helped prevent so-called “controlled flight into terrain” accidents. The system assumes control of the aircraft when an imminent collision with the ground is detected and returns control back to the pilot when the collision is averted. 12. Dod directive 3000.09’s requirements that weapons be designed to allow commanders and operators to exercise appropriate levels of human judgment over the use of force reflect a deliberate decision to permit weapons that are programmed to make “decisions” that relate to targeting. 2 dod directive 3000.09, glossary. 3 dod directive 3000.09, 4.a.(1)(b); see also dod directive 3000.09, enclosure 3, 1.a.(2). 4 dod directive 3000.09, 4.a. Ccw/gge.2/2018/wp.4 3 13. Autonomy has already been used sensibly in targeting-related functions such as identifying, selecting, and determining whether and when to engage targets. As we noted in a working paper submitted in 2017, there is no requirement that the machine itself be programmed to make law of war assessments, such as whether the target is a military objective. Rather, there are a variety of ways to ensure that even relatively simple forms of automation can be used appropriately in military operations. 14. For example, an autonomous system might be programmed to operate only within certain geographic boundaries. If deployed and limited to an area that was a military objective, such as an enemy military headquarters complex, then its use would be analogous to the use of other weapons, like artillery, that are used to target areas of land that qualify as military objectives. 15. Similarly, an autonomous system might be equipped with sensors that are designed to detect specific “signatures” – unique, identifying characteristics that would be specific to a military objective, such as frequencies of electromagnetic radiation that are generally not found naturally or among civilian objects. Many states have used weapons that detect the specific electromagnetic signals emitted by enemy radar. Practical measures to ensure the use of autonomy in weapon system effectuates human intentions 16. Dod directive 3000.09 establishes a number of requirements – at different stages of the weapon design, development, and deployment process – intended to ensure the use of autonomy in weapon systems effectuates human intentions. 17. A key theme among these requirements is ensuring that systems “function as anticipated.”5 this entails engineering weapon systems to perform reliably, training personnel to understand the systems, and establishing clear human-machine interfaces. 18. First, a variety of measures are taken to ensure that weapons are engineered to perform as expected. 19. Dod directive 3000.09 establishes requirements for verification and validation and test and evaluation. Before fielding systems that would use autonomy in novel ways, such reviews must “assess system performance, capability, reliability, effectiveness, and suitability under realistic conditions, including possible adversary actions, consistent with the potential consequences of an unintended engagement or loss of control of the system.”6 such testing should include “analysis of unanticipated emergent behavior resulting from the effects of complex operational environments on autonomous or semi-autonomous systems.”7 20. Dod directive 3000.09 also requires that “safeties, anti-tamper mechanisms, and information assurance” have been implemented in autonomous and semi-autonomous weapon systems.8 these measures are intended to “minimize the probability or consequences of failures that could lead to unintended engagements or to loss of control of the system” by, for example, safeguarding against attempts by unauthorized individuals to fire the weapon.9 21. Second, dod directive 3000.09 seeks to ensure that personnel properly understand the weapon systems. A key insight from past studies of accidents involving human use of automation, such as studies of accidental shoot-downs of friendly aircraft by the patriot missile system, is that failures can often result from operator error and that better training and adherence to established tactics, techniques, and procedures (ttps) and doctrine could prevent mistakes that would result in unintended engagements. 5 dod directive 3000.09, 4.a.(1)(a). 6 dod directive 3000.09, enclosure 3, 1.b.(3). 7 dod directive 3000.09, enclosure 2, a. 8 dod directive 3000.09, 4.a.(2)(a). 9 dod directive 3000.09, enclosure 3, 1.b.(2). Ccw/gge.2/2018/wp.4 4 22. Therefore, dod directive 3000.09 generally requires the establishment of “t]raining, doctrine, and tactics, techniques, and procedures.”10 moreover, before systems that employ autonomy in new ways are fielded, senior officials must determine that “a]dequate training, ttps, and doctrine are available, periodically reviewed, and used by system operators and commanders to understand the functioning, capabilities, and limitations of the system’s autonomy in realistic operational conditions.”11 23. Officials responsible for training and equipping forces are to “c]ertify that operators of autonomous and semi-autonomous weapon systems have been trained in system capabilities, doctrine, and ttps in order to exercise appropriate levels of human judgment in the use of force and employ systems with appropriate care and in accordance with the law of war, applicable treaties, weapon system safety rules, and applicable roe.”12 24. In addition, commanders must use weapons “in a manner consistent with their design, testing, certification, operator training, doctrine, ttps, and approval as autonomous or semi-autonomous systems.”13 25. Third, dod directive 3000.09 requires that the interface between humans and machines be clear “i]n order for operators to make informed and appropriate decisions in engaging targets.”14 26. In particular, dod directive 3000.09 requires that “the interface between people and machines for autonomous and semi-autonomous weapon systems shall: (a) be readily understandable to trained operators; (b) provide traceable feedback on system status; (c) provide clear procedures for trained operators to activate and deactivate system functions.”15 holistic, proactive, review processes guided by the fundamental principles of the law of war 27. Emerging technologies are difficult to regulate because technologies continue to change as scientists and engineers develop advancements. A best practice today might not be a best practice in the near future. Similarly, a weapon system that, if built today, would risk creating indiscriminate effects, might, if built with future technologies, prove more discriminating than existing alternatives by reducing the risk of civilian casualties. 28. Thus, rather than seeking to codify best practices or set new international standards, states should seek to exchange practice and implement holistic, proactive review processes that, are guided by the fundamental principles of the law of war. Holistic processes across the touch points in the human-machine interface 29. The chair of the gge has helpfully framed “four broad areas of touch points in the human-machine interface” – 1) “research & development”; 2) “testing and evaluation,” “verification and validation,” and “reviews”; 3) “deployment, command & control”; and 4) “use & abort.”16 30. In addressing issues in human-machine interaction, we recommend a holistic approach that considers all the touch points of human-machine interaction. For example, 10 dod directive 3000.09, 4.a.(1). 11 dod directive 3000.09, enclosure 3, 1.b.(4). 12 dod directive 3000.09, enclosure 4, 8.a.(5). 13 dod directive 3000.09, enclosure 4, 10.a. 14 dod directive 3000.09, 4.a.(3). 15 dod directive 3000.09, 4.a.(3). 16 chair’s summary of the discussion on agenda item 6(a) 9 and 10 april 2018, agenda item 6(c) 12 april 2018, agenda item 6(d) 13 april 2018. Ccw/gge.2/2018/wp.4 5 the solution to a problem identified during use of a weapon might be generated by a research laboratory, or an issue identified in the development of a weapon might be resolved by new policies or rules of engagement. 31. As a case in point, trust and accountability issues are posed by the fact that current ai systems often use processes that are opaque to the human operators of the systems. To help address trust and accountability issues, the defense advanced research projects agency’s explainable ai project seeks to develop new machine-learning systems that “have the ability to explain their rationale, characterize their strengths and weaknesses, and convey an understanding of how they will behave in the future.”17 by seeking to develop ai systems that are more transparent to human operators, such work in the research and development area can address concerns that might be posed by the use of such technology. Proactive reviews during development and before fielding 32. We also recommend a proactive approach in addressing issues in human-machine interaction. States seeking to develop new uses for autonomy in their weapons should be affirmatively seeking to identify and address these issues in their respective processes for managing the life cycle of such weapons. For example, dod directive 3000.09 requires senior officials to review weapon systems that use autonomy in new ways. Such reviews, which are required before a system enters formal development and, again, before fielding, ensure that military, acquisition, legal, and policy expertise is brought to bear before new types of weapons systems are used. 33. This practice in conducting a special policy review is consistent with broader dod practice in conducting legal reviews of the intended acquisition or procurement of any weapon by the department of defense, as reflected in u.s. Department of defense directive 5000.01, the defense acquisition system. Such reviews, among other things, help ensure consistency with the law of war. Guidance from the fundamental principles of the law of war 34. In applying holistic approaches and proactive review processes, states should be guided by the fundamental principles of the law of war. 35. The u.s. Military has long used the fundamental principles of law of war as a general guide for conduct during war, when no more specific rule applies.18 these principles are: military necessity, humanity, distinction, proportionality, and honor.19 36. These principles have also been the basis for many codifications of the law of war, including the geneva conventions of 1949, which, as the international court of justice 17 david gunning, explainable artificial intelligence (xai), available at: https://www.darpa.mil/program/explainable-artificial-intelligence. 18 see, e.g., u.s. Department of defense law of war manual § 2.1.2.2 (june 2015, updated december 2016) (“when no specific rule applies, the principles of the law of war form the general guide for conduct during war.”). U.s. War department, part two, rules of land warfare, basic field manual, volume vii, military law, p.1, ¶4, jan. 2, 1934 (“among the so-called unwritten rules or laws of war are three interdependent basic principles that underlie all of the other rules or laws of civilized warfare, both written and unwritten, and form the general guide for conduct where no more specific rule applies, … .”); instructions for the government of armies of the united states in the field, prepared by francis lieber, issued as general orders no. 100, adjutant general’s office, 1863, arts. 14-16 (discussing the principle of military necessity), art 30 (“no conventional restriction of the modes adopted to injure the enemy is any longer admitted; but the law of war imposes many limitations and restrictions on principles of justice, faith, and honor.”). 19 u.s. Department of defense law of war manual, chapter ii (june 2015, updated december 2016). Ccw/gge.2/2018/wp.4 6 (icj) has observed, “are in some respects a development, and in other respects no more than the expression, of” fundamental general principles of international humanitarian law.20 37. The practice of resorting to the fundamental principles of the law of war even though specific rules might not apply, has itself been codified in the so-called “martens clause.” first included in the preamble to the 1899 hague convention ii with respect to the laws and customs of war on land, the clause also is included in a common article to the 1949 geneva conventions, which states that denunciation of the convention “shall in no way impair the obligations which the parties to the conflict shall remain bound to fulfil by virtue of the principles of the law of nations, as they result from the usages established among civilized peoples, from the laws of humanity and the dictates of the public conscience.”21 38. The icj has observed that, in relation to “the cardinal principles constituting the fabric of humanitarian law,” the martens clause “has proved to be an effective means of addressing the rapid evolution of military technology.”22 the icj’s observation has been reflected in the practice of the united states. For example, careful consideration of the principles of military necessity and humanity has been critical to the u.s. Department of defense’s review of the legality of new weapons.23 39. In addition to helping to assess whether a new weapon falls under a legal prohibition, the fundamental principles of the law of war may also serve as a guide in answering novel ethical or policy questions in human-machine interaction presented by emerging technologies in the area of laws. 40. For example, if the use of a new technology advances the universal values inherent in the law of war, such as the protection of civilians, then the development or use of this technology is likely to be more ethical than refraining from such use. 41. The following questions might be useful to consider in assessing whether to develop or deploy an emerging technology in the area of lethal autonomous weapons systems: (a) does military necessity justify developing or using this new technology? (b) under the principle of humanity, does the use of this new technology reduce unnecessary suffering? (c) are there ways this new technology can enhance the ability to distinguish between civilians and combatants? (d) under the principle of proportionality, has sufficient care been taken to avoid creating unreasonable or excessive incidental effects? (e) under the principle of the honor, does the use of this technology respect and avoid undermining the existing law of war rules? “human control” 42. The key issue for human-machine interaction in the development, deployment, and use of emerging technologies in the area of lethal autonomous weapons systems is ensuring 20 military and paramilitary activities in and against nicaragua, (nicaragua v. United states of america), merits, judgment, i.c.j. Reports 1986, p.14, 113 (june 27, 1986, ¶218). 21 geneva convention for the amelioration of the wounded and sick in armed forces in the field of august 12, 1949, art. 63, 1950 unts 32, 68; geneva convention for the amelioration of the condition of wounded, sick and shipwrecked members of the armed forces at sea of august 12, 1949, art. 62, 1950 unts 86, 120; geneva convention relative to the treatment of prisoners of war of august 12, 1949, art. 142, 1950 unts 136, 242; geneva convention relative to the protection of civilian persons in time of war of august 12, 1949, art. 158, 1950 unts 288, 392. 22 legality of the threat or use of nuclear weapons, advisory opinion, i.c.j. Reports 1996, p. 226, 257 (july 8, 1996, 78). 23 u.s. Department of defense law of war manual, §§ 6.6.2, 6.6.3.1 (june 2015, updated december 2016) (discussing the application of the principles of humanity and military necessity in the context of applying the prohibition against weapons calculated to cause superfluous injury). Ccw/gge.2/2018/wp.4 7 that when it is necessary to use force, such force is used to effectuate the intentions of commanders and operators. In particular, practical measures should be taken to reduce the risk of unintended engagements (e.g., those resulting from accidents or sabotage) and to ensure that personnel exercise appropriate levels of human judgment over any use of force. 43. We view this as distinct from the concept of “human control,” a term that risks obscuring the genuine challenges in human-machine interaction. 44. Practical measures to facilitate effective human-machine interaction – ensuring that force is used to effectuate human intentions – are set forth in dod directive 3000.09, autonomy in weapon systems. 45. Seeking to codify best practices or set new international standards for human-machine interaction in this area is impractical because rapid technological advancements may render such practices or standards obsolete shortly after they are established. Instead, states should ensure responsible use of emerging technologies in military operations by implementing holistic, proactive review processes that are guided by the fundamental principles of the law of war. Terminologies and conceptualizations: the misplaced focus of “human control” 46. During the april 2018 session of the gge, delegations presented a range of different terminologies and conceptualizations regarding human-machine interaction, including human control, supervision, oversight, and judgment. Some have advocated that ccw gge discussions focus in particular on the issue of “human control” of weapons systems and have advocated for the promulgation of new standards to ensure minimum levels of control or “meaningful human control.” the concept of “human control” is subject to divergent interpretations that can hinder meaningful discussion. 47. As we explain below, we believe that emphasis on “control” would obscure rather than clarify the genuine challenges in this area. International discussions about weapon control systems related to emerging technologies are not likely to produce useful common understandings with respect to all weapons that use such technologies 48. On a practical level, discussions of the technical systems that are used to control weapon systems manually are not likely to advance our collective understanding of the challenges and benefits presented by emerging technologies. How a weapon system is controlled is often very specific to the weapon system, and control systems can vary greatly from system to system. Accordingly, any insight that can be gained from discussing human control of one weapon system may only be of limited relevance to other weapons systems. 49. Similarly, past regulation of weapons systems under international humanitarian law has not included broadly applicable standards for weapon control systems. Moreover, existing international humanitarian law instruments, such as the ccw and its protocols, do not seek to enhance “human control” as such. Rather, these instruments seek, inter alia, to ensure the use of weapons consistent with the fundamental principles of distinction and proportionality, and the obligation to take feasible precaution for the protection of the civilian population. Although control over weapon systems can be a useful means in implementing these principles, “control” is not, and should not be, an end in itself. Autonomous functions in a weapon system can enhance human control over the use of force 50. Some may think it important to emphasize “human control” because they view developments in the use of automation or autonomy in a weapon system as decreasing human control over the use of force. We believe such a view would be mistaken. 51. Technical sophistication in a weapon system that enables it to perform functions autonomously – what are often called “smart” weapons – does not necessarily mean that ccw/gge.2/2018/wp.4 8 there is any less human involvement in the decision-making of how that weapon is used. The use of technology, such as sensors and computers, allows personnel to set the parameters for when, where, and how force is deployed without manually controlling the weapons system at all times. 52. The use of “smart” weaponry with autonomous functions has increased the degree of control that states exercise over the use of force. For example, many states employ weapons such as hellfire or javelin missiles, which use autonomy in critical functions to home-in on targets identified by human operators. Other common weapons, such as the high-speed anti-radiation missile (harm) or smart 155 artillery shells, have autonomous functions that allow them to sense categories of targets according to how they have been programmed and to guide themselves to those targets. 53. Personnel use these weapons with the intention to achieve specific military effects. The fact that the projectile might also “select” a target that has been identified by a human operator or that has been programmed into it and autonomously maneuver itself toward a target does not amount to a delegation of decision-making from humans to machines. Rather, the machine’s programming and sensors enable it to effectuate the intentions of the forces using this weapon in a way that is superior to weapons without such programming and sensors. Manual control of a weapons system is not a prerequisite for holding humans accountable 54. Some may argue that it is important to emphasize control because of concerns that the use of autonomous weapons systems somehow removes individuals from responsibility. However, personnel are responsible for their decisions to use force regardless of the nature of the weapon system they utilize. The lack of a manual control over a weapon system does not remove this responsibility or result in an accountability gap. 55. Computers can enable machines to respond to inputs from sensors through an application of the algorithms or other processes with which they have been programmed. Machines, however, are not intervening moral agents, and human beings do not escape responsibility for their decisions by using a weapon with autonomous functions. 56. When using weapons systems with autonomous functions, the commander must make the legal judgments required by ihl, including by the principles of distinction and proportionality. The human operators of the system and their superior commanders are responsible and accountable for their use of the system, even if that system has sophisticated autonomous functions.
Unclassified – september 1, 2020 1 u.s. Commentaries on the guiding principles this paper provides u.s. Commentaries on the eleven guiding principles adopted by the group of governmental experts (gge) on emerging technologies in the area of lethal autonomous weapons systems (laws) and endorsed by high contracting parties to the convention on certain conventional weapons (ccw).1 the guiding principles serve as a foundation for the gge’s future work and can also guide states in the responsible development and use of emerging technologies in the area of laws. The guiding principles are a cohesive framework with each principle reinforcing others.2 (a) international humanitarian law continues to apply fully to all weapons systems, including the potential development and use of lethal autonomous weapons systems. This guiding principle is a foundational one for the gge’s work. Understanding how ihl applies to the potential development and use of lethal autonomous weapons systems is critical for effectively implementing the other guiding principles, including guiding principles (c), (d), (e), and (h). The gge should build on its successful 2019 work on ihl by further clarifying ihl requirements applicable to the use of emerging technologies in the area of laws. What ihl requires often depends on how weapons or tools are being used. Thus, clarifying ihl requirements can be done by considering how militaries have used autonomous functions in weapon systems. In its 2019 working paper, the united states described three general scenarios for the use of autonomous functions in weapon systems: 1) using autonomous functions to effectuate more accurately and reliably a commander or operator’s intent to strike a specific target or target group; 2) using autonomous functions to inform a commander or operator’s decision-making about what targets he or she intends to strike; 3) using autonomous functions to select and engage specific targets that the commander or operator did not know of when he or she activated the weapon system. 1 the united states reaffirms its support for the gge’s relevant conclusions in previous years’ reports and the views previously expressed in u.s. Working papers to the gge, which may elaborate on the points in this submission. See implementing international humanitarian law in the use of autonomy in weapon systems, march 28, 2019, ccw/gge.1/2019/wp.5; human-machine interaction in the development, deployment and use of emerging technologies in the area of lethal autonomous weapons systems, august 28, 2019, ccw/gge.2/2018/wp.4; humanitarian benefits of emerging technologies in the area of lethal autonomous weapon systems, march 28, 2018, ccw/gge.1/2018/wp.4; characteristics of lethal autonomous weapons systems, november 10, 2017, ccw/gge.1/2017/wp.7; autonomy in weapon systems, november 10, 2017, ccw/gge.1/2017/wp.6. 2 for example, further work on elaborating how international humanitarian law (ihl) applies to the potential development and use of lethal autonomous weapons systems (principle (a)), can assist states in conducting legal reviews of new weapons (principle (e)), and such legal reviews also provide an opportunity to consider good practices in human-machine interaction to ensure compliance with ihl (principle (c)), as well as risk assessments and mitigation measures (principle (g)). Unclassified – september 1, 2020 2 the united states proposes the following draft conclusions for the gge’s consideration. 1. Consistent with ihl, autonomous functions may be used to effectuate more accurately and reliably a commander or operator’s intent to strike a specific target or target group. A. The addition of autonomous functions, such as the automation of target selection and engagement, to weapon systems can make weapons more precise and accurate in striking military objectives by allowing weapons or munitions to “home in” on targets selected by a human operator. B. If the addition of autonomous functions to a weapon system makes it inherently indiscriminate, i.e., incapable of being used consistent with the principles of distinction and proportionality, then any use of that weapon system would be unlawful. C. The addition of autonomous functions to a weapon system can strengthen the implementation of ihl when these functions can be used to reduce the likelihood of harm to civilians and civilian objects. 2. Consistent with ihl, emerging technologies in the area of laws may be used to inform decision-making. A. When making a decision governed by ihl, commanders and other decisionmakers must make a good faith assessment of the information that is available to them at the time. B. Ihl generally does not prohibit commanders and other decision-makers from using tools to aid decision-making in armed conflict. Whether the use of a tool to aid decision-making in armed conflict is consistent with ihl may depend on the nature of the tool, the circumstances of its use, as well the applicable rules and duties under ihl. C. Reliance on a machine assessment to consider a target to be a military objective must be compatible with the decision-maker’s duty under ihl to exercise due regard to reduce the risk of harm to civilians and civilian objects. Such compatibility depends on the relevant circumstances ruling at the time, including: i. How accurately and consistently the machine performs in not mischaracterizing civilian objects as military objectives (i.e., false positives); ii. The decision-maker giving the machine assessment appropriate weight relative to other information relevant to whether the target was a military objective (e.g., operational context, intelligence reporting of the threat identified by the system); and unclassified – september 1, 2020 3 iii. The urgency to make a decision (e.g., whether the decision occurred in combat operations or in the face of an imminent threat of an attack, or whether more time could be taken before making a decision). 3. Consistent with ihl, weapons systems that autonomously select and engage targets may be used where the human operator has not expressly intended to strike a specific target or group of targets when activating the weapon system. A. The commander or operator could act consistently with the principle of distinction by: i. Using weapon systems that autonomously select and engage targets in areas that constitute military objectives; or ii. Using weapon systems that autonomously select and engage targets with the intent of making potential targets constituting military objectives (e.g., potential incoming projectiles in an active protection system) the object of attack, provided that the weapon systems perform with sufficient reliability (e.g., an active protection system consistently selecting and engaging incoming projectiles) to ensure that force is directed against such targets. B. The expected loss of civilian life, injury to civilians, and damage to civilian objects incidental to the employment of weapons systems that autonomously select and engage targets must not be excessive in relation to the concrete and direct military advantage expected to be gained. I. The expected loss of civilian life, injury to civilians and damage to civilian objects is to be informed by all available and relevant information, including information about: (i) the presence of civilians or civilian objects within the area and during the time when the weapon system is expected to be operating; (ii) the performance of the weapon’s autonomous functions in selecting and engaging military objectives; (iii) the risks posed to civilians and civilian objects when the weapon engages military objectives; (iv) the incidence of military objectives that could be engaged by the weapon system in the operational area; and (v) the effectiveness of any precautions taken to reduce the risk of harm to civilians and civilian objects. Ii. The concrete and direct military advantage expected to be gained is to be informed by all available and relevant information, which may include information about how the employment of the weapon system: (i) threatens military objectives belonging to the adversary; (ii) contributes to the security of the operating forces; (iii) diverts enemy resources and unclassified – september 1, 2020 4 attention; (iv) shapes or diverts the movement of enemy forces; and (v) supports military strategies and operational plans. C. Feasible precautions must be taken in use of weapon systems that autonomously select and engage targets to reduce the expected harm to civilians and civilian objects. Such precautions may include: i. Warnings (e.g., to potential civilian air traffic or notices to mariners); ii. Monitoring the operation of the weapon system; and iii. Activation or employment of self-destruct, self-deactivation, or selfneutralization mechanisms (e.g., use of rounds that self-destruct in flight or torpedoes that sink to the bottom if they miss their targets). (b) human responsibility for decisions on the use of weapons systems must be retained since accountability cannot be transferred to machines. This should be considered across the entire life cycle of the weapons system. This guiding principle reflects the fundamental importance of human responsibility in using machines. The gge should elaborate on guiding principle (b) by addressing how wellestablished international legal principles of state and individual responsibility apply to states and persons who use weapon systems with autonomous functions. Such work could inform practical measures to promote accountability for such decisions, addressed under guiding principle (d). The united states proposes the following conclusions for the gge’s consideration. 1. Under principles of state responsibility, every internationally wrongful act of a state, including such acts involving the use of emerging technologies in the area of laws, entails the international responsibility of that state.3 2. A state remains responsible for all acts committed by persons forming part of its armed forces, including any such use of emerging technologies in the area of laws, in accordance with applicable international law. 3. An individual, including a designer, developer, an official authorizing acquisition or deployment, a commander, or a system operator, is responsible for his or her decisions governed by ihl with regard to emerging technologies in the area of laws. 4. Under applicable international and domestic law, an individual remains responsible for his or her conduct in violation of ihl, including any such violations involving emerging technologies in the area of laws. The use of machines, including emerging 3 adapted from article 1 of the international law commission’s draft articles on responsibility of states for internationally wrongful acts. Unclassified – september 1, 2020 5 technologies in the area of laws, does not provide a basis for excluding legal responsibility. 5. The responsibilities of any particular individual in implementing a state or a party to a conflict’s obligations under ihl may depend on that person’s role in the organization or military operations, including whether that individual has the authority to make the decisions and judgments necessary to the performance of that duty under ihl. 6. Under ihl, a decision, including decisions involving emerging technologies in the area of laws, must be judged based on the information available to the decision-maker at the time and not on the basis of information that subsequently becomes available. 7. Unintended harm to civilians and other persons protected by ihl from accidents or equipment malfunctions, including those involving emerging technologies in the area of laws, is not a violation of ihl as such. 8. States and parties to a conflict have affirmative obligations with respect to the protection of civilians and other classes of persons under ihl, which continue to apply when emerging technologies in the area of laws are used. These obligations are to be assessed in light of the general practice of states, including common standards of the military profession in conducting operations. (c) human-machine interaction, which may take various forms and be implemented at various stages of the life cycle of a weapon, should ensure that the potential use of weapons systems based on emerging technologies in the area of lethal autonomous weapons systems is in compliance with applicable international law, in particular ihl. In determining the quality and extent of human-machine interaction, a range of factors should be considered including the operational context, and the characteristics and capabilities of the weapons system as a whole. This principle recognizes that human-machine interaction should ensure ihl compliance and as well as the need to consider human-machine interaction comprehensively, across the life cycle of the weapon system. The gge should elaborate on good practices in human-machine interaction that can strengthen compliance with ihl. The united states proposes the following conclusions on human-machine interaction for the gge’s consideration.4 1. Weapons systems based on emerging technologies in the area of laws should effectuate the intent of commanders and operators to comply with ihl, in particular, by avoiding 4 these and other u.s. Practices to ensure that the use of machines helps effectuate human intent are discussed in greater detail in the u.s. Working paper, human-machine interaction in the development, deployment and use of emerging technologies in the area of lethal autonomous weapons systems. These practices are reflected in u.s. Department of defense directive 3000.09, autonomy in weapon systems, november 21, 2012 (updated may 8, 2017), available at www.esd.whs.mil. Unclassified – september 1, 2020 6 unintended engagements and minimizing harm to civilians and civilian objects. This can be effectuated through the following measures: a. Weapons systems based on emerging technologies in the area of laws should be engineered to perform as anticipated. This should include verification and validation and testing and evaluation before fielding systems. B. Relevant personnel should properly understand weapons systems based on emerging technologies in the area of laws. Training, doctrine, and tactics, techniques, and procedures should be established for the weapon system. Operators should be certified by relevant authorities that they have been trained to operate the weapon system in accordance with applicable rules. C. User interfaces for weapons systems based on emerging technologies in the area of laws should be clear in order for operators to make informed and appropriate decisions in engaging targets. In particular, interface between people and machines for autonomous and semi-autonomous weapon systems should: (i) be readily understandable to trained operators; (ii) provide traceable feedback on system status; and (ii) provide clear procedures for trained operators to activate and deactivate system functions. (d) accountability for developing, deploying and using any emerging weapons system in the framework of the ccw must be ensured in accordance with applicable international law, including through the operation of such systems within a responsible chain of human command and control. This guiding principle recognizes that state and individual responsibility must be ensured through the effective implementation of accountability measures, including the military chain of command. Such implementation is an essential part of the responsible use of emerging technologies in the area of laws. The gge should elaborate on guiding principle (d) by articulating good practices to help ensure accountability. The united states proposes the following conclusions on human-machine interaction for the gge’s consideration. 1. The following general practices help ensure accountability in military operations, including operations involving the use of emerging technologies in the area of laws: a. Conducting operations under a clear operational chain of command. B. Subjecting members of the armed forces to a system of military law and discipline. C. Establishing and using procedures for the reporting of incidents involving potential violations. Unclassified – september 1, 2020 7 d. Conducting assessments, investigations, or other reviews of incidents involving potential violations. E. Disciplinary and punitive measures as appropriate. 2. The following practices with respect to the use of weapons systems, including those based on emerging technologies in the area of laws, can promote accountability: a. Rigorous testing of and training on the weapon system, so commanders and operators understand the likely effects of employing the weapon system. B. Establishing procedure and doctrine applicable to the use of the weapon system, which provide standards for commanders and operators on responsible use and under which they can be held accountable under the state’s domestic law. C. Using the weapon system in accordance with established training, doctrine, and procedures and refraining from unauthorized uses or modifications of the weapon system. (e) in accordance with states’ obligations under international law, in the study, development, acquisition, or adoption of a new weapon, means or method of warfare, determination must be made whether its employment would, in some or all circumstances, be prohibited by international law. This guiding principle reaffirms the principle in article 36 of the 1977 additional protocol i to the 1949 geneva conventions. The united states is not a party to the additional protocol i and does not regard article 36 as reflecting customary law, but engages in robust practice of conducting reviews of the legality of weapons. Such reviews are a good practice to facilitate the implementation of international law applicable to weapons and their use in armed conflict. “emerging technologies” are novel by definition and thus may be construed as “new” under this principle. The use of autonomy in weapon systems, however, is not necessarily new. There is substantial state practice in using autonomous functions and features in weapon systems for decades. In that light, the united states proposes the following good practices for the legal review of weapons systems based on emerging technologies in the area of laws for the gge’s consideration. 1. Legal advisers should be consulted regularly in the development or acquisition process as decisions that could pose legal issues are being made so that legal issues can be identified and more in-depth reviews can be conducted where necessary. A. A weapon system under modification should be reviewed to determine whether the modification poses any legal issues. Unclassified – september 1, 2020 8 b. New concepts for the employment of existing weapons should also be reviewed, when such concepts differ significantly from the intended uses that were considered when those systems were previously reviewed. 2. The nature of the legal review and advice should be tailored to the stage of the process of developing or acquiring the weapon. A. Providing legal advice early in the development or acquisition process allows ihl considerations to be taken into account early in the life cycle of the weapon. B. At the end of the development or acquisition process, formal legal opinions can memorialize relevant conclusions and analysis while also being useful to consider in subsequent reviews. 3. The legal review should consider the international law obligations applicable to the state intending to develop or acquire the weapon system, including prohibitions or other restrictions applicable to specific types of weapons, and whether the intended or expected uses of the weapon system can be consistent with those obligations under ihl. 4. The legal review should consider whether the weapon is illegal per se, i.e., whether the use of the weapon is prohibited in all circumstances. A. The legal review should consider whether the weapon is of a nature to cause superfluous injury or unnecessary suffering, or if it is inherently indiscriminate, or is otherwise incapable of being used in accordance with the requirements and principles of ihl. B. Analyzing whether a weapon is “inherently indiscriminate,” should consider whether the weapon is capable of being used in accordance with the principles of distinction and proportionality. C. In considering whether a weapon with new autonomous features or functions is consistent with the prohibitions against weapons calculated to cause superfluous injury or against weapons that are inherently indiscriminate, it may be useful to compare the weapon to existing weapons not falling under these prohibitions. 5. The legal review should advise those developing or acquiring the weapon system or its concepts of employment to consider potential measures to reduce the likelihood that use of the weapon will cause harm to civilians or civilian objects. 6. Persons conducting the legal review should understand the likely effects of employing the weapon in different operational contexts. Such expectation should be produced through realistic system developmental and operational test and evaluation. Unclassified – september 1, 2020 9 7. Bearing in mind national security considerations or commercial restrictions on proprietary information, states should share good practices on weapons reviews or legal reviews of particular weapons where appropriate. (f) when developing or acquiring new weapons systems based on emerging technologies in the area of lethal autonomous weapons systems, physical security, appropriate nonphysical safeguards (including cyber-security against hacking or data spoofing), the risk of acquisition by terrorist groups and the risk of proliferation should be considered. The responsible development and use of new weapons systems based on emerging technologies in the area of laws should consider a variety of issues, including those not addressed specifically by ihl. In u.s. Military practice, dod directive 3000.09 requires that in order to mitigate the potential consequences of an unintended engagement or loss of control of a system to unauthorized parties, “physical software and hardware will be designed with appropriate …] safeties, anti-tamper mechanisms, and information assurance …].”5 (g) risk assessments and mitigation measures should be part of the design, development, testing and deployment cycle of emerging technologies in any weapons systems. Risk assessments and mitigation measures are useful tools to address the uncertainty in the anticipated pace and trajectory of the future development of emerging technologies. Risk assessments allow for a weighing of the benefits of the emerging technologies against potential risks and allow for adjustments to be made as further research and development occurs. Risk assessments can also support the training of commanders and operators by helping them understand the function, capabilities, limitations, and likely effects of using a weapon system. The gge should build on the work reflected in paragraphs 23(a) and 23(b) of its 2019 report by further cataloging potential risks and mitigation measures that should be considered in the design, development, testing, and deployment of weapons systems based on emerging technologies in the area of laws. (h) consideration should be given to the use of emerging technologies in the area of lethal autonomous weapons systems in upholding compliance with ihl and other applicable international legal obligations. This principle recognizes that emerging technologies in the area of laws can be used to provide benefits, such as strengthening the implementation of ihl and reducing the incidence of civilian casualties and other tragic outcomes in armed conflict that may occur even when all parties have complied with the law. This principle should be implemented during legal reviews of new weapons, during the formulation of military strategies and plans, and during the planning and conduct of military operations. To facilitate such consideration and to encourage innovation that furthers the objects and purposes of the ccw, the gge should develop examples of specific practices that those 5 id. At paragraph 4(a)(2)(a). Unclassified – september 1, 2020 10 involved in these activities could consider. For example, the gge could begin this workstream by cataloging examples of ways in which emerging technologies in the area of laws could be used to reduce risks to civilians in military operations, such as by: 1. Incorporating autonomous self-destruct, self-deactivation, or self-neutralization mechanisms into munitions; 2. Increasing awareness of civilians and civilian objects on the battlefield; 3. Improving assessments of the likely effects of military operations; 4. Automating target identification, tracking, selection, and engagement to improve speed, precision, and accuracy; and 5. Reducing the need for immediate fires in self-defense.6 (i) in crafting potential policy measures, emerging technologies in the area of lethal autonomous weapons systems should not be anthropomorphized. Anthropomorphizing emerging technologies in the area of laws can lead to legal and technical misunderstandings that could be detrimental to the efficacy of potential policy measures. From a technical perspective, anthropomorphizing emerging technologies in the area of laws can lead to mis-estimating machine capabilities. From a legal perspective, anthropomorphizing emerging technologies in the area of laws can obscure the important point that ihl imposes obligations on states, parties to a conflict, and individuals, rather than machines. “smart” weapons cannot violate ihl any more than “dumb” weapons can. Similarly, machines are not intervening moral agents, and human beings do not escape responsibility for their decisions by using a weapon with autonomous functions. Anthropomorphizing emerging technologies in the area of laws could incorrectly suggest a diminished responsibility of human beings simply by the use of emerging technologies in the area of laws. (j) discussions and any potential policy measures taken within the context of the ccw should not hamper progress in or access to peaceful uses of intelligent autonomous technologies. Technology should not be stigmatized. Autonomy-related technologies, such as artificial intelligence (ai) and machine learning, have remarkable potential to improve the quality of human life with applications such as driverless cars and artificial assistants. The use of autonomy-related technologies can even save lives, for example, by improving the accuracy of medical diagnoses and surgical procedures or by reducing the risk of car accidents. Similarly, the potential for these technologies to save lives in armed conflict warrants close consideration, 6 these practices are discussed in the u.s. Working paper, humanitarian benefits of emerging technologies in the area of lethal autonomous weapon systems, march 28, 2018, ccw/gge.1/2018/wp.4. For a discussion of other potential humanitarian benefits, in addition to reducing the risk of civilian casualties in military operations, see paragraph 15 of the u.s. Working paper, implementing international humanitarian law in the use of autonomy in weapon systems, march 28, 2019, ccw/gge.1/2019/wp.5. Unclassified – september 1, 2020 11 including potential applications to help uphold ihl as reflected in guiding principle (h). As a result, research and development on autonomy-related technologies should not be restricted based on the rationale that such technologies could be used for weapons systems. Moreover, although the use of technologies for the purpose of violating international law, must be condemned, the use of autonomy-related technologies for defensive or other beneficial purposes should remain unhindered. (k) the ccw offers an appropriate framework for dealing with the issue of emerging technologies in the area of lethal autonomous weapons systems within the context of the objectives and purposes of the convention, which seeks to strike a balance between military necessity and humanitarian considerations. The united states strongly supports the ccw gge as the appropriate multilateral forum for states to address emerging technologies in the area of laws because states can use the gge to engage in a substantive, non-politicized dialogue on ihl issues. The gge allows states to send technical, legal, policy, and military experts as part of their delegations, submit working papers, and exchange state practice. The ccw gge is open to all states, includes states with relevant practice, and develops its reports by consensus. Civil society participants can observe the proceedings and participate in the discussions. The high contracting parties to the ccw have successfully put this framework to use in their consideration of emerging technologies in the areas of laws as reflected in gge’s substantive reports and the guiding principles.
Geneva, 9 - 13 april 2018 item 6 of the provisional agenda other matters humanitarian benefits of emerging technologies in the area of lethal autonomous weapon systems submitted by the united states of america i. Introduction using emerging technologies to limit civilian casualties poses an important humanitarian challenge 1. This working paper draws from existing state practice to identify potential humanitarian benefits of emerging technologies in the area of lethal autonomous weapons systems. 2. Civilian casualties are a tragic part of war. Although civilian casualties do not necessarily reflect a violation of international humanitarian law (ihl), protecting civilians from unnecessary suffering is one of the main purposes of ihl. Reducing civilian casualties promotes the objectives and purposes of the ccw, whose preamble recalls the “general principle of the protection of the civilian population against the effects of hostilities.” 3. Emerging autonomy-related technologies, such as artificial intelligence (ai) and machine learning, have remarkable potential to improve the quality of human life with applications such as driverless cars and artificial assistants. The use of autonomy-related technologies can even save lives, for example, by improving the accuracy of medical diagnoses and surgical procedures or by reducing the risk of car accidents.1 similarly, the potential for these technologies to save lives in armed conflict warrants close consideration. 4. In particular, the united states believes that discussion of the possible options for addressing the humanitarian and international security challenges posed by emerging technologies in the area of lethal autonomous weapons systems in the context of the objectives and purpose of the convention must involve consideration of how these technologies can be used to enhance the protection of the civilian population against the effects of hostilities. 5. This is especially the case because “smart” weapons that use computers and autonomous functions to deploy force more precisely and efficiently have been shown to reduce risks of harm to civilians and civilian objects. 1 https://singularityhub.com/2017/11/30/the-doctor-in-the-machine-how-ai-is-saving-lives-in-healthcare/; https://spectrum.ieee.org/the-human-os/biomedical/devices/in-fleshcutting-task-autonomous-robot-surgeon-beats-human-surgeons; http://theinstitute.ieee.org/ieee-roundup/blogs/blog/ai-can-save-lives-by-taking-over-these-important-tasks. Ccw/gge.1/2018/wp.4 group of governmental experts of the high contracting parties to the convention on prohibitions or restrictions on the use of certain conventional weapons which may be deemed to be excessively injurious or to have indiscriminate effects 28 march 2018 english only ccw/gge.1/2018/wp.4 2 6. The fundamental ihl principles of distinction and proportionality are consistent with military doctrines that are the basis for effective combat operations. Independent of legal considerations, sound military doctrine condemns the use of indiscriminate and excessive force as costly, inefficient, and a waste of scarce resources, and promotes discriminate uses of force as consistent with overarching strategic objectives. For example, in recent military operations to defeat terrorist groups in iraq, u.s. Commanders have sought to minimize the destruction to terrorist-controlled infrastructure in order to facilitate post-liberation reconstruction efforts. Military efforts to develop more precise and efficient weapons reflect a convergence between military effectiveness and humanitarian protection. 7. Existing state practice provides many examples of ways in which emerging technologies in the area of lethal autonomous weapons systems could be used to reduce risks to civilians: (1) incorporating autonomous self-destruct, self-deactivation, or self-neutralization mechanisms; (2) increasing awareness of civilians and civilian objects on the battlefield; (3) improving assessments of the likely effects of military operations; (4) automating target identification, tracking, selection, and engagement; and (5) reducing the need for immediate fires in self-defense. Ii. Autonomous self-destruct, self-deactivation, or self-neutralization mechanisms 8. Autonomous self-destruct, self-deactivation, or self-neutralization mechanisms can be used to reduce the risk of weapons causing unintended harm to civilians or civilian objects. These mechanisms are not necessarily new, but they have become more effective with advances in technology. 9. For example, the amended protocol ii to the convention recognizes that self-destruction or self-neutralization mechanisms can help ensure that a mine will no longer function as a mine when the mine no longer serves the military purpose for which it was emplaced. 10. Similarly, the hague viii convention relative to the laying of automatic submarine contact mines, october 18, 1907, also recognizes that naval mines and torpedoes should be constructed so as to become harmless after they have fulfilled their military purpose. 11. Although the united states is not a party to convention on cluster munitions and does not regard its prohibitions as reflecting customary international law, that instrument recognizes that electronic self-destruction mechanisms and electronic self-deactivating features in explosive submunitions that are designed to be dispersed or released from a conventional munition can help avoid indiscriminate area effects and the risks posed by unexploded submunitions. 12. Apart from mines and bombs employing submunitions, a number of weapons systems can use self-destructing ammunition, which automatically destroys the projectile after a period of time so that it poses less risk of inadvertently striking civilians and civilian objects. For example, anti-aircraft guns often use self-destructing rounds to reduce the risk that shots that miss the target cause damage upon falling to the ground.2 similarly, u.s. Army engineers have worked to develop .50 caliber ammunition that would automatically disassemble after a certain distance to reduce the risk of collateral damage.3 13. More sophisticated autonomous self-destruct, self-deactivation, or self-neutralization mechanisms could be applied to a broad range of weapons to reduce the risk that weapons may pose to civilians and civilian objects. For example, u.s. Department of defense policy provides that measures be taken to ensure that autonomous or semi-autonomous weapon systems “complete engagements in a timeframe consistent with commander and operator 2 https://www.gd-ots.com/wp-content/uploads/2017/11/20mm-m940-ammunition.pdf. 3 https://www.army.mil/article/162556/. Ccw/gge.1/2018/wp.4 3 intentions and, if unable to do so, to terminate engagements or seek additional human operator input before continuing the engagement.”4 iii. Increasing military awareness of civilians and civilian objects 14. Civilian casualties can result from a lack of awareness of the presence of civilians on the battlefield due to the “fog of war.” for example, commanders might be unaware that civilians are in or near a military objective. Similarly, commanders might, in good faith, misidentify civilians as combatants – particularly when engaged in armed conflict against armed groups that go to great lengths to hide among the civilian population. 15. Ai could help commanders increase their awareness of the presence of civilians and civilian objects on the battlefield by automating the processing and analysis of data. 16. One of the most ubiquitous and useful applications of ai has been to allow humans to search through large amounts of data to find relevant information quickly, such as through internet search engines. Companies are investing in ai to generate insights from “big data” with a view towards better serving their customers and increasing their profitability. 17. Commanders may similarly have an overwhelming amount of information to consider during military operations, such as hours of video from intelligence, surveillance, and reconnaissance (isr) platforms. 18. The u.s. Department of defense is engaged in an effort to use ai to improve its analysis of video from isr platforms.5 by using ai to identify objects of interest from imagery autonomously, analysts are able to search through larger quantities of data and focus on more sophisticated and important tasks requiring human judgment. 19. One of the results of improving the efficiency and accuracy of intelligence processes could be to increase commanders’ awareness of the presence of civilians, objects under special protection such as cultural property and hospitals, and other civilian objects. 20. This increased awareness could help commanders better assess the totality of the expected incidental loss of civilian life, injury to civilians, and damage to civilian objects from an attack, including incidental harms that otherwise would not have been foreseeable. This increased awareness could also help commanders identify and take additional precautions, including by identifying additional property or areas that should not be attacked or that would require additional review or higher-level approval before being attacked. Iv. Improving assessments of the likely effects of military operations 21. Ai could be used to improve the process of assessing the likely effects of weapons. 22. As part of targeting processes, military planners assess the likely effects of different types of weapon systems with a view toward minimizing collateral damage. 23. U.s. Planners regularly use software tools in planning military operations to assist in assessing the likely effects of weapons, such as estimating potential collateral damage. The use of software tools allows estimates that once took hours or days to be generated in minutes.6 4 dod directive 3000.09, autonomy in weapon systems, paragraph 4a(1)(b); see also enclosure 3, paragraph 1a(2). 5 https://www.defense.gov/news/article/article/1254719/project-maven-to-deploy-computer-algorithms-to-war-zone-by-years-end/. 6 http://www.defense-aerospace.com/article-view/feature/18894/usaf-plans-to-minimize-civilian-casualties.html; https://www.washingtonpost.com/archive/politics/2003/02/21/military-turns-to-software-to-cut-civilian-casualties/af3e06a3-e2b2-4258-b511-31a3425bde31/?utm_term=.ff1dd031bbda. Ccw/gge.1/2018/wp.4 4 24. More sophisticated computer modelling software could help military planners more accurately assess the presence of civilians or predict the likely effects that the weapon would create when striking the military objective. Assessments could be generated more quickly and more often, further reducing the risk of civilian casualties. 25. Much like with improved military awareness of civilians and civilian objects, improved assessments of the likely effects of military operations could help commanders better assess the totality of the expected incidental loss of civilian life, injury to civilians, and damage to civilian objects from an attack, including incidental harms that otherwise would not have been foreseeable. Improved assessments could also help commanders identify and take additional precautions, including by selecting weapons, aim points, and attack angles that reduce the risk of harm to civilians and civilian objects, while offering the same or superior military advantage in neutralizing or destroying a military objective. V. Automating target identification, tracking, selection, and engagement 26. Automated target identification, tracking, selection, and engagement functions can allow weapons to strike military objectives more accurately and with less risk of collateral damage. 27. Many types of missiles or bombs have “lock-on-after-launch” functions that allow the projectile to guide itself autonomously to targets after being launched by the human operator. The projectile has sensors that allow it to identify the target that the human operator intends to hit, and computers and guidance systems that allow it to select and engage that target. For example, the aim-120 advanced medium-range, air-to-air missile (amraam) incorporates an active radar in connection with an inertial reference unit and microcomputer system, which allows the missile to use its active radar to guide it to intercept its target.7 28. The use of munitions with guidance systems allows commanders to strike military objectives more accurately and with less risk of harm to civilians and civilian objects. Moreover, when the weapon is more accurate, fewer weapons need to be fired to create the same military advantage. A famous case study is of the thanh hoa bridge, which was a supply line used during the vietnam war, and which was targeted by hundreds of u.s. Sorties and bombs over seven years before being successfully destroyed by a handful of newly developed laser-guided bombs.8 29. Similarly, more accurate weapons can also allow for a smaller warhead to be used to generate the same military effect. For example, the gbu-53/b small diameter bomb increment ii (sdb ii) under development uses millimeter wave radar and imaging infrared sensors to find and identify targets, refine aim points, and guide the weapon to impact.9 the approximately 200 lb. Weapon includes capabilities for target search, classification, and prioritization, and its small warhead allows for targets to be hit with less risk of collateral damage. As another example, the dagr® missile also under development is designed to provide commanders the ability to strike high-value targets with less risk of collateral damage to civilians and friendly forces by delivering a 10 lb. Warhead within one meter of the intended target.10 7 http://www.navy.mil/navydata/fact_display.asp?cid=2200&tid=100&ct=2; http://www.navair.navy.mil/index.cfm?fuseaction=home.display&key=d3fac4ab-2e9f-4150-9664-6afbc83f203e; http://www.af.mil/about-us/fact-sheets/display/article/104576/aim-120-amraam/; https://www.raytheon.com/capabilities/products/amraam/. 8 https://media.defense.gov/2010/oct/13/2001330008/-1/-1/0/afd-101013-042.pdf; http://www.airforcemag.com/magazinearchive/documents/2011/august%202011/0811jaw.pdf. 9 http://www.airforcemag.com/sitecollectiondocuments/reports/2010/august%202010/day25/sdbii_factsheet_0810.pdf. 10 https://www.lockheedmartin.com/content/dam/lockheed/data/mfc/pc/dagr/mfc_dagr-pc.pdf. Ccw/gge.1/2018/wp.4 5 30. Automated targeting features are being developed for guns. For example, the common remotely operated weapon station (crows) is a stabilized mount for weapons on vehicles that contains a sensor suite and fire control software.11 crows features programmable target reference points for multiple locations, programmable sector surveillance scanning, automatic target ballistic lead, automatic target tracking, and programmable no-fire zones. A u.s. Company, trackingpoint, has recently developed a system that uses sensors and software to allow rifles to strike targets within one-half an inch at distances of one-half a mile.12 31. The addition of automated target detection and engagement functions can reduce the risk to civilians posed by weapons. As the convention on cluster munitions recognizes, the use of explosive submunitions designed to detect and engage a single target object can be used to avoid indiscriminate area effects and the risks posed by unexploded submunitions. For example, the cbu-105 (sensor-fuzed weapon) uses submunitions with advanced sensors to target precisely and engage enemy tanks and armored vehicles, rather than dispersing its submunitions in an unguided fashion. Vi. Reducing civilian casualties from the immediate use of force in self-defense 32. Emerging technologies could reduce risk of civilian casualties from the immediate use of force in self-defense. 33. Civilians are at increased risk in situations in which military forces are in contact with the enemy and respond to enemy fires in self-defense. In those operational situations, the imperative to take immediate action to counter a threat from the enemy reduces the time available to take precautions to reduce the risk of civilian casualties. 34. Existing practice, however, suggests that emerging technologies may offer a number of ways to reduce civilian casualties as a result of such engagements. 35. First, the use of robotic and autonomous systems can reduce the need for immediate self-defense fires by reducing the exposure of human beings to hostile fire.13 for example, remotely piloted aircraft or ground robots have been used to scout ahead of forces conducting patrols in environments where they might be surprised by enemy ambushes or roadside bombs. Robotic and autonomous systems can provide a greater standoff distance from enemy formations, allowing forces to exercise tactical patience to reduce the risk of civilian casualties. 36. Second, technologies to identify automatically the direction and location of incoming fire can reduce the risk of misidentifying the location or source of enemy fire. For example, the lightweight counter mortar radar can identify indirect fire threats by automatically detecting and tracking shells and backtracking to the position of the weapon that fired the shell. Similarly, the “boomerang” detection system uses microphones and computers to identify the source of incoming gunfire in less than a second.14 37. Third, the use of defensive autonomous weapons, such as the counter-rocket, artillery, mortar (c-ram) intercept land-based phalanx weapon system, used to counter incoming rockets, mortars, and artillery, can provide additional time to develop a considered response to an enemy threat. 11 http://www.dote.osd.mil/pub/reports/fy2012/pdf/army/2012crows.pdf; https://en.wikipedia.org/wiki/crows. 12 https://www.tracking-point.com/technology/how-it-works/. 13 http://www.tradoc.army.mil/frontpagecontent/docs/ras_strategy.pdf. 14 https://www.raytheon.com/capabilities/products/boomerang. Ccw/gge.1/2018/wp.4 6 vii. Conclusion – entirely new capabilities 38. The examples that we have discussed help illustrate the potential of emerging technologies in the area of lethal autonomous weapons systems to reduce the risk of civilian casualties and damage to civilian objects, but it is important to recall that technology is often applied in innovative ways that are wholly unlike previous applications. 39. Emerging technologies in the area of lethal autonomous weapons systems could be used to create entirely new capabilities that would increase the ability of states to reduce the risk of civilian casualties in applying force. 40. Rather than trying to stigmatize or ban such emerging technologies in the area of lethal autonomous weapon systems, states should encourage such innovation that furthers the objectives and purposes of the convention.
Introduction and overview 1. This u.s. Working paper addresses the implementation of international humanitarian law (ihl) in the use of autonomy in weapon systems. The paper builds on u.s. Working papers submitted in 2017 and 2018.1 specifically, this working paper discusses: ihl requirements and autonomous functions in weapon systems; steps that states can take to help implement ihl requirements; and the potential for emerging technologies in the area of lethal autonomous weapon systems (laws) to strengthen implementation of ihl. 2. In summary, this paper offers the following main conclusions: (a) existing ihl, including the requirements of distinction, proportionality, and precaution, provides a comprehensive framework to govern the use of autonomy in weapon systems. (b) internal procedures for review and testing, including the legal review of weapons, are essential to implementing ihl requirements. (c) emerging technologies in the area of laws could strengthen the implementation of ihl, by, inter alia, reducing the risk of civilian casualties, facilitating the investigation or reporting of incidents involving potential violations, enhancing the ability to implement corrective actions, and automatically generating information on unexploded ordnance. 1 u.s. Working papers to the ccw gge include: human-machine interaction in the development, deployment and use of emerging technologies in the area of lethal autonomous weapons systems, aug. 28, 2018, ccw/gge.2/2018/wp.4; humanitarian benefits of emerging technologies in the area of lethal autonomous weapon systems, march 28, 2018 ccw/gge.1/2018/wp.4; autonomy in weapon systems, nov. 10, 2017, ccw/gge.1/2017/wp.6; characteristics of lethal autonomous weapons systems, nov. 10, 2017, ccw/gge.1/2017/wp.7. Ccw/gge.1/2019/wp.5 convention on prohibitions or restrictions on the use of certain conventional weapons which may be deemed to be excessively injurious or to have indiscriminate effects 28 march 2019 english only ccw/gge.1/2019/wp.5 2 ii. Ihl requirements and autonomous functions in weapon systems 3. Although a wide range of ihl requirements could be implicated by the use of autonomous weapon systems,2 this paper seeks to focus on ihl issues distinctly presented by the use of autonomous functions in weapon systems to select and engage targets.3 the following ihl requirements are of particular relevance: (a) distinction. “combatants may make military objectives the object of attack, but may not direct attacks against civilians, civilian objects, or other protected persons and objects.”4 (b) proportionality. “combatants must refrain from attacks in which the expected loss of life or injury to civilians, and damage to civilian objects incidental to the attack, would be excessive in relation to the concrete and direct military advantage expected to be gained.”5 (c) precaution. “combatants must take feasible precautions in planning and conducting attacks to reduce the risk of harm to civilians and other persons and objects protected from being made the object of attack.”6 4. In considering these requirements, it may be helpful first to recall: (a) these requirements impose duties on combatants (and, more broadly, on parties to conflict), and do not impose duties on machines. (b) these requirements address “attacks,” rather than the firing or activation of weapon systems as such. For example, the single firing of a weapon system might only be one part of an “attack,” and the mere activation of a weapon system might not constitute an “attack” at all. (c) these requirements are implemented in military operations through responsible commands, and not every duty will be implemented by every individual within the command. For example, the decision about whether a particular precaution is feasible might be made by a commander at a particular level of command with the authority to direct the resources necessary to take that precaution, or individual units within the command might be tasked with carrying out a precaution, such as delivering a warning. (d) although the implementation of these requirements through the chain of command necessarily will entail different duties being fulfilled by different individuals within the command, all combatants, when prosecuting attacks against military objectives, must exercise due regard to reduce the risk of incidental harm to the civilian population and other persons and objects that may not be made the object of attack. This standard of due regard must be assessed based on the general practice of states and common standards of the military profession in conducting operations. 5. As a threshold matter, how ihl requirements are implicated could depend on how the commander or operator is using the autonomous functions in the weapon system. Based on past practice in using autonomy in weapon systems, we can envision at least three general scenarios for the use of autonomous functions in weapon systems.7 2 the definition of “autonomous weapon system” provided in u.s. Department of defense directive 3000.09, autonomy in weapon systems, is one that “once activated can select and engage targets without further intervention by a human operator.” 3 for example, this paper is not intended to focus on the prohibition on killing or wounding by resort to perfidy or the requirements of the ccw amended protocol ii, although it is possible that the use of autonomous weapon systems could present such issues depending on the nature of the systems and intended use of the weapon systems. 4 u.s. Department of defense (dod) law of war manual 5.4.2. 5 dod law of war manual 5.4.2. 6 dod law of war manual 5.4.2. 7 because emerging technologies continue to be developed; there could be novel uses of emerging technologies that would not be reflected in these scenarios. Ccw/gge.1/2019/wp.5 3 (a) first, a weapon system’s autonomous function could be used to effectuate more accurately and reliably a commander or operator’s intent to strike a specific target or a specific target group. For example, the operator identifies an enemy surface-to-air missile system and fires a missile at it. Rather than only being guided by the operator’s aiming of the missile at the target, the missile also has sensors and computers that provide it the capability to recognize enemy surface-to-air missile systems (e.g., through detection of electromagnetic emissions of the enemy surface-to-air missile system and comparison with an onboard “library” of such emission “signatures”), and, after being fired, the missile automatically identifies, acquires, and guides itself to the target that the operator intended to strike. (b) second, a weapon system’s autonomous functions could inform a commander or operator’s decision-making about what targets he or she intends to strike. For example, the computers and sensors on the weapon system could generate an assessment of a potential target that the operator would consider along with other relevant information (e.g., the operational context in which the weapon is deployed) in deciding whether to engage a target. These types of computers and sensors also could, in principle, be distinct from the weapon system used to engage a target. For example, counter-battery radar systems are used to identify the location from which incoming rockets, artillery, and mortars were launched, which is used to direct counter-battery fire by an artillery system. (c) third, a weapon system’s autonomous function could be used by a commander or operator to select and engage specific targets that the commander or operator did not know of when he or she activated the weapon system. For example, a commander might assess that there is a general risk of enemy missile or rocket attacks against a given location or against a given unit or platform, but the commander might not know of a specific incoming missile or rocket attack. In order to protect that location, unit, or platform, the commander might direct the activation of a weapon system, such as an active protection system, that would select and engage incoming projectiles automatically if such an attack occurs. Similarly, a commander might assess a risk that enemy tanks will attack and deploy a field of anti-tank mines at a strategic location, such as at a mountain pass, to counter that potential threat. 6. In the first scenario described in paragraph 5.a., the analysis of ihl requirements proceeds almost identically as in a case of use of a “dumb” weapon without the autonomous function because the weapon is being used no differently. The addition of autonomous functions, however, is intended to enhance the effectiveness of the weapon system by making it more accurate and precise in striking military objectives. (a) the addition of autonomous functions would render the use of the missile illegal if the autonomous functions, contrary to the intention of making the weapon more precise and accurate, actually made the missile inherently indiscriminate, i.e., incapable of being used consistent with the principles of distinction and proportionality. On the other hand, it would be reasonable to rely on the autonomous function in the missile to identify, acquire, and guide itself to the target or target group, to the degree that the autonomous function performed accurately and consistently in selecting and engaging the correct targets. For example, if testing indicated that addition of the autonomous function to an already lawful missile system served only to improve its accuracy in striking military objectives, then it clearly would be appropriate to rely on the autonomous functions in the weapon system because ihl does not prohibit increasing the precision of weapons. (b) this first scenario illustrates that if weapons systems with autonomy in targeting functions are used in the same way as weapon systems lacking such capabilities (i.e., to strike a specific target while complying with ihl requirements), they do not seem to present new issues of ihl compliance. Moreover, when such a reliable autonomous function is available, the use of such a weapon could also be deemed a feasible precaution to reduce the risk of civilian casualties. 7. The second scenario, described in paragraph 5.b., presents a general issue of when it is permissible or appropriate to rely on autonomous functions to aid in decision-making in armed conflict. Ccw/gge.1/2019/wp.5 4 (a) armed conflict, and combat operations in particular, takes place in a difficult decision-making environment – often referred to as the “fog of war.” the facts may be difficult to discern due to the efforts by the adversary to deceive as well as the stress and chaos accompanying combat operations, including the constant threat of attack by the adversary. Recognizing that information during armed conflict may be imperfect or lacking, commanders and other decision-makers must make a good faith assessment of the information that is available to them at the time when conducting military operations.8 (b) an autonomous function that was intended to provide probative information that increased the accuracy of decision-making, such as the counter-battery radar system, would generally be a permissible tool for the commander or weapon system operator to consider as one of the available sources of information in making decisions. (c) more specifically, whether it were permissible under ihl to rely on an automated assessment to consider a target to be a military objective would depend on whether such reliance was consistent with the exercise of due regard to reduce the risk of incidental harm to the civilian population and other persons and objects that may not be made the object of attack. This would depend, inter alia, on: i) an understanding of how accurately and consistently the machine performs in not mischaracterizing civilian objects as military objectives; ii) the operator giving the automated assessment appropriate weight relative to other information that would be probative of whether the target, in fact, was a military objective; and iii) the urgency to make a decision. For example, if the automated assessment has a very low rate of “false positives” (even relative to the “false positive” rate for a person making such an assessment), the operational context corroborated the automated assessment (e.g., intelligence reporting indicated the possibility of the threat identified by the system), and the context involved combat operations, then it would seem to be reasonable to rely on the assessment to conclude that the target was a military objective and, provided other ihl requirements were met, to strike the target. On the other hand, if the system performed with a significant rate of “false positives,” there was no need for a rapid decision, and the contextual factors contradicted the automated assessment, then it would not seem to be reasonable for a person to rely on the automated assessment to conduct a strike immediately, rather than first seeking further information. 8. The use of autonomous functions in a weapon system in the third scenario, described in paragraph 5.c., to select and engage targets may also be consistent with the requirements of distinction, proportionality, and precaution. That is, although the commander or operator would not expressly intend to strike a specific target or target group when activating the weapon system, it may be possible that the weapon system nonetheless could be operated consistent with these ihl requirements. By way of comparison, anti-tank mines similarly may be used consistent with ihl without an express intention at the time of emplacing or activating the mines that the mines detonate against a specific tank. (a) the commander or operator could act consistently with the principle of distinction by having the intention of making potential military objectives (e.g., the potential incoming projectiles in the active protection system mentioned above) the object of attack and provided that the autonomous functions in the weapon system perform with sufficient reliability (e.g., consistently selecting and engaging the incoming projectiles) to ensure that force can be directed against the potential military objectives. Alternatively, if the commander or operator identified an area that constituted a military objective, the weapon system could be deployed against that area, for example, to divert enemy forces from that area. (b) the commander or operator could act consistently with the principle of proportionality by assessing that the risk of civilian casualties from the activation of the weapon would not be excessive in relation to the military advantage expected to be gained. An assessment of the risk of civilian casualties could be informed by a variety of factors, including any precautions taken to reduce that risk. For example, if the weapon’s autonomous function performed accurately and reliably to fire only against incoming projectiles and used rounds that self-destructed in flight to reduce the potential for the 8 dod law of war manual 5.3 (june 2015, updated december 2016). Ccw/gge.1/2019/wp.5 5 round to cause harm if it missed the target, then the commander could be able to assess the risk of civilian casualties to be minimal and not excessive compared to the military advantage expected to be gained from activating the system. Warnings to potential civilian air traffic and monitoring the operation of the weapon system could also be important precautions to take that would reduce the risk of civilian casualties and ensure that the activation of the system would not be excessive. (c) even if the risk of civilian casualties was not expected to be excessive in relation to the military advantage expected to be gained, it would be important to take further feasible precautions. For example, warnings, monitoring, and self-destruct, self-deactivation, or self-neutralization mechanisms are all precautions that have been usefully employed to reduce the risk of civilian casualties in relation to the use of land or naval mines. These precautions also should be considered in relation to new types of autonomous weapon systems that would be employed in a similar fashion. Moreover, emerging technologies might afford even more capabilities to reduce the risk of civilian casualties. For example, a new type of system that could autonomously target enemy forces entering an area could be more discriminate than existing lawful means of securing that military objective, such as emplacing a marked and monitored minefield or providing interdiction fires through artillery. In such a case, use of the autonomous system might even be regarded as an additional precaution that should be taken, consistent with ihl. Iii. Practical measures to help implement ihl requirements 9. Considering these scenarios helps highlight the importance of practical measures to implement ihl requirements in respect of autonomous functions in weapon systems. These practical measures include: 1) rigorous testing to assess system performance and reliability; 2) establishing doctrine, training, and procedures to ensure that weapons are used in accordance with how they have been designed, tested, and reviewed; and 3) the legal review of weapons prior to their use. 10. Rigorous testing to assess weapon system performance and reliability supports compliance with ihl. For example, in the first scenario described in paragraph 5.a., if the autonomous function performs erratically, randomly engaging civilian objects rather than the intended military targets, then the weapon would appear to be prohibited as inherently indiscriminate. Similarly, in the second scenario described in paragraph 5.b., the degree to which the autonomous function misclassifies civilian objects as military objectives would be a significant factor in whether it would be reasonable to rely on the assessment as the basis for conducting a strike. States have every incentive to develop reliable systems; rigorous testing of systems reflects a strong convergence of military and humanitarian interests. 11. Establishing doctrine, training, and procedures for the weapon system is another important mechanism that helps ensure that the weapon is used consistent with ihl. What ihl considerations are salient could depend on how the weapon is to be used. If a weapon system was developed with a particular concept of employment in mind (e.g., scenario 1 described in paragraph 5.a.), it might create unanticipated problems if the weapon were used in ways not contemplated by those who developed and tested the weapon system (e.g., scenario 3 described in paragraph 5.c.). 12. The legal review of weapons prior to their use enables the state developing or acquiring the weapons to consider relevant ihl issues, including precautions to reduce the risk of civilian casualties. The legal review of the weapon also affords an opportunity to ensure that designers and developers of the weapon system and others tasked with ensuring the reliability of the weapon system have applied their expertise. Similarly, the legal review of the weapon also provides a mechanism for reviewing doctrine, training, and procedures for the weapon system and considering additional doctrine, training, and procedures that would help ensure the weapon is used consistent with ihl. Ccw/gge.1/2019/wp.5 6 iv. Potential for emerging technologies in the area of laws to strengthen implementation of ihl requirements 13. One of the guiding principles for the gge’s work is that “c]onsideration should be given to the use of emerging technologies in the area of lethal autonomous weapons systems in upholding compliance with ihl and other applicable international legal obligations.”9 thus, it is important to consider how emerging technologies in the area of laws can strengthen implementation of ihl requirements. 14. As the united states has noted in its working paper on humanitarian benefits of emerging technologies in the area of lethal autonomous weapon systems submitted at the august 2018 gge meeting, new advancements in autonomy in weapon systems hold great promise for strengthening the implementation of ihl. For example, as also discussed in this paper, autonomous functions could be used to make weapons more precise and increase the accuracy of human decision-making in stressful and time-critical situations. 15. In addition, emerging technologies in the area of laws could have benefits that extend beyond simply reducing the risk of civilian casualties in military operations. (a) for example, emerging technologies in the area of laws could strengthen efforts to ensure accountability over the use of force by having system logs that automatically record the operation of the weapon system. This kind of recording could facilitate investigations of both the weapon system’s performance and use. (b) automated systems also could identify incidents meriting further review or investigation. By way of comparison, some banks, credit card companies, and other financial institutions use automated systems to identify suspicious activity and potentially fraudulent transactions. Weapons systems with autonomous functions could similarly be programmed with reporting mechanisms to highlight unusual uses meriting further review. (c) the use of software control systems in weapon systems also creates the possibility of improving the weapon system’s ability to avoid civilian casualties through updates to the software controlling the weapon system. (d) automated tracking systems could assist in the tracking of unexploded ordnance and fulfilling associated responsibilities under the ccw protocol v on explosive remnants of war. For example, a weapon system that automatically tracked its own fires could identify and record the location where its ordnance did not explode as intended, thereby facilitating the clearance of explosive remnants of war. 9 report of the 2018 session of the group of governmental experts on emerging technologies in the area of lethal autonomous weapons systems, oct. 23, 2018, ccw/gge.1/2018/3. U.s. Proposals on aspects of the normative and operational framework submitted by the united states of america 1. As described in the discussion paper submitted by australia, canada, japan, the republic of korea, the united kingdom, and the united states, the convention on certain conventional weapons (ccw) group of governmental experts (gge) on emerging technologies in the area of lethal autonomous weapons systems (laws) has already reached a significant number of consensus conclusions under the four elements that numerous delegations have proposed to serve as the focus for organizing the gge’s consensus recommendations in relation to the clarification, consideration and development of aspects of the normative and operational framework on emerging technologies in the area of lethal autonomous weapons systems. 2. The united states believes that the gge can accomplish even more in these four areas, and this paper provides u.s. Proposals for further conclusions under each element: (a) application of international humanitarian law (ihl); (b) human responsibility; (c) human-machine interaction; and (d) weapons reviews. 3. These proposals are also available in the u.s. Commentaries on the guiding principles adopted by the ccw gge, which the united states submitted in 2020. A. U.s. Proposals on the application of ihl clarifying how ihl requirements apply to three general scenarios for the use of autonomous functions in weapon systems: 1) homing munitions that involve autonomous functions; 2) decision support tools that that can inform decision-making about targeting; and 3) relying on autonomous functions in weapon systems to select and engage targets 4. Consistent with ihl, autonomous functions may be used to effectuate more accurately and reliably a commander or operator’s intent to strike a specific target or target group. (a) the addition of autonomous functions, such as the automation of target selection and engagement, to weapon systems can make weapons more precise and accurate in striking military objectives by allowing weapons or munitions to “home in” on targets selected by a human operator. Ccw/gge.1/2021/wp.3 convention on prohibitions or restrictions on the use of certain conventional weapons which may be deemed to be excessively injurious or to have indiscriminate effects 27 september 2021 english only ccw/gge.1/2021/wp.3 2 (b) if the addition of autonomous functions to a weapon system makes it inherently indiscriminate, i.e., incapable of being used consistent with the principles of distinction and proportionality, then any use of that weapon system would be unlawful. (c) the addition of autonomous functions to a weapon system can strengthen the implementation of ihl when these functions can be used to reduce the likelihood of harm to civilians and civilian objects. 5. Consistent with ihl, emerging technologies in the area of laws may be used to inform decision-making. (a) when making a decision governed by ihl, commanders and other decision-makers must make a good faith assessment of the information that is available to them at the time. (b) ihl generally does not prohibit commanders and other decision-makers from using tools to aid decision-making in armed conflict. Whether the use of a tool to aid decision-making in armed conflict is consistent with ihl may depend on the nature of the tool, the circumstances of its use, as well the applicable rules and duties under ihl. (c) reliance on a machine assessment to consider a target to be a military objective must be compatible with the decision-maker’s duty under ihl to exercise due regard to reduce the risk of harm to civilians and civilian objects. Such compatibility depends on the relevant circumstances ruling at the time, including: i. How accurately and consistently the machine performs in not mischaracterizing civilian objects as military objectives (i.e., false positives); ii. The decision-maker giving the machine assessment appropriate weight relative to other information relevant to whether the target was a military objective (e.g., operational context, intelligence reporting of the threat identified by the system); and iii. The urgency to make a decision (e.g., whether the decision occurred in combat operations or in the face of an imminent threat of an attack, or whether more time could be taken before making a decision). 6. Consistent with ihl, weapons systems that autonomously select and engage targets may be used where the human operator has not expressly intended to strike a specific target or group of targets when activating the weapon system. (a) the commander or operator could act consistently with the principle of distinction by: i. Using weapon systems that autonomously select and engage targets in areas that constitute military objectives; or ii. Using weapon systems that autonomously select and engage targets with the intent of making potential targets constituting military objectives (e.g., potential incoming projectiles in an active protection system) the object of attack, provided that the weapon systems perform with sufficient reliability (e.g., an active protection system consistently selecting and engaging incoming projectiles) to ensure that force is directed against such targets. (b) the expected loss of civilian life, injury to civilians, and damage to civilian objects incidental to the employment of weapons systems that autonomously select and engage targets must not be excessive in relation to the concrete and direct military advantage expected to be gained. I. The expected loss of civilian life, injury to civilians, and damage to civilian objects is to be informed by all available and relevant information, including information about: (i) the presence of civilians or civilian objects within the area and during the time when the weapon system is expected to be operating; (ii) the performance of the weapon’s autonomous functions in selecting and engaging military objectives; (iii) the risks posed to civilians and civilian objects when the weapon engages military objectives; (iv) the incidence of military objectives that ccw/gge.1/2021/wp.3 3 could be engaged by the weapon system in the operational area; and (v) the effectiveness of any precautions taken to reduce the risk of harm to civilians and civilian objects. Ii. The concrete and direct military advantage expected to be gained is to be informed by all available and relevant information, which may include information about how the employment of the weapon system: (i) threatens military objectives belonging to the adversary; (ii) contributes to the security of the operating forces; (iii) diverts enemy resources and attention; (iv) shapes or diverts the movement of enemy forces; and (v) supports military strategies and operational plans. (c) feasible precautions must be taken in use of weapon systems that autonomously select and engage targets to reduce the expected harm to civilians and civilian objects. Such precautions may include: i. Warnings (e.g., to potential civilian air traffic or notices to mariners); ii. Monitoring the operation of the weapon system; and iii. Activation or employment of self-destruct, self-deactivation, or self-neutralization mechanisms (e.g., use of rounds that self-destruct in flight or torpedoes that sink to the bottom if they miss their targets). Examples of ways in which emerging technologies in the area of laws could be used to reduce the risks to civilians in military operations 7. Ccw gge guiding principle (h) (“consideration should be given to the use of emerging technologies in the area of lethal autonomous weapons systems in upholding compliance with ihl and other applicable international legal obligations”) should be implemented during legal reviews of new weapons, during the formulation of military strategies and plans, and during the planning and conduct of military operations. To facilitate such consideration and to encourage innovation that furthers the objects and purposes of the ccw, the gge should develop examples of specific practices that those involved in these activities could consider. For example, the gge could begin this workstream by cataloging examples of ways in which emerging technologies in the area of laws could be used to reduce risks to civilians in military operations, such as by: • incorporating autonomous self-destruct, self-deactivation, or self-neutralization mechanisms into munitions; • increasing awareness of civilians and civilian objects on the battlefield; • improving assessments of the likely effects of military operations; • automating target identification, tracking, selection, and engagement to improve speed, precision, and accuracy; and • reducing the need for immediate fires in self-defense.1 1 these practices are discussed in the u.s. Working paper, humanitarian benefits of emerging technologies in the area of lethal autonomous weapon systems, march 28, 2018, ccw/gge.1/2018/wp.4. For a discussion of other potential humanitarian benefits, in addition to reducing the risk of civilian casualties in military operations, see paragraph 15 of the u.s. Working paper, implementing international humanitarian law in the use of autonomy in weapon systems, march 28, 2019, ccw/gge.1/2019/wp.5. Ccw/gge.1/2021/wp.3 4 b. U.s. Proposals on human responsibility legal responsibility (a) under principles of state responsibility, every internationally wrongful act of a state, including such acts involving the use of emerging technologies in the area of laws, entails the international responsibility of that state.2 (b) a state remains responsible for all acts committed by persons forming part of its armed forces, including any such use of emerging technologies in the area of laws, in accordance with applicable international law. (c) an individual, including a designer, developer, an official authorizing acquisition or deployment, a commander, or a system operator, is responsible for his or her decisions governed by ihl with regard to emerging technologies in the area of laws. (d) under applicable international and domestic law, an individual remains responsible for his or her conduct in violation of ihl, including any such violations involving emerging technologies in the area of laws. The use of machines, including emerging technologies in the area of laws, does not provide a basis for excluding legal responsibility. (e) the responsibilities of any particular individual in implementing a state or a party to a conflict’s obligations under ihl may depend on that person’s role in the organization or military operations, including whether that individual has the authority to make the decisions and judgments necessary to the performance of that duty under ihl. (f) under ihl, a decision, including decisions involving emerging technologies in the area of laws, must be judged based on the information available to the decision-maker at the time and not on the basis of information that subsequently becomes available. (g) unintended harm to civilians and other persons protected by ihl from accidents or equipment malfunctions, including those involving emerging technologies in the area of laws, is not a violation of ihl as such. (h) states and parties to a conflict have affirmative obligations with respect to the protection of civilians and other classes of persons under ihl, which continue to apply when emerging technologies in the area of laws are used. These obligations are to be assessed in light of the general practice of states, including common standards of the military profession in conducting operations. Accountability practices 8. The following general practices help ensure accountability in military operations, including operations involving the use of emerging technologies in the area of laws: (a) conducting operations under a clear operational chain of command. (b) subjecting members of the armed forces to a system of military law and discipline. (c) establishing and using procedures for the reporting of incidents involving potential violations. (d) conducting assessments, investigations, or other reviews of incidents involving potential violations. (e) disciplinary and punitive measures as appropriate. 2 adapted from article 1 of the international law commission’s draft articles on responsibility of states for internationally wrongful acts. Ccw/gge.1/2021/wp.3 5 9. The following practices with respect to the use of weapons systems, including those based on emerging technologies in the area of laws, can promote accountability: (a) rigorous testing of and training on the weapon system, so commanders and operators understand the likely effects of employing the weapon system. (b) establishing procedure and doctrine applicable to the use of the weapon system, which provide standards for commanders and operators on responsible use and under which they can be held accountable under the state’s domestic law. (c) using the weapon system in accordance with established training, doctrine, and procedures and refraining from unauthorized uses or modifications of the weapon system. C. U.s. Proposals on human-machine interaction 10. Weapons systems based on emerging technologies in the area of laws should effectuate the intent of commanders and operators to comply with ihl, in particular, by avoiding unintended engagements and minimizing harm to civilians and civilian objects. This can be effectuated through the following measures: (a) weapons systems based on emerging technologies in the area of laws should be engineered to perform as anticipated. This should include verification and validation and testing and evaluation before fielding systems. (b) relevant personnel should properly understand weapons systems based on emerging technologies in the area of laws. Training, doctrine, and tactics, techniques, and procedures should be established for the weapon system. Operators should be certified by relevant authorities that they have been trained to operate the weapon system in accordance with applicable rules. (c) user interfaces for weapons systems based on emerging technologies in the area of laws should be clear in order for operators to make informed and appropriate decisions in engaging targets. In particular, the interface between people and machines for autonomous and semi-autonomous weapon systems should: (i) be readily understandable to trained operators; (ii) provide traceable feedback on system status; and (ii) provide clear procedures for trained operators to activate and deactivate system functions. D. U.s. Proposals on weapons reviews guidelines and good practices for militaries to consider using in conducting legal reviews of weapons systems based on emerging technologies in the area of laws 11. Legal advisers should be consulted regularly in the development or acquisition process as decisions that could pose legal issues are being made so that legal issues can be identified and more in-depth reviews can be conducted where necessary. (a) a weapon system under modification should be reviewed to determine whether the modification poses any legal issues. (b) new concepts for the employment of existing weapons should also be reviewed, when such concepts differ significantly from the intended uses that were considered when those systems were previously reviewed. 12. The nature of the legal review and advice should be tailored to the stage of the process of developing or acquiring the weapon. (a) providing legal advice early in the development or acquisition process allows ihl considerations to be taken into account early in the life cycle of the weapon. Ccw/gge.1/2021/wp.3 6 (b) at the end of the development or acquisition process, formal legal opinions can memorialize relevant conclusions and analysis while also being useful to consider in subsequent reviews. 13. The legal review should consider the international law obligations applicable to the state intending to develop or acquire the weapon system, including prohibitions or other restrictions applicable to specific types of weapons, and whether the intended or expected uses of the weapon system can be consistent with those obligations under ihl. 14. The legal review should consider whether the weapon is illegal per se, i.e., whether the use of the weapon is prohibited in all circumstances. (a) the legal review should consider whether the weapon is of a nature to cause superfluous injury or unnecessary suffering, or if it is inherently indiscriminate, or is otherwise incapable of being used in accordance with the requirements and principles of ihl. (b) analyzing whether a weapon is “inherently indiscriminate,” should consider whether the weapon is capable of being used in accordance with the principles of distinction and proportionality. (c) in considering whether a weapon with new autonomous features or functions is consistent with the prohibitions against weapons calculated to cause superfluous injury or against weapons that are inherently indiscriminate, it may be useful to compare the weapon to existing weapons not falling under these prohibitions. 15. The legal review should advise those developing or acquiring the weapon system or its concepts of employment to consider potential measures to reduce the likelihood that use of the weapon will cause harm to civilians or civilian objects. 16. Persons conducting the legal review should understand the likely effects of employing the weapon in different operational contexts. Such expectation should be produced through realistic system developmental and operational test and evaluation. 17. Bearing in mind national security considerations or commercial restrictions on proprietary information, states should share good practices on weapons reviews or legal reviews of particular weapons where appropriate.
Intervention by the united states karl chang • thank you, chair. • in the spirit of interactive dialogue, i wanted to respond to a few of the comments made by other states this morning. • we do not agree with the suggestion by some delegations that “human control” is agreed as a shared frame for understanding human-machine interaction, and have previously indicated - and reiterate again today - that we do not believe that such a principle helps improve our collective understanding of risks and benefits related to laws and how technology can be used to reduce suffering during war. As we have explained before, we think the notion of autonomous weapons being under human control to be an overly simplistic construct that fails to capture the various human touchpoints throughout the life-cycle of the weapon that can be used to ensure compliance with ihl • for example, an operator might be able to exercise technical control over every aspect of a weapon system, but if the operator is only reflexively pressing a button to approve strikes recommended by the weapon system, the operator would be exercising little, if any, judgment over the use of force. • i would also highlight the use scenario that we have put forward in our working paper last year, in which the machines are being relied upon by human beings to inform decision-making in armed conflict. When human beings are relying on machine assessments to make decisions, in human-machine teaming concepts, like those described the uk, it’s a much more complex issue than simply an issue of the human controlling the machine and pressing the stop button if they see something inappropriate. • we also appreciate the point made by our colleague from japan that we should be learning and studying first, in particular in developing areas like machine self-learning, and looking to the good practices already developed by industry and in other contexts. • i also wanted to highlight and appreciate a comment by our colleague from switzerland, who noted that consensus seems to be emerging around the idea that “human control” or “human-machine interaction” is not an end in and of itself, but rather, one means to help ensure that emerging technologies in the area of laws are used in compliance with ihl. • the type and extent of human machine interaction that is appropriate for a particular weapon in a particular context will necessarily vary based on the specifics of the situation. This is a point that was well-made by our swedish colleague, our israeli colleague, and our japanese colleague as well. Because what is required by ihl can be so contextual, developing strict new requirements like requirements for “stop” buttons may be difficult to do across all contexts. • however, we find the suggestion of our colleague from switzerland that the chair consider offering good practices for human-machine interaction under guiding principle (c) to be a helpful one. We have ourselves been thinking along these same lines, and have offered some suggestion of our own for good practices in human-machine interaction for the gge to consider. • yesterday, we proposed specific conclusions on human-machine interaction, which are highlighted in our national commentary on guiding principle (c). I think there are common elements in our proposals that are reflected in the statements of other delegations, including elements relating to testing and evaluation of weapon systems, training of personnel, establishment of doctrine and procedures for the use of weapon systems, and user interfaces for weapon systems. • lastly, we appreciate the chair’s suggestion of the establishment of friends of the chair in the area of legal, technical, and military expertise to further explore some of these ideas, and hope to be able to contribute our experience. Thank you.
Unclassified//final// 09 22 2020 unclassified agenda item 5(b) characterization of the systems under consideration in order to promote a common understanding on concepts and characteristics relevant to the objectives and purposes of the convention statement: the united states continues to support identifying general characteristics of systems that are under the gge’s consideration in order to facilitate the gge’s understanding of the relevant concepts and issues. The flexibility inherent in this approach of identifying characteristics is important given that scientists and engineers continue to develop new technological advancements and that our understanding continues to improve. This discussion could also help delegations understand better what we mean by the terms we are using. Some delegations may be using the same term to mean different things or some delegations may be using different terms to mean the same thing. In identifying characteristics of laws, we must not prejudice future decisions regarding potential outcomes. For example, characteristics should be identified in order to promote common understandings, not with a view towards advancing a particular policy objective, like a ban. Similarly, we must be cautious not to make hasty judgments about the value or likely effects of emerging or future technologies. Frequently, we may change our views of technologies over time as we gain more experience with them. In discussing the general characteristics of such systems, we must not lose sight of the fact that no matter their level of sophistication or how many autonomous features or functions they have, these weapons systems are tools for human use. Guiding principle (i) reminds us of this, stating: “in crafting potential policy measures, emerging technologies in the area of lethal autonomous weapons systems should not be anthropomorphized.” in particular, anthropomorphizing emerging technologies in the area of laws can lead to legal and technical misunderstandings that could be detrimental to the efficacy of potential policy measures. From a technical perspective, anthropomorphizing emerging technologies in the area of laws can lead to mis-estimating machine capabilities. From a legal perspective, anthropomorphizing emerging technologies in the area of laws can obscure the important point that ihl imposes obligations on states, parties to a conflict, and individuals, rather than machines. “smart” weapons cannot violate ihl any more than “dumb” weapons can. Similarly, machines are not intervening moral agents, and human beings do not escape responsibility for their decisions by using a weapon with autonomous functions. Anthropomorphizing emerging technologies in the area of laws could incorrectly suggest a diminished responsibility of human beings simply by the use of emerging technologies in the area of laws. The u.s. Department of defense policy directive on the use of autonomy in weapon systems establishes definitions of an “autonomous weapon system” and “semi-autonomous weapon system” for the purposes of that policy directive. These definitions focus on what we believe to be the most important issues posed by the use of autonomy in weapon systems — people who employ these weapons can rely on the weapon systems to select and engage targets. We will not unclassified 2 unclassified repeat the specific definitions today, but, for reference, those definitions are reproduced in the u.s. Working paper from november 2017. In discussing concerns about autonomous weapons, it may be important consider whether these concerns are fundamentally about the type of weapon system or whether the concerns are about how weapons systems are used. For example, consider a missile with automated target recognition capabilities that can select and engage enemy tanks. In one scenario, an operator identifies a specific target and fires the missile at this target. Under the definitions applied by the u.s. Military, this is a semi-autonomous weapon system. That same weapon system and capability could, however, be classified as an autonomous system if it is used in a different way. If the operator does not identify a specific tank, but instead fires the weapon to loiter in an area and autonomously select and engage tanks, the weapon is classified as an autonomous weapon in u.s. Military practice. The weapon system’s technical characteristics are the same, but how it is to be used changes whether it is classified as autonomous or semi-autonomous. Some delegations this morning and yesterday have raised concerns about laws being inherently unpredictable and in the spirit of the interactive discussion that our chair has invited, i would ask them to consider whether this concern is based on a characteristic of the weapon system or whether it is actually based on assumptions about how those weapon systems would be used. We believe that making progress in our discussions involves developing our common understanding of how emerging technologies in the area of laws can be used consistent with ihl and the conclusions we have proposed in our national commentary on guiding principle (a) try to do that.
June 11, 2021 Page 1 of 7 U.S. Proposals As described in the Discussion Paper submitted by Australia, Canada, Japan, the United Kingdom, and the United States, the Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts (GGE) on Emerging Technologies in the Area of Lethal Autonomous Weapons Systems (LAWS) has already reached a significant number of consensus conclusions under the four elements that numerous delegations have proposed to serve as the focus for organizing the GGE’s consensus recommendations in relation to the clarification, consideration and development of aspects of the normative and operational framework on emerging technologies in the area of lethal autonomous weapons systems. The United States believes that the GGE can accomplish even more in these four areas, and this paper provides U.S. proposals for further conclusions under each element: A) application of international humanitarian law (IHL); B) human responsibility; C) human-machine interaction; and D) weapons reviews. These proposals are also available in the U.S. Commentaries on the Guiding Principles adopted by the CCW GGE, which the United States submitted in 2020. A. U.S. proposals on the Application of IHL Clarifying how IHL requirements apply to three general scenarios for the use of autonomous functions in weapon systems: 1) homing munitions that involve autonomous functions; 2) decision support tools that that can inform decision-making about targeting; and 3) relying on autonomous functions in weapon systems to select and engage targets 1. Consistent with IHL, autonomous functions may be used to effectuate more accurately and reliably a commander or operator’s intent to strike a specific target or target group. a. The addition of autonomous functions, such as the automation of target selection and engagement, to weapon systems can make weapons more precise and accurate in striking military objectives by allowing weapons or munitions to “home in” on targets selected by a human operator. b. If the addition of autonomous functions to a weapon system makes it inherently indiscriminate, i.e., incapable of being used consistent with the principles of distinction and proportionality, then any use of that weapon system would be unlawful. c. The addition of autonomous functions to a weapon system can strengthen the implementation of IHL when these functions can be used to reduce the likelihood of harm to civilians and civilian objects. 2. Consistent with IHL, emerging technologies in the area of LAWS may be used to inform decision-making. June 11, 2021 Page 2 of 7 a. When making a decision governed by IHL, commanders and other decision-makers must make a good faith assessment of the information that is available to them at the time. b. IHL generally does not prohibit commanders and other decision-makers from using tools to aid decision-making in armed conflict. Whether the use of a tool to aid decision-making in armed conflict is consistent with IHL may depend on the nature of the tool, the circumstances of its use, as well the applicable rules and duties under IHL. c. Reliance on a machine assessment to consider a target to be a military objective must be compatible with the decision-maker’s duty under IHL to exercise due regard to reduce the risk of harm to civilians and civilian objects. Such compatibility depends on the relevant circumstances ruling at the time, including: i. how accurately and consistently the machine performs in not mischaracterizing civilian objects as military objectives (i.e., false positives); ii. the decision-maker giving the machine assessment appropriate weight relative to other information relevant to whether the target was a military objective (e.g., operational context, intelligence reporting of the threat identified by the system); and iii. the urgency to make a decision (e.g., whether the decision occurred in combat operations or in the face of an imminent threat of an attack, or whether more time could be taken before making a decision). 3. Consistent with IHL, weapons systems that autonomously select and engage targets may be used where the human operator has not expressly intended to strike a specific target or group of targets when activating the weapon system. a. The commander or operator could act consistently with the principle of distinction by: i. Using weapon systems that autonomously select and engage targets in areas that constitute military objectives; or ii. Using weapon systems that autonomously select and engage targets with the intent of making potential targets constituting military objectives (e.g., potential incoming projectiles in an active protection system) the object of attack, provided that the weapon systems perform with sufficient reliability (e.g., an active protection system consistently selecting and engaging incoming projectiles) to ensure that force is directed against such targets. June 11, 2021 Page 3 of 7 b. The expected loss of civilian life, injury to civilians, and damage to civilian objects incidental to the employment of weapons systems that autonomously select and engage targets must not be excessive in relation to the concrete and direct military advantage expected to be gained. i. The expected loss of civilian life, injury to civilians, and damage to civilian objects is to be informed by all available and relevant information, including information about: (i) the presence of civilians or civilian objects within the area and during the time when the weapon system is expected to be operating; (ii) the performance of the weapon’s autonomous functions in selecting and engaging military objectives; (iii) the risks posed to civilians and civilian objects when the weapon engages military objectives; (iv) the incidence of military objectives that could be engaged by the weapon system in the operational area; and (v) the effectiveness of any precautions taken to reduce the risk of harm to civilians and civilian objects. ii. The concrete and direct military advantage expected to be gained is to be informed by all available and relevant information, which may include information about how the employment of the weapon system: (i) threatens military objectives belonging to the adversary; (ii) contributes to the security of the operating forces; (iii) diverts enemy resources and attention; (iv) shapes or diverts the movement of enemy forces; and (v) supports military strategies and operational plans. c. Feasible precautions must be taken in use of weapon systems that autonomously select and engage targets to reduce the expected harm to civilians and civilian objects. Such precautions may include: i. Warnings (e.g., to potential civilian air traffic or notices to mariners); ii. Monitoring the operation of the weapon system; and iii. Activation or employment of self-destruct, self-deactivation, or self-neutralization mechanisms (e.g., use of rounds that self-destruct in flight or torpedoes that sink to the bottom if they miss their targets). Examples of ways in which emerging technologies in the area of LAWS could be used to reduce the risks to civilians in military operations CCW GGE Guiding Principle (h) (“Consideration should be given to the use of emerging technologies in the area of lethal autonomous weapons systems in upholding compliance with IHL and other applicable international legal obligations”) should be implemented during legal reviews of new weapons, during the formulation of military strategies and plans, and during the planning and conduct of military operations. To facilitate such consideration and to encourage innovation that furthers the objects and purposes of the CCW, the GGE should develop examples June 11, 2021 Page 4 of 7 of specific practices that those involved in these activities could consider. For example, the GGE could begin this workstream by cataloging examples of ways in which emerging technologies in the area of LAWS could be used to reduce risks to civilians in military operations, such as by: 1. incorporating autonomous self-destruct, self-deactivation, or self-neutralization mechanisms into munitions; 2. increasing awareness of civilians and civilian objects on the battlefield; 3. improving assessments of the likely effects of military operations; 4. automating target identification, tracking, selection, and engagement to improve speed, precision, and accuracy; and 5. reducing the need for immediate fires in self-defense.1 B. U.S. proposals on Human Responsibility Legal responsibility 1. Under principles of State responsibility, every internationally wrongful act of a State, including such acts involving the use of emerging technologies in the area of LAWS, entails the international responsibility of that State.2 2. A State remains responsible for all acts committed by persons forming part of its armed forces, including any such use of emerging technologies in the area of LAWS, in accordance with applicable international law. 3. An individual, including a designer, developer, an official authorizing acquisition or deployment, a commander, or a system operator, is responsible for his or her decisions governed by IHL with regard to emerging technologies in the area of LAWS. 4. Under applicable international and domestic law, an individual remains responsible for his or her conduct in violation of IHL, including any such violations involving emerging technologies in the area of LAWS. The use of machines, including emerging technologies in the area of LAWS, does not provide a basis for excluding legal responsibility. 1 These practices are discussed in the U.S. Working Paper, Humanitarian Benefits of Emerging Technologies in the Area of Lethal Autonomous Weapon Systems, March 28, 2018, CCW/GGE.1/2018/WP.4. For a discussion of other potential humanitarian benefits, in addition to reducing the risk of civilian casualties in military operations, see paragraph 15 of the U.S. Working Paper, Implementing International Humanitarian Law in the Use of Autonomy in Weapon Systems, March 28, 2019, CCW/GGE.1/2019/WP.5. 2 Adapted from Article 1 of the International Law Commission’s Draft articles on Responsibility of States for Internationally Wrongful Acts. June 11, 2021 Page 5 of 7 5. The responsibilities of any particular individual in implementing a State or a party to a conflict’s obligations under IHL may depend on that person’s role in the organization or military operations, including whether that individual has the authority to make the decisions and judgments necessary to the performance of that duty under IHL. 6. Under IHL, a decision, including decisions involving emerging technologies in the area of LAWS, must be judged based on the information available to the decision-maker at the time and not on the basis of information that subsequently becomes available. 7. Unintended harm to civilians and other persons protected by IHL from accidents or equipment malfunctions, including those involving emerging technologies in the area of LAWS, is not a violation of IHL as such. 8. States and parties to a conflict have affirmative obligations with respect to the protection of civilians and other classes of persons under IHL, which continue to apply when emerging technologies in the area of LAWS are used. These obligations are to be assessed in light of the general practice of States, including common standards of the military profession in conducting operations. Accountability practices 1. The following general practices help ensure accountability in military operations, including operations involving the use of emerging technologies in the area of LAWS: a. Conducting operations under a clear operational chain of command. b. Subjecting members of the armed forces to a system of military law and discipline. c. Establishing and using procedures for the reporting of incidents involving potential violations. d. Conducting assessments, investigations, or other reviews of incidents involving potential violations. e. Disciplinary and punitive measures as appropriate. 2. The following practices with respect to the use of weapons systems, including those based on emerging technologies in the area of LAWS, can promote accountability: a. Rigorous testing of and training on the weapon system, so commanders and operators understand the likely effects of employing the weapon system. b. Establishing procedure and doctrine applicable to the use of the weapon system, which provide standards for commanders and operators on responsible use and under which they can be held accountable under the State’s domestic law. June 11, 2021 Page 6 of 7 c. Using the weapon system in accordance with established training, doctrine, and procedures and refraining from unauthorized uses or modifications of the weapon system. C. U.S. proposals on Human-Machine Interaction 1. Weapons systems based on emerging technologies in the area of LAWS should effectuate the intent of commanders and operators to comply with IHL, in particular, by avoiding unintended engagements and minimizing harm to civilians and civilian objects. This can be effectuated through the following measures: a. Weapons systems based on emerging technologies in the area of LAWS should be engineered to perform as anticipated. This should include verification and validation and testing and evaluation before fielding systems. b. Relevant personnel should properly understand weapons systems based on emerging technologies in the area of LAWS. Training, doctrine, and tactics, techniques, and procedures should be established for the weapon system. Operators should be certified by relevant authorities that they have been trained to operate the weapon system in accordance with applicable rules. c. User interfaces for weapons systems based on emerging technologies in the area of LAWS should be clear in order for operators to make informed and appropriate decisions in engaging targets. In particular, the interface between people and machines for autonomous and semi-autonomous weapon systems should: (i) be readily understandable to trained operators; (ii) provide traceable feedback on system status; and (ii) provide clear procedures for trained operators to activate and deactivate system functions. D. U.S. Proposals on Weapons Reviews Guidelines and good practices for militaries to consider using in conducting legal reviews of weapons systems based on emerging technologies in the area of LAWS 1. Legal advisers should be consulted regularly in the development or acquisition process as decisions that could pose legal issues are being made so that legal issues can be identified and more in-depth reviews can be conducted where necessary. a. A weapon system under modification should be reviewed to determine whether the modification poses any legal issues. b. New concepts for the employment of existing weapons should also be reviewed, when such concepts differ significantly from the intended uses that were considered when those systems were previously reviewed. June 11, 2021 Page 7 of 7 2. The nature of the legal review and advice should be tailored to the stage of the process of developing or acquiring the weapon. a. Providing legal advice early in the development or acquisition process allows IHL considerations to be taken into account early in the life cycle of the weapon. b. At the end of the development or acquisition process, formal legal opinions can memorialize relevant conclusions and analysis while also being useful to consider in subsequent reviews. 3. The legal review should consider the international law obligations applicable to the State intending to develop or acquire the weapon system, including prohibitions or other restrictions applicable to specific types of weapons, and whether the intended or expected uses of the weapon system can be consistent with those obligations under IHL. 4. The legal review should consider whether the weapon is illegal per se, i.e., whether the use of the weapon is prohibited in all circumstances. a. The legal review should consider whether the weapon is of a nature to cause superfluous injury or unnecessary suffering, or if it is inherently indiscriminate, or is otherwise incapable of being used in accordance with the requirements and principles of IHL. b. Analyzing whether a weapon is “inherently indiscriminate,” should consider whether the weapon is capable of being used in accordance with the principles of distinction and proportionality. c. In considering whether a weapon with new autonomous features or functions is consistent with the prohibitions against weapons calculated to cause superfluous injury or against weapons that are inherently indiscriminate, it may be useful to compare the weapon to existing weapons not falling under these prohibitions. 5. The legal review should advise those developing or acquiring the weapon system or its concepts of employment to consider potential measures to reduce the likelihood that use of the weapon will cause harm to civilians or civilian objects. 6. Persons conducting the legal review should understand the likely effects of employing the weapon in different operational contexts. Such expectation should be produced through realistic system developmental and operational test and evaluation. 7. Bearing in mind national security considerations or commercial restrictions on proprietary information, States should share good practices on weapons reviews or legal reviews of particular weapons where appropriate.


June 11, 2021 Documents Reflecting U.S. Practice Related to Emerging Technologies in the Area of Lethal Autonomous Weapons Systems. The United States appreciates the Chair’s invitation to share relevant national policies and practices. The United States has long advocated for the sharing of national practices and policies related to the implementation of international humanitarian law (IHL). IHL establishes rules governing the use of weapon systems in armed conflict, no matter the type of technology incorporated in the weapon systems. Through robust national implementation measures, States can ensure the effective implementation of IHL, and through the sharing of practices, practitioners in one State can benefit from lessons learned in another. In the context of the Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts (GGE) on emerging technologies in the area of lethal autonomous weapons systems, the United States has appreciated the efforts of other States to share their practice, including a growing number of States that are developing and publicly articulating their national policies on ensuring the responsible use of emerging technologies. The United States has also sought to share U.S. practice. For example, the United States has shared in past GGE discussions U.S. practice: • on the Counter-Rocket, Artillery, and Mortar System (April 2018); • on a system to counter naval mines, the Single Sortie Detect to Engage (Sept. 2018); and • on the AN/TPQ-53 Counterfire Radar System (March 2019). More generally, U.S. Department of Defense (DoD) Directive 3000.09, Autonomy in Weapon Systems, provides policy guidance on the development and use of autonomous and semi-autonomous functions in weapon systems. (Attachment 1). For example, this policy establishes guidelines designed to minimize failures in autonomous and semi-autonomous weapon systems that could lead to unintended engagements. DoD also requires the legal review of weapon systems. DoD Directive 2311.01, DoD Law of War Program, provides that “t]he intended acquisition, procurement, or modification of weapons or weapon systems is reviewed for consistency with the law of war.” ¶1.2.d. The Directive also establishes the DoD Law of War Working Group, which among other functions, develops and coordinates law of war “analysis regarding the legality of new means or methods of warfare under consideration by DoD components.” ¶3.1. Under DoD Directive 2311.01, the DoD Law of War Manual serves as the authoritative statement on the law of war within DoD and includes extensive guidance in Chapter VI regarding the legal review of new weapons and legal rules specific to certain types of weapons. In addition, DoD Directive 5000.01, The Defense Acquisition System, provides that: The acquisition and procurement of DoD weapons and information systems must be consistent with all applicable domestic law, and the resulting systems must comply with applicable treaties and international agreements (for arms control agreements, see DoD Directive (DoDD) 2060.01), customary international law, and the law of armed conflict (also known as the laws and customs of war). An attorney Page 2 authorized to conduct such legal reviews in the DoD must conduct the legal review of the intended acquisition of weapons or weapons systems. ¶1.2v. The Military Departments within DoD have implemented this requirement and provided further guidance on the legal review of weapons in issuances for their personnel: • Department of the Army: Army Regulation 27-53, Legal Review of Weapons and Weapon Systems, Sept. 23, 2019. • Department of the Navy: Paragraph 10 of Enclosure 3 of SECNAV Instruction 5000.2F, Defense Acquisition System and Joint Capabilities Integration and Development System Implementation, March 26, 2019, addresses “Mandatory Legal Review of Potential Weapons & Weapon Systems.” • Department of the Air Force: Part 2 of Air Force Instruction 51-401, The Law of War, Aug. 3, 2018, addresses “Legal Reviews of Weapons and Cyber Capabilities.” See also Air Force Policy Directive 51-4, Operations and International Law, July 24, 2018. U.S. military practice in conducting the legal review of weapons was described extensively in a 2017 DoD submission to a study by the Stockholm International Peace Research Institute (SIPRI). Artificial Intelligence (AI) has received attention both within DoD and in our GGE discussions. AI applications across all sectors of life, such as transportation and health care, present the possibility of great benefits to society, especially in light of the rapid pace of ongoing developments in this field. However, there are also concerns that AI could be misused or misapplied. Organizations across various sectors are seeking proactively to develop principles to guide the responsible development and use of AI. Similarly, a key focus area of DoD’s Strategy on Artificial Intelligence is “Leading in military ethics and AI safety.” In February 2020, the Secretary of Defense reaffirmed “that the Department will use AI consistent with applicable domestic and international law, in particular the law of war and adopted Artificial Intelligence Ethical Principles for the Department of Defense. (Attachment 2). On May 26, 2021, the Deputy Secretary of Defense reaffirmed these principles and established DoD’s holistic, integrated, and disciplined approach for implementing them, including articulating six foundational tenets. (Attachment 3). As AI has the potential to transform positively a whole spectrum of DoD activities, these memos are not limited to weapon systems. The DoD AI Ethical Principles apply to all DoD AI capabilities, of any scale, including AI-enabled autonomous systems, for warfighting and business applications. We have shared these references and documents in order: (1) to continue to provide transparency on U.S. practice; (2) to encourage others to share their practice and to consider U.S. practice; and (3) to facilitate further discussion about the development and use of emerging technologies in the area of lethal autonomous weapons systems. We are happy to discuss U.S. practice with other delegations to the GGE. Attachments: 1. U.S. Department of Defense Directive 3000.09, Autonomy in Weapons Systems, Nov. 21, 2012, incorporating Change 1, May 8, 2017 Page 3 2. Secretary of Defense, Artificial Intelligence Ethical Principles for the Department of Defense, Feb. 21, 2020 3. Deputy Secretary of Defense, Implementing Responsible Artificial Intelligence in the Department of Defense, May 26, 2021 Tab 1 (This page intentionally left blank) Tab 2 (This page intentionally left blank) Tab 3 (This page intentionally left blank)
2014 MX LAWS

U.S. DELEGATION OPENING STATEMENT Thank you, Mr. Chairman. The United States Delegation appreciates the work you have done in laying the groundwork for this important informal meeting on emerging technological challenges associated with increasing autonomy in weapons systems; we look forward to productive discussions under your guidance this week. We are confident that over the next four days we will all come to a better understanding of the varied and complex issues related to lethal autonomous weapons systems. You can count on our delegation to participate fully over the next four days. We will provide specific comments during the sessions to come, but at the outset, we would like to make three framing points and then highlight one issue that is, to us, critical in thinking about autonomous features of weapons systems. First, this important discussion is just beginning and we believe considerable work still needs to be done to establish a common baseline of understanding among states. Too often, the phrase "lethal autonomous weapons system" appears still to evoke the idea of a humanoid machine independently selecting targets for engagement and operating in a dynamic and complex urban environment. But that is a far cry from what we should be focusing on, which is the likely trajectory of technological development, not images from popular culture. To move toward a common understanding does not mean that we need to define "lethal autonomous weapons systems" at the outset. Recent discussions in which we have participated, along with other states, and scientists, roboticists, lawyers, and ethicists, have shown that some ideas about lethal autonomous weapons systems are so widely divergent that it would be imprudent, if not impossible, to precisely define the term now. Much examination and discussion is necessary before we try to undertake that task. As we begin our discussions here, though, we must be clear on one point — we are here to discuss future weapons or, in the words of the mandate for this meeting, "emerging technologies." Therefore we need to be clear, in these discussions we are not referring to remotely piloted aircraft, which as their name indicates are not autonomous and therefore, conceptually distinct from LAWS. Second, it follows from the fact that we are indeed at such an early stage of our discussions that the United States believes it is premature to determine where these discussions might or should lead. In our view, it is enough for now for us collectively to acknowledge the value in discussing lethal autonomous weapons systems in the CCW, a forum focused on international humanitarian law, which is the relevant framework for this discussion. Third, we must bear in mind the complex and multifaceted nature of this issue. This complexity means we need to carefully think through the full range of possible consequences of different approaches. For instance, our discussion here will necessarily touch on the development of civilian technology, which we expect to continue unrestricted by those discussions. With that said, the United States would like to highlight one of the key issues we think states should focus on in considering autonomy in weapons systems -- and that is risk. We will elaborate on this further in the coming days, but, to give just one example, how does the battle-field — whether cluttered or uncluttered — affect the risk of using a particular weapons system? In order to assess risk associated with the use of any weapons system, States need a robust domestic legal and policy process and methodology. We think states may also need to tailor those legal and policy processes when considering weapons with autonomous features. For that reason, as you know, after a comprehensive policy review, the United States Department of Defense issued DoD Directive 3000.09, "Autonomy in Weapon Systems," in 2012. The Department developed the directive in order to better understand and identify the risks posed by autonomy, as well as to consider possible ways to mitigate risks that are identified. It established a high-level, detailed process for considering weapons with autonomous features and issued specific guidelines designed to "minimize the probability and consequences of failures that could lead to unintended engagements." The United States intends to discuss the risks of autonomy, as well as possible benefits, and means of analyzing those risks, over the coming days, using the Directive as an example. We would likewise encourage other states to consider presenting their own ways and means of thinking about the risks of autonomy and whether they have their own domestic processes for evaluating those risks. Mr. Chairman, as we have said, the issues related to LAWS are complex. We are here to share our thoughts on autonomy in weapons systems and learn from others. We have brought a broad range of experts with us and look forward to engaging with all interested delegations. At this early stage, we cannot say, and, to reiterate, should not prejudge, where the discussion will lead, but we do recognize that it is a good time for this discussion to begin.
