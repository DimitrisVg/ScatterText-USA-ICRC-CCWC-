Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 27-31 August 2018, Geneva Statement of the International Committee of the Red Cross (ICRC) under agenda item 6(d) Thank you Mr. Chair. At the outset, let me, on behalf of the ICRC, thank the many delegations that have taken the floor under this important agenda item to share their views. The ICRC appreciates just how far the discussions in the CCW on autonomous weapon systems have come in the last five years. Rapid developments in military applications of robotics and artificial intelligence technologies lend urgency to these ongoing discussions and to achieving progress in identifying and building on common ground, with the aim of ensuring that decisions to kill, injure and destroy remain with humans, and that such critical decisions are not effectively left to sensors and software. Autonomous weapon systems raise concerns about loss of human control of the use of force, which could have serious consequences for both civilians and combatants in armed conflict. The ICRC’s view is that these characteristics of autonomous weapon systems raise unique issues and challenges for legal compliance and for humanity and, in turn, raise the question of whether new internationally agreed policies, standards or rules are needed. The meetings of this Group of Governmental Experts (GGE) in April, and again this week, were encouraging in the general agreement among all States that human control must be maintained over weapon systems and the use of force. This “humancentred” approach should now guide the further work of this GGE to set limits on autonomy in weapon systems and ensure human control is maintained. We’ve heard several proposal put forward by States on how to achieve this goal, including: o the development of new law, in particular the negotiation of a legally binding instrument (a Protocol) to either prohibit autonomous weapon systems, or establish a positive obligation of human control; o a political declaration, reaffirming the applicability of international law (in particular, international humanitarian law (IHL)) and stating human control must be maintained; and o improved implementation and best practices for national legal reviews of new weapons. Independent of the policy response chosen, all approaches share the same requirement for core substantive work, namely that States must determine the type and degree of human control in the use of weapon systems with autonomy in their critical functions that is needed to ensure compliance with international law, in particular IHL, and ethical acceptability. In the ICRC’s view, focused work by States will help determine what (minimum/ sufficient /effective/meaningful/appropriate) human control means in practice. It should be driven by the necessity to preserve human judgement and responsibility in targeting decisions. This approach will enable States at the CCW to: 1) Specify a positive obligation for human control over weapon systems and the use of force, in line with legal obligations and ethical considerations; 2) establish internationally agreed limits on autonomy in weapon systems, that address legal, ethical and humanitarian concerns; and 3) identify “autonomous weapon systems of concern” that fall outside these limits, and would therefore be unlawful and/or unacceptable. The advantage of this approach, which focusses on a positive obligation of human control, in contrast to a negative approach (a prohibition), is that we would avoid getting stuck with debates about definitions and technical characteristics. Focussing on a positive obligation of human control would also, in our submission, result in a standard that withstands the test of time (as you have called for Mr. Chair), as it is focussed on the needed level of human control over weapon systems and the use of force, rather than on technological characteristics. The ICRC is encouraged that an increasing number of States are supporting this approach. Thank you. ICRC statement, CCW GGE on “LAWS”, 27-31 August 2018 2 Convention on Certain Conventional Weapons (CCW) Meeting of Experts on Lethal Autonomous Weapons Systems (LAWS) 11-15 April 2016, Geneva Views of the International Committee of the Red Cross (ICRC) on autonomous weapon system 11 April 2016 Introduction Views on autonomous weapon systems, including those of the International Committee of the Red Cross (ICRC), continue to evolve as a better understanding is gained of current and potential technological capabilities, the military purpose of autonomy in weapon systems, and the resulting questions for compliance with international humanitarian law (IHL) and ethical acceptability. Expert discussions of the last three years in the framework of the Convention on Certain Conventional Weapons (CCW) and in meetings convened by the ICRC and other organisations have been crucial to enhancing this understanding. The ICRC held a second expert meeting on ‘Autonomous weapon systems: Implications of increasing autonomy in the critical functions of weapons’ from 15-16 March 2016 in Versoix, Switzerland.1 Representatives of 20 States together with individual experts and representations of the United Nations and civil society organisations participated in the meeting. The ICRC will soon publish a summary report of the meeting. In the meantime, as a contribution to ongoing discussions in the CCW, this paper highlights some of the key issues on autonomous weapon systems from the perspective of the ICRC, and in the light of discussions at its recent expert meeting. 1. Definitions The ICRC has defined autonomous weapon systems as: “Any weapon system with autonomy in its critical functions. That is, a weapon system that can select (i.e. search for or detect, identify, track, select) and attack (i.e. use force against, neutralize, damage or destroy) targets without human intervention.” The advantage of this broad definition, which encompasses some existing weapon systems, is that it enables real-world consideration of weapons technology to assess what may make certain existing weapon systems acceptable – legally and ethically – and which emerging technology developments may raise concerns under international humanitarian law (IHL) and under the principles of humanity and the dictates of the public conscience. Some prefer a narrow definition more closely linked to identifying the type of weapon systems of greatest concern. In addition, some narrow definitions distinguish between “highly automated” and “autonomous” weapons. However, many experts emphasise that there is not such a clear difference from a technical perspective, and the core legal and ethical questions remain the same. 1 For a report of the first meeting see: ICRC (2014) Autonomous weapon systems technical, military, legal and humanitarian aspects, Report of an Expert Meeting held 26-28 March 2014 (published November 2014), https://www.icrc.org/en/download/file/1707/4221-002-autonomous-weapons-systems-full-report.pdf 2 In the view of the ICRC it is useful to start with a broad scope since lessons from experiences with existing weapon systems with autonomy in their critical functions can inform current discussions on emerging technology. 2. Autonomy in existing weapons There are weapon systems in use today which can select and attack targets without human intervention. After activation by a human operator it is the weapon system, through its sensors and computer programming, which selects a target and launches an attack. One broad group are anti-material defensive weapons used to protect vehicles, facilities or areas from incoming attacks with missiles, rockets, mortars or other projectiles. These include missile and rocket defence weapons and vehicle “active protection” weapons. The ability to effectively control these weapons and the use of force seems to require certain operational constraints including: limits on the task carried out (i.e. a single function to defend against incoming projectiles); limits on the targets (i.e. primarily objects and vehicles); controls over the operational environment (e.g. limitations on the geographical area and time frame of autonomous operation); and procedures for human intervention to deactivate the weapon, i.e. to cease its operation. Some offensive weapon systems, including certain missiles and torpedoes also have a level of autonomy in selecting and attacking targets after launch. Many of these weapons are fired into a particular target area, after which on-board sensors and programming take over to autonomously select and attack a specific target object or person within that area. Some have more freedom of action in time and/or space, and therefore greater autonomy. These include, in particular, loitering munitions that search for targets over a wide geographical area for long time periods, and encapsulated torpedo weapons that remain stationary underwater over long time periods but can carry out attacks autonomously. The trend in the development of missiles appears to be increasing autonomy with respect to movement in time and space. Indications are that future developments could also include increasing adaptability of these weapon systems to their environment. The ability to effectively control these weapons and the use of force may depend on a number of factors (as with defensive systems discussed above) including: limits on the task carried out; the ability of the system to discriminate targets; controls over the operational environment, such as limitations in time and space; and the ability for humans to communicate with the weapon system, for example to deactivate it. The latter is particularly difficult for underwater systems. There are other anti-personnel weapons that may be capable of autonomously selecting and attacking targets, such as so called “sentry” weapons used to defend facilities and borders. However, the systems in use today apparently remain under remote control for initiation of attacks. In sum, autonomy for selecting and attacking targets in existing weapon systems is limited by the operational parameters described above. Moreover, the technical characteristics and performance of existing weapon systems, combined with the operational parameters of their use, provide a certain degree of predictability of the outcomes of using these weapon systems. This predictability may be lost as autonomous weapon systems are used for more complex tasks or deployed in more dynamic environments than has been the case until now. 3. Emerging technology and future autonomous weapons Although it is difficult to foresee the future development of autonomous weapon systems, it is clear there are a number of military drivers for increased autonomy, including enabling: increased mobility of robotic/unmanned weapon systems or platforms; operation of these systems in “communications denied” environments; shorter decision-making times between identifying and attacking a target; increased performance over remotely operated systems; and operation of increased numbers of robotic/unmanned weapon systems by fewer operators. Interest in autonomous weapon systems could also develop in different ways among non-State armed groups. The general trend in civilian robotics is towards supervised autonomy, where robotic systems are increasingly autonomous while human operators retain oversight and often the ability to intervene. 3 The degree of autonomy in a robotic system is related to the level of human intervention in its operation, both in terms of the degree of human intervention and the stages at which the intervention is made. Even so, machines can and do effectively take decisions that have been delegated to them by humans through their computer programming, and without the need to be “conscious” or to have human-like levels of intelligence. There are a number of developments that might make increasingly autonomous weapon systems become less predictable. These include: increased mobility, meaning the weapon system would encounter more varied environments over greater time periods; increased adaptability, such as systems that set their own goals or change their functioning in response to the environment (e.g. a system that defends itself against an attack) or even incorporate learning algorithms; and increased interaction of multiple weapon systems in self-organising swarms. In addition to decreasing the predictability of the weapon system, these developments could raise related problems for the validity of testing to ensure reliability. For example, autonomous weapon systems that could set their own goals, or even “learn” and adapt their functioning, would by their nature be unpredictable. Highly mobile autonomous weapon systems would also create problems of predictability with respect to the target of specific attacks, especially if a system moved over a wide area or carried out multiple attacks. 4. Legal and ethical implications of increasing autonomy in weapon systems It is clear that IHL rules on the conduct of hostilities are addressed to the parties to an armed conflict, more specifically to the human combatants and fighters, who are responsible for respecting them, and will be held accountable for violations. These obligations cannot be transferred to a machine. Still, in practical terms, the question remains, what limits are needed on autonomy in weapon systems to ensure compliance with IHL. Control exercised by human beings can take various forms and operate at different stages of the “life cycle” of an autonomous weapon system, including: 1) the development of the weapon system, including its programming; 2) the deployment and use of the weapon system, including the decision by the commander or operator to use or activate the weapon system; and 3) the operation of the weapon system during which it selects and attacks targets. It is clear that human control is exerted in the development and deployment stages of the weapon’s “life cycle”. It is in stage three, however, when the weapon is in operation – when it autonomously selects and attacks the target(s) – that the important question arises as to whether human control in the first two stages is sufficient to overcome minimal or no human control at this last stage, from a legal, ethical and military-operational standpoint (see also section 5 below). The assessment of whether an autonomous weapon system can be used in compliance with IHL may depend on the specific technical characteristics and performance of the weapon system and the intended and expected circumstances of its use. Certain technical characteristics and their interaction with different operational parameters could significantly affect this assessment, including:  The task the weapon system carries out;  The type of target the weapon system attacks;  The environment in which the weapon system operates;  The movement of weapon system in space;  The time-frame of operation of the weapon system;  The adaptability of the weapon system, i.e. its ability to adapt its behaviour to changes in its environment, to determine its own functions and to set its own goals;  Degree of reliability of the weapon system, i.e. robustness to failures and vulnerability to malfunction or hacking; and  Potential for human supervision and intervention to deactivate the weapon system. The combination of these technical characteristics and performance of the weapon system with the operational parameters of its use are critical to determining the foreseeable effects of the weapon – in other words, the predictability of the outcomes of using the weapon – and therefore in determining whether it can be used in conformity with IHL rules. 4 Indeed, deploying a weapon system whose effects are wholly or partially unpredictable would create a significant risk that IHL will not be respected. The risks may be too high to allow use of the weapon, or else mitigating the risks may require limiting or even obviating the weapons’ autonomy. In this respect, the last factor in the list above – human intervention – could be considered as a risk mitigation factor. The level of risk resulting from a decrease in predictability of the weapon system may be influenced by a number of factors, such as the environment in which the weapon is used. Predicting the outcome of using autonomous weapon systems may become increasingly difficult as the weapon systems become more complex or are given more freedom of action in their operations. For example, the legal assessment of an autonomous weapon system that carries out a single task against a limited type of target in a simple (uncluttered) environment, and that is stationary and limited in the duration of its operation (e.g. some existing missile and rocket defence systems) may conclude that there is an acceptable level of predictability, allowing for responsibility and accountability of the human operator. However, the conclusion may be very different regarding an autonomous weapon system that carries out multiple tasks (or is adaptable) against different types of targets in a complex (cluttered) environment, and that is mobile over a wide area and/or operating for a long duration. For the purposes of an assessment under IHL there is no legal distinction between an offensive attack and a defensive one, as they both constitute attacks under the law. Nevertheless, the distinction between defensive and offensive weapon systems may be of greater relevance from a militaryoperational or ethical perspective. The obligation to carry out legal reviews of new weapons under article 36 of Additional Protocol I to the Geneva Conventions is important to ensure that a State's armed forces are capable of conducting hostilities in accordance with its international obligations. The above challenges for IHL compliance will need to be carefully considered by States when carrying out legal reviews of any autonomous weapon system they develop or acquire. As with all weapons, the lawfulness of a weapon with autonomy in its critical functions depends on its specific characteristics, and whether, given those characteristics, it can be employed in conformity with the rules of IHL in all of the circumstances in which it is intended and expected to be used. The ability to carry out such a review entails fully understanding the weapon’s capabilities and foreseeing its effects, notably through testing. Yet foreseeing such effects may become increasingly difficult if autonomous weapon systems were to become more complex or to be given more freedom of action in their operations, and therefore become less predictable. Questions arise as to how IHL’s “targeting rules” (e.g. the rules of proportionality and precautions in attack) are considered in reviewing weapons. Where it is the weapon itself that takes on the targeting functions, the legal review would demand a very high level of confidence that the weapon is capable of carrying out those functions in compliance with IHL. An additional challenge for reviewing the legality of an autonomous weapon system is the absence of standard methods and protocols for testing and evaluation to assess the performance of these weapons, and the possible risks associated with their use. Questions arise regarding: How is the reliability (e.g. risk of malfunction or vulnerability to cyber-attack) and predictability of the weapon tested? What level of reliability and predictability are considered to be necessary? The legal review procedure faces these and other practical challenges to assess whether an autonomous weapon system will perform as anticipated in the intended or expected circumstances of use. Although there are different views on the adequacy of national legal reviews of new weapons for ensuring IHL compliance of autonomous weapon systems, especially given the low level of implementation among States, this mechanism remains a critical measure for States to ensure respect for IHL. In any case, efforts to strengthen national legal review processes should be seen as complementary and mutually reinforcing of CCW discussions at the international level. Some have raised concerns that use of autonomous weapon systems may lead to an “accountability gap” in case of violations of IHL. Others are of the view that no such gap would ever exist as there will always be a human involved in the decision to deploy the weapon to whom responsibility could be attributed. 5 Under IHL and international criminal law, the limits to control over, or the unpredictability of, an autonomous weapon system could make it difficult to find individuals involved in the programming and deployment of the weapon liable for serious violations of IHL. They may not have the knowledge or intent required for such a finding, owing to the fact that the machine can select and attack targets independently. Programmers might not have knowledge of the concrete situations in which at a later stage the weapon system might be deployed and in which IHL violations could occur. On the other hand, a programmer who intentionally programmes an autonomous weapon to commit war crimes would certainly be criminally liable. Likewise, a commander would be liable for deciding to use an autonomous weapon system in an unlawful manner, for example deploying in a populated area an anti-personnel autonomous weapon that is incapable of distinguishing civilians from combatants. In addition, a commander who knowingly decides to deploy an autonomous weapon whose performance and effects he/she cannot predict may be held criminally responsible for any serious violations of IHL that ensue, to the extent that his/her decision to deploy the weapon is deemed reckless under the circumstances. Overall, as long as there will be a human involved in the decision to deploy the weapon to whom responsibility could be attributed, there might not be an accountability gap. Under the law of State responsibility a State could be held liable for violations of IHL caused by the use of any autonomous weapon system. Indeed under general international law governing the responsibility of States, they would be held responsible for internationally wrongful acts, such as violations of IHL committed by their armed forces using autonomous weapon systems. A State would also be responsible if it were to use an autonomous weapon system that it has not, or has inadequately, been tested or reviewed prior to deployment. Autonomous weapon systems also raise ethical concerns that deserve careful consideration. The fundamental question at the heart of concerns, and irrespective of whether they can be used in compliance with IHL, is whether the principles of humanity and the dictates of public conscience would allow machines to make life-and-death decisions in armed conflict without human involvement. The debates of recent years among States, experts, civil society and the public have shown that there is a sense of deep discomfort with the idea of any weapon system that places the use of force beyond human control. The question remains, however, what degree of human control is required, and in which circumstances, in light of ethical considerations? Is it sufficient for a human being to program an autonomous weapon system according to certain parameters and then make the decision to deploy it in a particular context? Or is it necessary that a human being bring his or her judgment to bear also on each individual attack? If the weapon autonomously uses force against a human target, what ethical considerations would this entail? 5. Human control The notion of human control has become the overarching issue in the debates on autonomous weapon systems. There is broad agreement that human control over weapon systems and the use of force must be retained, although less clarity on whether this is for legal, ethical, military operational, and/or policy reasons, and what makes it “meaningful”, “appropriate” or “effective”. From the ICRC’s perspective, a focus on the role of the human in the targeting process and the humanmachine interface could provide a fruitful avenue for increasing understanding of concerns that may be raised by autonomous weapon systems, rather than a purely technical focus on the ‘level of autonomy’ of weapon systems. A certain level of human control over attacks is inherent in, and required to ensure compliance with, the IHL rules of distinction, proportionality and precautions in attack. Considering more closely what these requirements are could help determine the boundaries of what is acceptable under IHL with respect to autonomy in the critical functions of selecting and attacking targets. There are already a number of considerations that have been suggested, which provide avenues for future work to establish these requirements, including:  Predictability of the weapon system in its intended or expected circumstances of use;  Reliability of the weapon system in its intended or expected circumstances of use; 6  Human intervention in the functioning of the weapon system during its development, deployment and use;  Knowledge and accurate information about the functioning of the weapon system and the context of its intended or expected use; and  Accountability for the functioning of the weapon system following its use. Many of the technical characteristics and operational parameters that are relevant to assessing compliance with IHL (see section 4) are also important factors for determining the requisite human control over the use of force, as well as relevant for military-operational or ethical considerations. For example, human control over existing autonomous weapon systems (see section 2) is largely governed by technical and operational constraints on the functioning of the system (e.g. limited tasks and targets, limits in space and time, physical controls over the environment, and human supervision and ability to deactivate). The military have a clear interest in maintaining human control of weapon systems, both to ensure compliance with IHL and to ensure that the commander has control over a given military operation. Rules of engagement associated with particular weapon systems are a primary way in which operational control is exerted over weapon systems and the use of force, and are therefore an important element in ensuring human control over the weapon system and the use of force. Deeper consideration of the elements constituting human control and understanding human-machine interaction is needed to help determine the boundaries necessary to ensure that human control is maintained over weapon systems and the use of force. As outlined above, there are already concerns that autonomous weapon systems which could adapt or change their functioning, and those that ‘hunt’ for targets over wide areas could raise serious questions about human control and IHL compliance due to the lack of predictability on when and where the use of force and specific attacks would take place. 6. The way forward There are at least three broad approaches that States could take to address the legal and ethical questions raised by autonomous weapon systems. The first regards strengthening national mechanisms for legal review and implementation of IHL to ensure any new weapons, including autonomous weapon systems, can be used in compliance with IHL. The second is for States to develop a definition of “lethal autonomous weapon systems” in terms of the weapon systems that may be problematic from a legal and/or ethical perspective with a view to establishing specific limits on autonomy in weapon systems. The third approach is for States to develop the parameters of human control in light of the specific requirements under IHL and ethical considerations (principles of humanity and the dictates of public conscience), thereby establishing specific limits on autonomy in weapon systems. Both the second and third approaches recognise that international consideration is needed of the limits of autonomy in weapons systems to ensure legal compliance and ethical acceptability. Ultimately these two approaches might lead to the same end point in terms of identifying weapon systems requiring possible regulation or prohibition. From the ICRC’s perspective, the third approach focussing on human control and the human-machine interaction could be an effective way forward for the CCW, with efforts to strengthen national legal reviews to be pursued in parallel as a mutually reinforcing initiative. The framework of human control provides a useful baseline from which common understandings can be developed among States, and through which boundaries or limits on autonomy in weapon systems can be established. This is consistent with the broad agreement among States, experts and other stakeholders that there is a need to maintain human control over weapon systems and the use of force in view of legal obligations, military operational requirements, and ethical considerations. + + + CHECK AGAINST DELIVERY Convention on Certain Conventional Weapons (CCW) Meeting of Experts on Lethal Autonomous Weapons Systems (LAWS) 11-15 April 2016, Geneva Statement of the International Committee of the Red Cross (ICRC) 11 April 2016 The International Committee of the Red Cross (ICRC) is pleased to contribute its views to this third CCW Meeting of Experts on “Lethal Autonomous Weapon Systems”. Discussions at the previous two CCW meetings of experts, as well as expert meetings convened by the ICRC and other organizations, have underscored the significant legal, ethical and societal questions raised by weapon systems that can select and attack targets without human intervention. Although views on this complex subject continue to evolve, including those of the ICRC, discussions have indicated broad agreement that, for legal, ethical or military-operational reasons, human control over weapon systems and the use of force must be retained. The ICRC has called on States to set limits on autonomy in weapon systems to ensure they are used in accordance with international humanitarian law (IHL) and within the bounds of what is acceptable under the principles of humanity and the dictates of public conscience. As an additional contribution and further to the first ICRC meeting held in March 2014, the ICRC convened a second meeting of international experts (“Autonomous weapon systems: Implications of increasing autonomy in the critical functions of weapons”) on 15-16 March 2016. Representatives of 20 States together with individual experts and representatives of the United Nations and civil society organizations participated in the meeting. The aim of the meeting was to link the real-world development of autonomy in the critical functions of weapon systems more closely to consideration of the legal and ethical implications of, and potential policy approaches to, autonomous weapon systems. A summary report of the ICRC’s expert meeting will be published shortly. In the meantime, as a contribution to this CCW meeting, we are circulating a short paper highlighting the ICRC’s perspective on some of the key issues raised at our expert meeting. In particular, we would like to highlight the following points:  Definitions: The ICRC has proposed that “autonomous weapon systems” is an umbrella term encompassing any weapon system that has autonomy in the critical functions of selecting and attacking targets. We wish to stress that the purpose of this working definition is to promote better understanding of the issue and to help frame related discussions. The advantage of such a broad definition is that it enables consideration to be given to experience of existing weapons systems with autonomy in their critical functions and to lessons learned. This could facilitate the process of determining the boundaries of what is acceptable under IHL and the dictates of public conscience.  Existing weapons: Some weapon systems in use today can select and attack targets without human intervention. The ability to effectively control these weapons and the use of force seems to be closely linked to their predictability and reliability, as well as to strict operational constraints with respect to the task carried out, the targets attacked, the operational environment, the geographical space and time of operation, the scope to enable human oversight of the operation of the weapon system, and the human ability to deactivate it if need be. 2  Emerging technology: There is a likelihood that increasingly autonomous weapon systems could become less predictable, particularly in case of increased mobility, increased adaptability and/or increased interaction of multiple systems (as swarms). The loss of predictability regarding the outcomes of using an autonomous weapon may point to the loss of human control over that weapon’s operation, with human decision-making over the use of force being replaced by machine processes. Therefore, significant questions may arise regarding compliance of such a weapon with IHL and its acceptability under the dictates of public conscience.  Legal reviews: The ICRC welcomes the recognition by States of the importance of reviewing new weapons to ensure their compatibility with international law, and stresses that efforts to strengthen national legal review processes are complementary and mutually reinforcing of discussions at the international level, in particular in the CCW meetings of experts. However, the legal review faces certain practical challenges regarding the assessment of whether an autonomous weapon system will perform as anticipated in the intended or expected circumstances of use. Two particular questions are raised: How is the predictability and reliability of the weapon assessed? What level of predictability and reliability is considered necessary?  The principles of humanity and the dictates of public conscience: Debates among States, experts, civil society and the general public have shown that there is a sense of deep discomfort with the idea of any weapon system that places the use of force beyond human control. Given the extreme gravity of the consequences and regardless of whether or not such systems are lawful, humanity may very well insist that decisions to kill or to destroy must continue to be taken by humans.  Human control: The notion of human control is the overarching issue in this debate. Whether for legal, ethical or military-operational reasons, there is broad agreement on the need for human control over weapons and the use of force. However, it remains unclear whether human control at the stages of the development and the deployment of an autonomous weapon system is sufficient to overcome minimal or no human control at the stage of the weapon system’s operation – that is, when it independently selects and attacks targets. There is now a need to determine the kind and degree of human control over the operation of weapon systems that are deemed necessary to comply with legal obligations and to satisfy ethical and societal considerations.  The way forward: Since the need to maintain human control –- whether “meaningful”, “appropriate” or “effective” – over weapon systems and the use of force is consistent with legal obligations, military operational requirements and ethical considerations, the ICRC encourages States to use it as a framework for ongoing CCW discussions. Human control and consideration of humanmachine interaction may provide a useful baseline from which common understandings can be developed among States, and through which limits on autonomy in weapon systems can be agreed. The ICRC urges CCW States Parties to agree at the Review Conference in December that future work focus on determining where these limits on autonomy in weapon systems should lie. Weapon systems that operate with autonomy in their critical functions already exist and new advances are constantly being made. There is therefore a need for States – and indeed for all stakeholders – to address this issue with a sense of responsibility and urgency, so as to ensure that technological developments do not outpace our legal and ethical deliberations. We encourage delegations to read the paper we have circulated, which goes into greater detail on these points. We will be pleased to elaborate further on our views during the thematic sessions. Thank you. Convention on Certain Conventional Weapons (CCW) Meeting of Experts on Lethal Autonomous Weapons Systems (LAWS) 11-15 April 2016, Geneva Views of the International Committee of the Red Cross (ICRC) on autonomous weapon system 11 April 2016 Introduction Views on autonomous weapon systems, including those of the International Committee of the Red Cross (ICRC), continue to evolve as a better understanding is gained of current and potential technological capabilities, the military purpose of autonomy in weapon systems, and the resulting questions for compliance with international humanitarian law (IHL) and ethical acceptability. Expert discussions of the last three years in the framework of the Convention on Certain Conventional Weapons (CCW) and in meetings convened by the ICRC and other organisations have been crucial to enhancing this understanding. The ICRC held a second expert meeting on ‘Autonomous weapon systems: Implications of increasing autonomy in the critical functions of weapons’ from 15-16 March 2016 in Versoix, Switzerland.1 Representatives of 20 States together with individual experts and representations of the United Nations and civil society organisations participated in the meeting. The ICRC will soon publish a summary report of the meeting. In the meantime, as a contribution to ongoing discussions in the CCW, this paper highlights some of the key issues on autonomous weapon systems from the perspective of the ICRC, and in the light of discussions at its recent expert meeting. 1. Definitions The ICRC has defined autonomous weapon systems as: “Any weapon system with autonomy in its critical functions. That is, a weapon system that can select (i.e. search for or detect, identify, track, select) and attack (i.e. use force against, neutralize, damage or destroy) targets without human intervention.” The advantage of this broad definition, which encompasses some existing weapon systems, is that it enables real-world consideration of weapons technology to assess what may make certain existing weapon systems acceptable – legally and ethically – and which emerging technology developments may raise concerns under international humanitarian law (IHL) and under the principles of humanity and the dictates of the public conscience. Some prefer a narrow definition more closely linked to identifying the type of weapon systems of greatest concern. In addition, some narrow definitions distinguish between “highly automated” and “autonomous” weapons. However, many experts emphasise that there is not such a clear difference from a technical perspective, and the core legal and ethical questions remain the same. 1 For a report of the first meeting see: ICRC (2014) Autonomous weapon systems technical, military, legal and humanitarian aspects, Report of an Expert Meeting held 26-28 March 2014 (published November 2014), https://www.icrc.org/en/download/file/1707/4221-002-autonomous-weapons-systems-full-report.pdf 2 In the view of the ICRC it is useful to start with a broad scope since lessons from experiences with existing weapon systems with autonomy in their critical functions can inform current discussions on emerging technology. 2. Autonomy in existing weapons There are weapon systems in use today which can select and attack targets without human intervention. After activation by a human operator it is the weapon system, through its sensors and computer programming, which selects a target and launches an attack. One broad group are anti-material defensive weapons used to protect vehicles, facilities or areas from incoming attacks with missiles, rockets, mortars or other projectiles. These include missile and rocket defence weapons and vehicle “active protection” weapons. The ability to effectively control these weapons and the use of force seems to require certain operational constraints including: limits on the task carried out (i.e. a single function to defend against incoming projectiles); limits on the targets (i.e. primarily objects and vehicles); controls over the operational environment (e.g. limitations on the geographical area and time frame of autonomous operation); and procedures for human intervention to deactivate the weapon, i.e. to cease its operation. Some offensive weapon systems, including certain missiles and torpedoes also have a level of autonomy in selecting and attacking targets after launch. Many of these weapons are fired into a particular target area, after which on-board sensors and programming take over to autonomously select and attack a specific target object or person within that area. Some have more freedom of action in time and/or space, and therefore greater autonomy. These include, in particular, loitering munitions that search for targets over a wide geographical area for long time periods, and encapsulated torpedo weapons that remain stationary underwater over long time periods but can carry out attacks autonomously. The trend in the development of missiles appears to be increasing autonomy with respect to movement in time and space. Indications are that future developments could also include increasing adaptability of these weapon systems to their environment. The ability to effectively control these weapons and the use of force may depend on a number of factors (as with defensive systems discussed above) including: limits on the task carried out; the ability of the system to discriminate targets; controls over the operational environment, such as limitations in time and space; and the ability for humans to communicate with the weapon system, for example to deactivate it. The latter is particularly difficult for underwater systems. There are other anti-personnel weapons that may be capable of autonomously selecting and attacking targets, such as so called “sentry” weapons used to defend facilities and borders. However, the systems in use today apparently remain under remote control for initiation of attacks. In sum, autonomy for selecting and attacking targets in existing weapon systems is limited by the operational parameters described above. Moreover, the technical characteristics and performance of existing weapon systems, combined with the operational parameters of their use, provide a certain degree of predictability of the outcomes of using these weapon systems. This predictability may be lost as autonomous weapon systems are used for more complex tasks or deployed in more dynamic environments than has been the case until now. 3. Emerging technology and future autonomous weapons Although it is difficult to foresee the future development of autonomous weapon systems, it is clear there are a number of military drivers for increased autonomy, including enabling: increased mobility of robotic/unmanned weapon systems or platforms; operation of these systems in “communications denied” environments; shorter decision-making times between identifying and attacking a target; increased performance over remotely operated systems; and operation of increased numbers of robotic/unmanned weapon systems by fewer operators. Interest in autonomous weapon systems could also develop in different ways among non-State armed groups. The general trend in civilian robotics is towards supervised autonomy, where robotic systems are increasingly autonomous while human operators retain oversight and often the ability to intervene. 3 The degree of autonomy in a robotic system is related to the level of human intervention in its operation, both in terms of the degree of human intervention and the stages at which the intervention is made. Even so, machines can and do effectively take decisions that have been delegated to them by humans through their computer programming, and without the need to be “conscious” or to have human-like levels of intelligence. There are a number of developments that might make increasingly autonomous weapon systems become less predictable. These include: increased mobility, meaning the weapon system would encounter more varied environments over greater time periods; increased adaptability, such as systems that set their own goals or change their functioning in response to the environment (e.g. a system that defends itself against an attack) or even incorporate learning algorithms; and increased interaction of multiple weapon systems in self-organising swarms. In addition to decreasing the predictability of the weapon system, these developments could raise related problems for the validity of testing to ensure reliability. For example, autonomous weapon systems that could set their own goals, or even “learn” and adapt their functioning, would by their nature be unpredictable. Highly mobile autonomous weapon systems would also create problems of predictability with respect to the target of specific attacks, especially if a system moved over a wide area or carried out multiple attacks. 4. Legal and ethical implications of increasing autonomy in weapon systems It is clear that IHL rules on the conduct of hostilities are addressed to the parties to an armed conflict, more specifically to the human combatants and fighters, who are responsible for respecting them, and will be held accountable for violations. These obligations cannot be transferred to a machine. Still, in practical terms, the question remains, what limits are needed on autonomy in weapon systems to ensure compliance with IHL. Control exercised by human beings can take various forms and operate at different stages of the “life cycle” of an autonomous weapon system, including: 1) the development of the weapon system, including its programming; 2) the deployment and use of the weapon system, including the decision by the commander or operator to use or activate the weapon system; and 3) the operation of the weapon system during which it selects and attacks targets. It is clear that human control is exerted in the development and deployment stages of the weapon’s “life cycle”. It is in stage three, however, when the weapon is in operation – when it autonomously selects and attacks the target(s) – that the important question arises as to whether human control in the first two stages is sufficient to overcome minimal or no human control at this last stage, from a legal, ethical and military-operational standpoint (see also section 5 below). The assessment of whether an autonomous weapon system can be used in compliance with IHL may depend on the specific technical characteristics and performance of the weapon system and the intended and expected circumstances of its use. Certain technical characteristics and their interaction with different operational parameters could significantly affect this assessment, including:  The task the weapon system carries out;  The type of target the weapon system attacks;  The environment in which the weapon system operates;  The movement of weapon system in space;  The time-frame of operation of the weapon system;  The adaptability of the weapon system, i.e. its ability to adapt its behaviour to changes in its environment, to determine its own functions and to set its own goals;  Degree of reliability of the weapon system, i.e. robustness to failures and vulnerability to malfunction or hacking; and  Potential for human supervision and intervention to deactivate the weapon system. The combination of these technical characteristics and performance of the weapon system with the operational parameters of its use are critical to determining the foreseeable effects of the weapon – in other words, the predictability of the outcomes of using the weapon – and therefore in determining whether it can be used in conformity with IHL rules. 4 Indeed, deploying a weapon system whose effects are wholly or partially unpredictable would create a significant risk that IHL will not be respected. The risks may be too high to allow use of the weapon, or else mitigating the risks may require limiting or even obviating the weapons’ autonomy. In this respect, the last factor in the list above – human intervention – could be considered as a risk mitigation factor. The level of risk resulting from a decrease in predictability of the weapon system may be influenced by a number of factors, such as the environment in which the weapon is used. Predicting the outcome of using autonomous weapon systems may become increasingly difficult as the weapon systems become more complex or are given more freedom of action in their operations. For example, the legal assessment of an autonomous weapon system that carries out a single task against a limited type of target in a simple (uncluttered) environment, and that is stationary and limited in the duration of its operation (e.g. some existing missile and rocket defence systems) may conclude that there is an acceptable level of predictability, allowing for responsibility and accountability of the human operator. However, the conclusion may be very different regarding an autonomous weapon system that carries out multiple tasks (or is adaptable) against different types of targets in a complex (cluttered) environment, and that is mobile over a wide area and/or operating for a long duration. For the purposes of an assessment under IHL there is no legal distinction between an offensive attack and a defensive one, as they both constitute attacks under the law. Nevertheless, the distinction between defensive and offensive weapon systems may be of greater relevance from a militaryoperational or ethical perspective. The obligation to carry out legal reviews of new weapons under article 36 of Additional Protocol I to the Geneva Conventions is important to ensure that a State's armed forces are capable of conducting hostilities in accordance with its international obligations. The above challenges for IHL compliance will need to be carefully considered by States when carrying out legal reviews of any autonomous weapon system they develop or acquire. As with all weapons, the lawfulness of a weapon with autonomy in its critical functions depends on its specific characteristics, and whether, given those characteristics, it can be employed in conformity with the rules of IHL in all of the circumstances in which it is intended and expected to be used. The ability to carry out such a review entails fully understanding the weapon’s capabilities and foreseeing its effects, notably through testing. Yet foreseeing such effects may become increasingly difficult if autonomous weapon systems were to become more complex or to be given more freedom of action in their operations, and therefore become less predictable. Questions arise as to how IHL’s “targeting rules” (e.g. the rules of proportionality and precautions in attack) are considered in reviewing weapons. Where it is the weapon itself that takes on the targeting functions, the legal review would demand a very high level of confidence that the weapon is capable of carrying out those functions in compliance with IHL. An additional challenge for reviewing the legality of an autonomous weapon system is the absence of standard methods and protocols for testing and evaluation to assess the performance of these weapons, and the possible risks associated with their use. Questions arise regarding: How is the reliability (e.g. risk of malfunction or vulnerability to cyber-attack) and predictability of the weapon tested? What level of reliability and predictability are considered to be necessary? The legal review procedure faces these and other practical challenges to assess whether an autonomous weapon system will perform as anticipated in the intended or expected circumstances of use. Although there are different views on the adequacy of national legal reviews of new weapons for ensuring IHL compliance of autonomous weapon systems, especially given the low level of implementation among States, this mechanism remains a critical measure for States to ensure respect for IHL. In any case, efforts to strengthen national legal review processes should be seen as complementary and mutually reinforcing of CCW discussions at the international level. Some have raised concerns that use of autonomous weapon systems may lead to an “accountability gap” in case of violations of IHL. Others are of the view that no such gap would ever exist as there will always be a human involved in the decision to deploy the weapon to whom responsibility could be attributed. 5 Under IHL and international criminal law, the limits to control over, or the unpredictability of, an autonomous weapon system could make it difficult to find individuals involved in the programming and deployment of the weapon liable for serious violations of IHL. They may not have the knowledge or intent required for such a finding, owing to the fact that the machine can select and attack targets independently. Programmers might not have knowledge of the concrete situations in which at a later stage the weapon system might be deployed and in which IHL violations could occur. On the other hand, a programmer who intentionally programmes an autonomous weapon to commit war crimes would certainly be criminally liable. Likewise, a commander would be liable for deciding to use an autonomous weapon system in an unlawful manner, for example deploying in a populated area an anti-personnel autonomous weapon that is incapable of distinguishing civilians from combatants. In addition, a commander who knowingly decides to deploy an autonomous weapon whose performance and effects he/she cannot predict may be held criminally responsible for any serious violations of IHL that ensue, to the extent that his/her decision to deploy the weapon is deemed reckless under the circumstances. Overall, as long as there will be a human involved in the decision to deploy the weapon to whom responsibility could be attributed, there might not be an accountability gap. Under the law of State responsibility a State could be held liable for violations of IHL caused by the use of any autonomous weapon system. Indeed under general international law governing the responsibility of States, they would be held responsible for internationally wrongful acts, such as violations of IHL committed by their armed forces using autonomous weapon systems. A State would also be responsible if it were to use an autonomous weapon system that it has not, or has inadequately, been tested or reviewed prior to deployment. Autonomous weapon systems also raise ethical concerns that deserve careful consideration. The fundamental question at the heart of concerns, and irrespective of whether they can be used in compliance with IHL, is whether the principles of humanity and the dictates of public conscience would allow machines to make life-and-death decisions in armed conflict without human involvement. The debates of recent years among States, experts, civil society and the public have shown that there is a sense of deep discomfort with the idea of any weapon system that places the use of force beyond human control. The question remains, however, what degree of human control is required, and in which circumstances, in light of ethical considerations? Is it sufficient for a human being to program an autonomous weapon system according to certain parameters and then make the decision to deploy it in a particular context? Or is it necessary that a human being bring his or her judgment to bear also on each individual attack? If the weapon autonomously uses force against a human target, what ethical considerations would this entail? 5. Human control The notion of human control has become the overarching issue in the debates on autonomous weapon systems. There is broad agreement that human control over weapon systems and the use of force must be retained, although less clarity on whether this is for legal, ethical, military operational, and/or policy reasons, and what makes it “meaningful”, “appropriate” or “effective”. From the ICRC’s perspective, a focus on the role of the human in the targeting process and the humanmachine interface could provide a fruitful avenue for increasing understanding of concerns that may be raised by autonomous weapon systems, rather than a purely technical focus on the ‘level of autonomy’ of weapon systems. A certain level of human control over attacks is inherent in, and required to ensure compliance with, the IHL rules of distinction, proportionality and precautions in attack. Considering more closely what these requirements are could help determine the boundaries of what is acceptable under IHL with respect to autonomy in the critical functions of selecting and attacking targets. There are already a number of considerations that have been suggested, which provide avenues for future work to establish these requirements, including:  Predictability of the weapon system in its intended or expected circumstances of use;  Reliability of the weapon system in its intended or expected circumstances of use; 6  Human intervention in the functioning of the weapon system during its development, deployment and use;  Knowledge and accurate information about the functioning of the weapon system and the context of its intended or expected use; and  Accountability for the functioning of the weapon system following its use. Many of the technical characteristics and operational parameters that are relevant to assessing compliance with IHL (see section 4) are also important factors for determining the requisite human control over the use of force, as well as relevant for military-operational or ethical considerations. For example, human control over existing autonomous weapon systems (see section 2) is largely governed by technical and operational constraints on the functioning of the system (e.g. limited tasks and targets, limits in space and time, physical controls over the environment, and human supervision and ability to deactivate). The military have a clear interest in maintaining human control of weapon systems, both to ensure compliance with IHL and to ensure that the commander has control over a given military operation. Rules of engagement associated with particular weapon systems are a primary way in which operational control is exerted over weapon systems and the use of force, and are therefore an important element in ensuring human control over the weapon system and the use of force. Deeper consideration of the elements constituting human control and understanding human-machine interaction is needed to help determine the boundaries necessary to ensure that human control is maintained over weapon systems and the use of force. As outlined above, there are already concerns that autonomous weapon systems which could adapt or change their functioning, and those that ‘hunt’ for targets over wide areas could raise serious questions about human control and IHL compliance due to the lack of predictability on when and where the use of force and specific attacks would take place. 6. The way forward There are at least three broad approaches that States could take to address the legal and ethical questions raised by autonomous weapon systems. The first regards strengthening national mechanisms for legal review and implementation of IHL to ensure any new weapons, including autonomous weapon systems, can be used in compliance with IHL. The second is for States to develop a definition of “lethal autonomous weapon systems” in terms of the weapon systems that may be problematic from a legal and/or ethical perspective with a view to establishing specific limits on autonomy in weapon systems. The third approach is for States to develop the parameters of human control in light of the specific requirements under IHL and ethical considerations (principles of humanity and the dictates of public conscience), thereby establishing specific limits on autonomy in weapon systems. Both the second and third approaches recognise that international consideration is needed of the limits of autonomy in weapons systems to ensure legal compliance and ethical acceptability. Ultimately these two approaches might lead to the same end point in terms of identifying weapon systems requiring possible regulation or prohibition. From the ICRC’s perspective, the third approach focussing on human control and the human-machine interaction could be an effective way forward for the CCW, with efforts to strengthen national legal reviews to be pursued in parallel as a mutually reinforcing initiative. The framework of human control provides a useful baseline from which common understandings can be developed among States, and through which boundaries or limits on autonomy in weapon systems can be established. This is consistent with the broad agreement among States, experts and other stakeholders that there is a need to maintain human control over weapon systems and the use of force in view of legal obligations, military operational requirements, and ethical considerations. + + + Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 13–17 November 2017, Geneva Statement of the International Committee of the Red Cross (ICRC) The International Committee of the Red Cross (ICRC) welcomes this first meeting of the Group of Governmental Experts on “Lethal Autonomous Weapons Systems”. With the High Contracting Parties to the Convention on Certain Conventional Weapons moving on to more formal discussions there is now an expectation that States will identify and build on common ground, such as the broad agreement that human control must be retained over weapon systems and the use of force. The ICRC’s view is that States must now work to establish limits on autonomy in weapon systems to ensure compliance with international law and to satisfy ethical concerns. In determining these limits, the ICRC is convinced that the focus must remain on obligations and responsibilities of humans in decisions to use force. For this reason, it has proposed that States assess the type and degree of human control required in the use of autonomous weapon systems – broadly defined as weapons with autonomy in their critical functions of selecting and attacking targets – to ensure compliance with international law, and acceptability under the principles of humanity and the dictates of the public conscience From the perspective of international humanitarian law, it is clear that the rules on the conduct of hostilities are addressed to those who plan, decide upon, and carry out an attack. These rules, which apply to all attacks regardless of the means or methods employed, give rise to obligations for human combatants, who are responsible for respecting them. These legal obligations, and accountability for them, cannot be transferred to a machine, a computer program, or a weapon system. In the ICRC’s view, compliance with these legal obligations would require that combatants retain a minimum level of human control over the use of weapon systems to carry out attacks in armed conflict. An examination of the way in which human control can be exerted over autonomous weapon systems (as broadly defined by the ICRC) points to the following key elements of human control to ensure legal compliance: predictability; human supervision and ability to intervene; and various operational restrictions, including on tasks, types of targets, the operating environment, time frame of operation, and scope of movement. Meaningful, effective or appropriate human control also requires that the operator have sufficient information on and understanding of the weapon system and operating environment, and the interaction between them. 2 These elements of human control are needed to link the decisions of the human commander or operator – which must comply with international humanitarian law and other applicable provisions of international law – to the outcome of a specific attack using the weapon system. The need for some degree of human control indicates that there will be limits to lawful levels of autonomy in weapon systems under international humanitarian law. The need for human control also raises concerns about the technical aspects of weapon system design that may lead to unpredictability. In particular, the use of artificial intelligence (AI) machine-learning algorithms in targeting would raise fundamental legal concerns to the extent that their functioning and outcomes would be inherently unpredictable. In recent months the ICRC has further evaluated the ethical aspects of autonomous weapon systems; it convened a small round-table of experts in August 2017 and has built on the outcomes of the expert meetings held with States in 2014 and 2016. The principles of humanity and the dictates of the public conscience provide moral guidance for these discussions, and support the ICRC’s call to set limits on autonomy in weapon systems. Perhaps the most significant ethical issues raised by autonomous weapon systems are those which transcend both the context of their deployment – whether in armed conflict or in peacetime – and the technology involved – whether simple or sophisticated. These concerns focus on the loss of human agency and responsibility in decisions to kill and destroy. Since moral responsibility for decisions to kill and destroy cannot be delegated to machines, meaningful, effective or appropriate human control – from an ethical point of view – would be the type and degree of control that preserves human agency and responsibility in these decisions. With increasing autonomy in weapon systems, a point may be reached where humans are so far removed in time and space from the acts of selecting and attacking targets that human decision-making is effectively substituted with computer-controlled processes, and life-anddeath decisions ceded to machines. This raises profound ethical questions about the role and responsibility of humans in the use of force and the taking of human life, which go beyond questions of compliance with international humanitarian law. With respect to the public conscience, there is a sense of deep discomfort with the idea of any weapon system that places the use of force beyond human control. With these legal and ethical questions in mind, and conscious of rapid technological developments in the fields of robotics and artificial intelligence and their current application in weapon systems, the ICRC urges all States present at this Group of Governmental Experts to work on establishing limits on autonomy in weapon systems, so that humans remain fully responsible for decisions to use force. Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 27-31 August 2018, Geneva Statement of the International Committee of the Red Cross (ICRC) under agenda item 6(b) Thank you, Mr Chair. The ICRC is encouraged to hear the emphasis and general agreement among delegations that human control must be maintained over weapon systems and the use of force. The ICRC would like to take this opportunity to recall several points we raised in our statements to the April Group of Governmental Experts on the legal and ethical basis for human control. Further we would like to offer some insights from a small meeting of technical experts we convened in June 2018 on the technical aspects of human control. Firstly, on the legal basis for human control, the ICRC is clear that it is humans who apply international law and are obliged to respect it. International humanitarian law rules on the conduct of hostilities require that those who plan, decide upon and carry out attacks make certain judgements when launching an attack. It follows that human combatants will need to retain a level of control over weapon systems and the use of force so that they can make context-specific legal judgements – of distinction, proportionality and precautions – in specific attacks. Legal concerns will arise where the design and/or the use of a weapon system with autonomy in its critical functions prevents the commander or operator from making the necessary judgements required by international humanitarian law. Secondly, in the ICRC’s view, ethical considerations also demand human control over weapon systems and the use of force. From an ethical perspective, (minimum/sufficient/effective/meaningful /appropriate) human control would be the type and degree that preserves human agency and upholds moral responsibility in decisions to use force. Mr Chair, In June 2018, ICRC convened a small meeting of independent experts in civilian artificial intelligence (AI), robotics and autonomous systems to better understand the technical aspects of human control. A report will be published in due course but two broad themes emerged: The first theme was that all autonomous robotic systems, which operate based on interaction with their environment, raise questions about human control and predictability. The greater the complexity of the environment and complexity of the task, the greater the need for human control and the less tolerance of autonomy. Humans can exert some control over autonomous systems through “human on the loop” interaction with them. However, this is not a panacea due to human-machine interaction problems, such as automation bias, over-trust in the system, or lack of operator awareness of the system state at the time of intervention. ICRC statement, CCW GGE on “LAWS”, 27-31 August 2018. 2 Further, quantifying the level of predictability (and reliability) required to ensure (minimum/sufficient /effective/meaningful/appropriate) human control, or judgement, is very difficult. Testing also raises unique challenges since it is not possible to test all possible environmental inputs to an autonomous system. However, setting boundaries – or operational constraints – in the operation of an autonomous robotic system – for example, on the task, time-frame of operation, scope of movement over an area, and operating environment – can contribute to increasing predictability. The second key theme that emerged from discussions was that AI - and especially machine learning – algorithms bring a new dimension of unpredictability to autonomous robotic systems. This is due both to the lack of transparency (“black-box” nature) in how the algorithms function, and the changing of their functioning over time. Not only are such algorithms unpredictable, but they can also introduce bias, whether by design or through bias in the data used to “train” (develop) the algorithms. This has reinforced the ICRC’s concerns about the unpredictability of machine learning algorithms, and the consequences if applied to weapon systems and the critical functions of selecting and attacking targets. Mr Chair, In closing, we would like to draw attention to the comments of the ICRC’s President published today, which stressed the importance of retaining human control over weapon systems and the use of force. He notes that, in some ways, recognising the need for human control is the easy part. The more difficult question is: how much and what type? It is this issue where the ICRC believes this Group of Governmental Experts should focus its work over the coming days, weeks and months. ICRAC Statement on the human control of weapons systems Mr Chairperson, We have been very pleased with this mornings session as states begin to contemplate a move towards policies on the human control of weapons systems. On a pedantic note that we cannot talk about the meaningful human control of LAWS as that would make them no longer and autonomous weapon. In the view of ICRAC, the control of weapons systems is more nuanced than can be captured by terms such as in-the-loop, on-the-loop, the broader loop, looping-the loop, human oversight, and appropriate human judgement. In this way we agree strongly with the statement made by Brazil and several others in this session who believe that the devil is in the detail. For human control to be meaningful we need to examine how humans interact with machines and understand the types of human-machine biases that can occur in the selection of legitimate targets. Lessons should be learned from 30 years of research on human supervisory control of machinery and more than 100 years of research on the psychology of human reasoning. This combination of this work can help us to design human-machine interfaces that allow weapons to be controlled in a manner that is fully compliant with international law and the principle of humanity. First, there should be a focus on what the human operator MUST do in the targeting cycle. This is control by use which is governed by targeting rules under International Humanitarian Law and International Human Rights Law. Further, international law rules that apply after the use of weapons – such as those that relate to human responsibility – must be satisfied. Second, the design of weapon systems must render them INCAPABLE of operating without meaningful human control. This is control by design, which is governed by international weapons law. In terms of international weapons law, if the weapon system, by its design, is incapable of being sufficiently controlled in terms of the law, then such a weapon is illegal per se. Ideally the following three conditions should be followed for the control of weapons systems: 1. a human commander (or operator) will have full contextual and situational awareness of the target area for each and every attack and is able to perceive and react to any change or unanticipated situations that may have arisen since planning the attack. 2. there will be active cognitive participation in every attack with sufficient time for deliberation on the nature of any target, its significance in terms of the necessity and appropriateness of attack, and likely incidental and possible accidental effects of the attack and 3. there will be a means for the rapid suspension or abortion of every attack. Thank you Mr Chairperson CHECK AGAINST DELIVERY Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 9–13 April 2018, Geneva Statement of the International Committee of the Red Cross (ICRC) Possible options for addressing the humanitarian and international security challenges posed by emerging technologies in the area of lethal autonomous weapons systems in the context of the objectives and purposes of the Convention without prejudging policy outcomes and taking into account past, present and future proposals Thank you, Mr Chair, and for ably guiding our work this week. The ICRC is pleased with how discussions have progressed, and appreciate the constructive engagement of delegations, though clearly much work remains to be done if common understandings are to be reached on key issues by the GGE’s next meeting in August. Discussions on “policy options” should be grounded on three truisms: Firstly, all new technologies of warfare must fit with existing law, not the other way around. Any weapon with autonomy in its critical functions of selecting and attacking targets must be capable of being used, and must be used, in accordance with international humanitarian law (IHL). This is undisputed. However, the unique implications of these weapons – namely the risk of loss of human control, and loss of human agency and intent, in targeting decisions -­-­ raise unique challenges for legal compliance and for humanity, which in turn raise the question of whether new internationally agreed policies, standards or rules are needed. Secondly, “policy options” must be informed by reality – the reality of how autonomy in is being employed in weapon systems today and may be applied in the foreseeable future, and the evidence of the weapons’ capabilities and limitations, their relative risks and benefits. Experience with existing weapon systems with autonomy in their critical functions can provide insights here. Thirdly, the further weaponization of autonomy is not inevitable, it is a choice. And in making that choice, we must put humans – and their legal obligations and moral responsibilities – first, not technology. Mr Chair It is ICRC’s view is that human control must be retained over weapon systems and the use of force and, therefore, the way forward should be determine the type and degree of human control necessary – in the use of weapon systems with autonomy in their critical functions – to ensure compliance with international (humanitarian) law, and ethical acceptability. With general agreement that “meaningful”, “effective” or “appropriate” human control must be retained, this approach will enable States to 2 • establish meaningful, and internationally agreed, limits on autonomy in weapon systems that address legal, ethical and humanitarian concerns, and • identify the specific characteristics of “autonomous weapon systems of concern”. The criteria for the minimum level of human control should be driven by the necessity to preserve human judgement and responsibility in targeting decisions, where predictability and human supervision are important factors. This may be seen as a positive obligation to ensure human control over weapon systems and the use of force, in line with legal obligations and ethical considerations. And, at this stage, the ICRC welcomes efforts to reach common understandings on the type and degree of human control required. Thank you. ICRC commentary on the ‘Guiding Principles’ of the CCW GGE on ‘Lethal Autonomous Weapons Systems’ The ‘Guiding Principles’ agreed by the Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts (GGE) on ‘Emerging Technologies in the Area of Lethal Autonomous Weapons Systems’ provide a useful basis for orientating the future work of States towards agreeing an effective ‘normative and operational framework’ to address autonomous weapon systems.1 This commentary groups the Guiding Principles under three main themes that, in the view of the ICRC, deserve States’ focused attention. 1. International humanitarian law limits the development and use of autonomous weapon systems It was affirmed that international law, in particular the United Nations Charter and International Humanitarian Law (IHL) as well as relevant ethical perspectives, should guide the continued work of the Group. Noting the potential challenges posed by emerging technologies in the area of lethal autonomous weapons systems to IHL, the following were affirmed, without prejudice to the result of future discussions The ICRC welcomes States’ unequivocal affirmation that both international law and ethical perspectives should guide the work of the GGE. The development and use of autonomous weapon systems is limited by international law, in particular the general rules of IHL governing the choice of means and methods of warfare and the specific treaty and customary rules prohibiting or restricting certain weapons.2 Additional constraints may derive from ethical considerations, including from the principles of humanity and the dictates of public conscience.3 (a) International humanitarian law continues to apply fully to all weapons systems, including the potential development and use of lethal autonomous weapons systems IHL regulates the conduct of armed conflict and seeks to limit its effects. It protects people not taking part in hostilities (such as civilians) and those who are no longer doing so (such as wounded or surrendered combatants). During an armed conflict, IHL governs the use of weapons, means and methods of warfare in the conduct of hostilities, including autonomous weapon systems. Outside armed conflict, the use of weapons is primarily governed by international human rights law, which is applicable at all times. In the view of the ICRC, autonomous weapon systems raise challenges for compliance with IHL. The rules on the conduct of hostilities, notably the rules of distinction, proportionality and precautions in attack, already set limits on the use of autonomous weapon systems, although many legal questions require clarification, and ethical concerns may demand limits that go beyond those found in existing law.4 The key question is not whether IHL applies to autonomous weapon systems in armed conflict, but how IHL is applied, that is, how IHL rules are and should be interpreted and implemented in practice, and whether new legally binding rules, policy standards or best practices are needed.5 1 United Nations, Report of the 2019 session of the Group of Governmental Experts on Emerging Technologies in the Area of Lethal Autonomous Weapons Systems. CCW/GGE.1/2019/3, 25 September 2019, Annex IV. 2 ICRC, International Humanitarian Law and the Challenges of Contemporary Armed Conflicts. Report to the 33rd International Conference of the Red Cross and Red Crescent, October 2019, Section 2 B, pp. 29–31. 3 Convention on Prohibitions or Restrictions on the Use of Certain Conventional Weapons Which May Be Deemed to be Excessively Injurious or to have Indiscriminate Effects, as amended on 21 December 2001, Preamble; ICRC, Ethics and autonomous weapons systems: An ethical basis for human control? Working Paper for the CCW GGE on LAWS, Geneva, 9-13 April 2018, CCW/GGE.1/2018/WP.5, 29 March 2018. 4 ICRC, International Humanitarian Law and the Challenges of Contemporary Armed Conflicts, 2019, op. cit. 5 ICRC, States must address concerns raised by autonomous weapons, Statement to the Meeting of the High Contracting Parties to the CCW, 14 November 2019; ICRC, International Humanitarian Law and the Challenges of Contemporary Armed Conflicts, 2019, op. cit.; ICRC, Statement under agenda item 5e. CCW 2 (e) In accordance with States’ obligations under international law, in the study, development, acquisition, or adoption of a new weapon, means or method of warfare, determination must be made whether its employment would, in some or all circumstances, be prohibited by international law States Parties to 1977 Additional Protocol I to the Geneva Conventions have a legal obligation to conduct legal reviews of new weapons.6 In the ICRC’s view, the requirement to carry out legal reviews also flows from the obligation to ensure respect for IHL. Besides these legal requirements, all States have an interest in assessing the lawfulness of new weapons.7 Effective legal reviews are critical to ensuring that a State’s armed forces comply with IHL in light of rapid technological developments. However, in the view of the ICRC, they are not sufficient alone to address the concerns raised by autonomous weapon systems given the complex legal and ethical questions involved, which require common understandings at the international level.8 Implementation of legal reviews of autonomous weapon systems raises practical challenges and questions, especially given the difficulties in foreseeing the likely consequences of use of autonomous weapon systems.9 In conducting reviews, particular attention should be given to measures needed to ensure human control over weapons and the use of force. (h) Consideration should be given to the use of emerging technologies in the area of lethal autonomous weapons systems in upholding compliance with IHL and other applicable international legal obligations (f) When developing or acquiring new weapons systems based on emerging technologies in the area of lethal autonomous weapons systems, physical security, appropriate non-physical safeguards (including cyber-security against hacking or data spoofing), the risk of acquisition by terrorist groups and the risk of proliferation should be considered (g) Risk assessments and mitigation measures should be part of the design, development, testing and deployment cycle of emerging technologies in any weapons systems Under IHL, all parties to armed conflict have a legal obligation to respect and ensure respect for IHL. This entails a duty to ensure that all weapons, means and methods of warfare, including autonomous weapon systems, are capable of being used, and are in fact used, in compliance with IHL and with other applicable international legal obligations (Guiding Principle (h)). These obligations, as well as additional obligations for States Parties to specific treaties, also demand consideration in the transfer of weapons (Guiding Principle (f)). Risk assessments and mitigation measures during the design, development, testing and deployment of new weapons may be required to ensure compliance with these legal obligations (Guiding Principle (g)), including as part of obligations to conduct legal reviews of new weapons (Guiding Principle (e)). 2. Human control is central to the legal compliance and ethical acceptability of autonomous weapon systems (c) Human-machine interaction, which may take various forms and be implemented at various stages of the life cycle of a weapon, should ensure that the potential use of weapons systems based on emerging technologies in the area of lethal autonomous weapons systems is in compliance with applicable international law, in particular IHL. In determining the quality and extent of human-machine interaction, a range of factors should be considered including the operational context, and the characteristics and capabilities of the weapons system as a whole For the ICRC, Guiding Principle (c) – together with Guiding Principles (b) and (d) – reflect the main risks posed by autonomous weapon systems: loss of human control over weapons and the use of force; GGE on LAWS, Geneva, 25–29 March 2019; Boulanin, V., Davison, N., Goussac, N. and Peldán Carlsson, M., Limits on Autonomy in Weapon Systems, ICRC & SIPRI, June 2020, Chapter 4, Recommendations 3 & 4. 6 ICRC, A Guide to the Legal Review of New Weapons, Means and Methods of Warfare, January 2006. 7 ICRC, International Humanitarian Law and the Challenges of Contemporary Armed Conflicts, 2019, op. cit., Section 2.E, pp. 34-35. 8 ICRC, Statement under agenda item 5e, op. cit. 9 Ibid. 3 diffusion, or abdication, of human responsibility for the consequences of their use; and practical challenges in ensuring accountability for violations of international law that may result. Measures pertaining to human control, responsibility and accountability – including but not limited to measures concerning ‘human-machine interaction’ and implemented throughout weapon development and use – are critical to ensuring compliance with applicable international law, in particular IHL, as well as ethical acceptability.10 Based on humanitarian, legal, and ethical considerations, as well as military operational realities, a recent report co-published by the ICRC and SIPRI proposes a combination of three types of control measures on autonomous weapon systems needed to satisfy legal obligations and ethical considerations: 1) controls on weapon parameters; 2) controls on the environment of use; and 3) controls through human-machine interaction.11 These measures should be considered in the use of autonomous weapon systems, as well as in their study, research, development and acquisition (Guiding Principle (e)).12 These types of control measures can inform internationally agreed limits on autonomous weapon systems, whether in the form of new legally binding rules, policy standards or best practices:13 • Controls on weapon parameters can inform limits on types of autonomous weapon systems including the targets they are used against, as well as limits on their duration and geographical scope of operation, and requirements for deactivation and fail-safe mechanisms. • Controls on the environment can inform limits on the situations and locations in which autonomous weapon systems may be used, notably, in terms of the presence and density of civilians and civilian objects. • Controls through human-machine interaction can inform requirements for human supervision, and ability to intervene and deactivate autonomous weapon systems, and requirements for predictable and transparent functioning. (b) Human responsibility for decisions on the use of weapons systems must be retained since accountability cannot be transferred to machines. This should be considered across the entire life cycle of the weapons system (d) Accountability for developing, deploying and using any emerging weapons system in the framework of the CCW must be ensured in accordance with applicable international law, including through the operation of such systems within a responsible chain of human command and control Legal obligations under IHL rules on the conduct of hostilities must be fulfilled by those persons who plan, decide on, and carry out military operations. It is humans, not machines, that comply with and implement these rules, and it is humans who can be held accountable for violations. Whatever the machine, computer program, or weapon system used, individuals and parties to conflicts remain responsible for their effects.14 Nevertheless, the way in which autonomous weapon systems function – i.e. independently selecting and applying force to targets without human intervention -- raises questions about the about the practical possibility of holding parties to conflict and individuals legally accountable for the consequences of their use, including for violations of IHL.15 10 ICRC, International Humanitarian Law and the Challenges of Contemporary Armed Conflicts, 2019, op. cit.; ICRC, Statement under agenda item 5a. CCW GGE on LAWS, Geneva, 25–29 March 2019; ICRC, Statement under agenda item 5c, CCW GGE on LAWS, Geneva, 25–29 March 2019; ICRC, The Element of Human Control, Working Paper for the Meeting of High Contracting Parties to the CCW 21-23 November 2018, Geneva, CCW/MSP/2018/WP.3, 19 November 2018; ICRC, Autonomy, artificial intelligence and robotics: Technical aspects of human control. Working Paper for the CCW GGE on LAWS, Geneva, 20-21 August 2019, CCW/GGE.1/2019/WP.7, 20 August 2019; ICRC, Ethics and autonomous weapons systems: An ethical basis for human control?, op. cit.; ICRC, Autonomous Weapon Systems: Implications of Increasing Autonomy in the Critical Functions of Weapons, August 2016; ICRC, Autonomous Weapon Systems: Technical, Military, Legal and Humanitarian Aspects, March & November 2014. 11 Boulanin, V., Davison, N., Goussac, N. and Peldán Carlsson, M., Limits on Autonomy in Weapon Systems, op. cit. Chapter 4, Recommendation 1. 12 Ibid., Chapter 4, Recommendation 5. 13 Ibid., Chapter 4, Recommendation 2. 14 ICRC, International Humanitarian Law and the Challenges of Contemporary Armed Conflicts, 2019, op. cit. 15 ICRC, International Humanitarian Law and the Challenges of Contemporary Armed Conflicts. Report to the 32nd International Conference of the Red Cross and Red Crescent, October 2015, Section VII I) ii), p. 46. 4 The rules on the conduct of hostilities – notably the rules of distinction, proportionality and precautions in attack – require complex assessments based on the circumstances prevailing at the time of the decision to attack, and during an attack. Commanders or operators must retain a level of human control over weapon systems sufficient to allow them to make context-specific judgments to apply the law in carrying out attacks.16 From an ethical perspective, human control is required to preserve human agency and uphold moral responsibility in decisions to use force. This requires a sufficiently direct and close connection to be maintained between the human intent of the user and the eventual consequences of the operation of the weapon system in a specific attack. Weapons, as inanimate objects, do not have moral agency and nor can they meaningfully be held responsible or accountable.17 Measures aimed at ensuring human control, responsibility and accountability are outlined under Guiding Principle (c). 3. Towards an effective multilateral response to autonomous weapon systems (k) The CCW offers an appropriate framework for dealing with the issue of emerging technologies in the area of lethal autonomous weapons systems within the context of the objectives and purposes of the Convention, which seeks to strike a balance between military necessity and humanitarian considerations In the view of the ICRC, the CCW – and the GGE on ‘Emerging Technologies in the Area of Lethal Autonomous Weapons Systems’ – offers an appropriate framework to address the risks posed by autonomous weapon systems falling within its scope. This is without prejudice to consideration of such risks in other relevant fora. In light of humanitarian, legal and ethical concerns, the ICRC reiterates its call to States at the GGE to urgently agree international limits on autonomous weapon systems. Rapid technological advances and military-doctrinal developments in a number of States indicate that the window for preventive action is fast closing.18 As Guiding Principles (b), (c) and (d) imply, an effective policy response to the risks posed by autonomous weapon systems requires consideration of what ‘quality and extent’ of human control is necessary and how ‘human responsibility’ and ‘accountability’ are ensured. Measures aimed at ensuring human control, responsibility and accountability can inform international limits on autonomous weapon systems. (i) In crafting potential policy measures, emerging technologies in the area of lethal autonomous weapons systems should not be anthropomorphized Legal obligations and ethical responsibilities rest with humans. Weapons, as inanimate objects, do not hold such obligations or responsibilities, and it should not be implied that they do (see also Guiding Principle (b)). (j) Discussions and any potential policy measures taken within the context of the CCW should not hamper progress in or access to peaceful uses of intelligent autonomous technologies Even the strictest measures taken to address the concerns raised by autonomous weapon systems can be crafted so as not to hamper progress in or access to relevant technologies for peaceful purposes. For example, the CCW Protocol IV prohibition of blinding laser weapons has not hampered progress in laser technology, nor have the Biological Weapons Convention or the Chemical Weapons Convention hampered progress in the peaceful uses of biology and chemistry. Geneva, July 2020 16 ICRC, International Humanitarian Law and the Challenges of Contemporary Armed Conflicts, op. cit. 17 ICRC, Ethics and autonomous weapons systems: An ethical basis for human control?, op. cit.; Boulanin, V., Davison, N., Goussac, N. and Peldán Carlsson, M., Limits on Autonomy in Weapon Systems, op. cit., Chapter 2, Section III. 18 ICRC, States must address concerns raised by autonomous weapons, op. cit.; ICRC, Statement under agenda item 5e, op. cit. Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 21-25 September 2020, Geneva Statement of the International Committee of the Red Cross The International Committee of the Red Cross (ICRC) welcomes the efforts of the High Contracting Parties to the Convention on Certain Conventional Weapons, as well as the Chair of the Group of Governmental Experts on Lethal Autonomous Weapons Systems, Ljupčo Jivan Gjorgjinski, to make progress in their important work, under difficult circumstances. The ICRC also appreciates the valuable work done by the outgoing Chair, Ambassador Janis Karklins, in collecting and summarizing commentaries by the High Contracting Parties on the 11 points of agreement, the Guiding Principles adopted at the end of 2019. His paper on commonalities illustrates the substantive common ground shared by States and other stakeholders. Autonomous weapon systems, as the ICRC understands them, select and apply force to targets without human intervention. To varying degrees, the user of the weapon will know neither the specific target nor the exact timing and location of the attack that will result. This raises serious concerns from a humanitarian, legal and ethical perspective, in particular the risk of losing human control over the use of force. The ICRC’s own commentary on the Guiding Principles focused on three overarching issues: - International humanitarian law regulates and limits the development and use of autonomous weapon systems in armed conflict. - Human control is critical to ensuring compliance with applicable international law, including international humanitarian law, as well as ethical acceptability. Specific measures to ensure human control pertain to weapon parameters, the environment of use, and human-machine interaction. - Internationally agreed limits on autonomous weapon systems are needed, whether in the form of new, legally binding rules, policy standards or best practices, to respond to the pressing humanitarian, legal and ethical concerns raised during deliberations in Geneva over the past seven years. It is clear to the ICRC that it is necessary to establish limits on the types of autonomous weapons used and the situations in which they are used, and these limits can be informed by measures to ensure human control. It is encouraging that many national submissions and the joint submissions by groups of States stressed similar points, which testifies to a growing convergence of views on these substantive issues. In the ICRC’s analysis, there is agreement on: the need to maintain human control, involvement or judgement and the rationale behind that need, including reducing the risk of harm to civilians, upholding legal obligations and ethical principles, and ensuring human responsibility and accountability. There is also a convergence of views on the types of measures that can contribute to ensuring human control, involvement or judgement, which include limits on tasks and target sets or profiles, temporal and spatial restrictions on the operation of the weapon, as well as requirements relating to human situational awareness, and intervention and deactivation capacities. Significantly, in his paper on commonalities, the Chair finds that further work is required to “determine the type and extent of human involvement or control necessary to ensure compliance with applicable law, notably international humanitarian law, and respond to ethical concerns.” In this respect, the ICRC 2 would also like to draw attention to a joint ICRC-SIPRI report published in June 2020, Limits on Autonomy in Weapon Systems: Identifying Practical Elements of Human Control, which sets out five recommendations for a way forward. The ICRC is grateful to the governments of the Netherlands, Sweden and Switzerland for supporting this independent study. The ICRC looks forward to engaging in greater depth on these issues during the course of discussions this week. Geneva, 9 - 13 April 2018 Item 6 of the provisional agenda Other matters Ethics and autonomous weapon systems: An ethical basis for human control? Submitted by the International Committee of the Red Cross (ICRC) Executive Summary In the view of the International Committee of the Red Cross (ICRC), human control must be maintained over weapon systems and the use of force to ensure compliance with international law and to satisfy ethical concerns, and States must work urgently to establish limits on autonomy in weapon systems. In August 2017, the ICRC convened a round-table meeting with independent experts to explore the ethical issues raised by autonomous weapon systems and the ethical dimension of the requirement for human control. This report summarizes discussions and highlights the ICRC’s main conclusions. The fundamental ethical question is whether the principles of humanity and the dictates of the public conscience can allow human decision-making on the use of force to be effectively substituted with computer-controlled processes, and life-and-death decisions to be ceded to machines. It is clear that ethical decisions by States, and by society at large, have preceded and motivated the development of new international legal constraints in warfare, including constraints on weapons that cause unacceptable harm. In international humanitarian law, notions of humanity and public conscience are drawn from the Martens Clause. As a potential marker of the public conscience, opinion polls to date suggest a general opposition to autonomous weapon systems – with autonomy eliciting a stronger response than remotecontrolled systems. Ethical issues are at the heart of the debate about the acceptability of autonomous weapon systems. It is precisely anxiety about the loss of human control over weapon systems and the use of force that goes beyond questions of the compatibility of autonomous weapon systems with our laws to encompass fundamental questions of acceptability to our values. A prominent aspect of the ethical debate has been a focus on autonomous weapon systems that are designed to kill or injure humans, rather than those that destroy or damage objects, which are already employed to a limited extent. The primary ethical argument for autonomous weapon systems has been resultsoriented: that their potential precision and reliability might enable better respect for both international law and human ethical values, resulting in fewer adverse humanitarian consequences. As with other weapons, such characteristics would depend on both the designdependent effects and the way the weapons were used. A secondary argument is that they CCW/GGE.1/2018/WP.5 Group of Governmental Experts of the High Contracting Parties to the Convention on Prohibitions or Restrictions on the Use of Certain Conventional Weapons Which May Be Deemed to Be Excessively Injurious or to Have Indiscriminate Effects 29 March 2018 English only CCW/GGE.1/2018/WP.5 2 would help fulfil the duty of militaries to protect their own forces – a quality not unique to autonomous weapon systems. While there are concerns regarding the technical capacity of autonomous weapons systems to function within legal and ethical constraints, the enduring ethical arguments against these weapons are those that transcend context – whether during armed conflict or in peacetime – and transcend technology – whether simple or sophisticated. The importance of retaining human agency – and intent – in decisions to use force, is one of the central ethical arguments for limits on autonomy in weapon systems. Many take the view that decisions to kill, injure and destroy must not be delegated to machines, and that humans must be present in this decision-making process sufficiently to preserve a direct link between the intention of the human and the eventual operation of the weapon system. Closely linked are concerns about a loss of human dignity. In other words, it matters not just if a person is killed or injured but how they are killed or injured, including the process by which these decisions are made. It is argued that, if human agency is lacking to the extent that machines have effectively, and functionally, been delegated these decisions, then it undermines the human dignity of those combatants targeted, and of civilians that are put at risk as a consequence of legitimate attacks on military targets. The need for human agency is also linked to moral responsibility and accountability for decisions to use force. These are human responsibilities (both ethical and legal), which cannot be transferred to inanimate machines, or computer algorithms. Predictability and reliability in using an autonomous weapon system are ways of connecting human agency and intent to the eventual consequences of an attack. However, as weapons that self-initiate attacks, autonomous weapon systems all raise questions about predictability, owing to varying degrees of uncertainty as to exactly when, where and/or why a resulting attack will take place. The application of AI and machine learning to targeting functions raises fundamental questions of inherent unpredictability. Context also affects ethical assessments. Constraints on the time-frame of operation and scope of movement over an area are key factors, as are the task for which the weapon is used and the operating environment. However, perhaps the most important factor is the type of target, since core ethical concerns about human agency, human dignity and moral responsibility are most acute in relation to the notion of anti-personnel autonomous weapon systems that target humans directly. From the ICRC’s perspective, ethical considerations parallel the requirement for a minimum level of human control over weapon systems and the use of force to ensure legal compliance. From an ethical viewpoint, “meaningful”, “effective” or “appropriate” human control would be the type and degree of control that preserves human agency and upholds moral responsibility in decisions to use force. This requires a sufficiently direct and close connection to be maintained between the human intent of the user and the eventual consequences of the operation of the weapon system in a specific attack. Ethical and legal considerations may demand some similar constraints on autonomy in weapon systems, so that meaningful human control is maintained – in particular, with respect to: human supervision and the ability to intervene and deactivate; technical requirements for predictability and reliability (including in the algorithms used); and operational constraints on the task for which the weapon is used, the type of target, the operating environment, the timeframe of operation and the scope of movement over an area. However, the combined and interconnected ethical concerns about loss of human agency in decisions to use force, diffusion of moral responsibility and loss of human dignity could have the most far-reaching consequences, perhaps precluding the development and use of anti-personnel autonomous weapon systems, and even limiting the applications of antimateriel systems, depending on the risks that destroying materiel targets present for human life. CCW/GGE.1/2018/WP.5 3 Contents Page 1. Introduction ............................................................................................................. 4 2. The principles of humanity and the dictates of the public conscience .................... 5 2.1 Ethics and the law .................................................................................. 5 2.2 The Martens Clause ................................................................................ 5 2.3 The public conscience in practice ........................................................... 6 3. The ethical debate on Autonomous Weapon Systems ............................................. 7 3.1 Main ethical arguments .......................................................................... 8 3.2 Human agency in decisions to use force ................................................ 9 3.3 Human dignity: process and results ....................................................... 10 4. Responsibility, accountability and transparency ..................................................... 11 4.1 Implications of autonomy for moral responsibility ................................ 11 4.2 Transparency in human-machine interaction ......................................... 13 5. Predictability, reliability and risk ............................................................................ 14 5.1 Artificial Intelligence (AI) and unpredictability ..................................... 15 5.2 Ethics and risk ........................................................................................ 16 6. Ethical issues in context .......................................................................................... 17 6.1 Constraints in time and space ................................................................. 17 6.2 Constraints in operating environments, tasks and targets ....................... 18 7. Public and military perceptions ............................................................................... 19 7.1 Opinion surveys...................................................................................... 19 7.2 Contrasting military and public perceptions ........................................... 20 8. Conclusions ............................................................................................................. 20 8.1 An ethical basis for human control? ....................................................... 22 CCW/GGE.1/2018/WP.5 4 1. Introduction 1. Since 2011, the ICRC has been engaged in debates about autonomous weapon systems, holding international expert meetings with States and independent experts in March 20141 and March 2016,2 and contributing to discussions at the United Nations Convention on Certain Conventional Weapons (CCW) since 2014. 2. The ICRC’s position is that States must establish limits on autonomy in weapon systems to ensure compliance with international humanitarian law and other applicable international law, and to satisfy ethical concerns. It has called on States to determine where these limits should be placed by assessing the type and degree of human control required in the use of autonomous weapon systems (broadly defined as weapons with autonomy in their critical functions of selecting and attacking targets)3 for legal compliance and ethical acceptability.4 3. As part of continuing reflections, the ICRC convened a two-day round-table meeting with independent experts to consider the ethical issues raised by autonomous weapon systems and the ethical dimension of the requirement for human control over weapon systems and the use of force.5 This report summarizes discussions at the meeting, 1 ICRC, Autonomous weapon systems: Technical, military, legal and humanitarian aspects, 2014 – report of an expert meeting: https://www.icrc.org/en/document/report-icrc-meeting-autonomousweapon- systems-26-28-march-2014. 2 ICRC, Autonomous weapon systems: Implications of increasing autonomy in the critical functions of weapons, 2016 – report of an expert meeting: https://www.icrc.org/en/publication/4283-autonomousweapons- systems. 3 The ICRC’s working definition of an autonomous weapon system is: “Any weapon system with autonomy in its critical functions. That is, a weapon system that can select (i.e. search for or detect, identify, track, select) and attack (i.e. use force against, neutralize, damage or destroy) targets without human intervention.” This definition encompasses a limited number of existing weapons, such as: anti-materiel weapon systems used to protect ships, vehicles, buildings or areas from incoming attacks with missiles, rockets, artillery, mortars or other projectiles; and some loitering munitions. There have been reports that some anti-personnel “sentry” weapon systems have autonomous modes. However, as far as is known to the ICRC, “sentry” weapon systems that have been deployed still require human remote authorization to launch an attack (even though they may identify targets autonomously). See: ICRC, Autonomous weapon systems: Implications of increasing autonomy in the critical functions of weapons, op. cit. (footnote 2), 2016, pp. 11–12. 4 ICRC, Statement to the Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on “Lethal Autonomous Weapon Systems”, 15 November 2017: https://www.icrc.org/en/document/expert-meeting-lethal-autonomous-weapons-systems; N Davison, “Autonomous weapon systems under international humanitarian law”, in Perspectives on Lethal Autonomous Weapon Systems, United Nations Office for Disarmament Affairs (UNODA) Occasional Papers No. 30, November 2017: https://www.un.org/disarmament/publications/occasionalpapers/unoda-occasional-papers-no-30- november-2017; ICRC, Views of the ICRC on autonomous weapon systems, 11 April 2016: https://www.icrc.org/en/document/views-icrc-autonomous-weapon-system. 5 The event was entitled “Ethics and autonomous weapon systems: An ethical basis for human control?” and was held at the Humanitarium, International Committee of the Red Cross (ICRC), Geneva, on 28 and 29 August 2017. With thanks to the following experts for their participation: Joanna Bryson (University of Bath, UK); Raja Chatila (Institut des Systèmes Intelligents et de Robotique, France); Markus Kneer (University of Zurich, Switzerland); Alexander Leveringhaus (University of Oxford, UK); Hine-Wai Loose (United Nations Office for Disarmament Affairs, Geneva); AJung Moon (Open Roboethics Institute, Canada); Bantan Nugroho (United Nations Office for Disarmament Affairs, Geneva); Heather Roff (Arizona State University, USA); Anders Sandberg (University of Oxford, UK); Robert Sparrow (Monash University, Australia); Ilse Verdiesen (Delft University of Technology, Netherlands); Kerstin Vignard (United Nations Institute for Disarmament Research); Wendell Wallach (Yale University, US); and Mary Wareham (Human Rights Watch). The ICRC was represented by: Kathleen Lawand, Neil Davison and Anna Chiapello (Arms Unit, Legal Division); Fiona Terry (Centre for Operational Research and Experience); and Sasha Radin (Law and Policy Forum). Report prepared by Neil Davison, ICRC. CCW/GGE.1/2018/WP.5 5 supplemented by additional research. The report highlights key themes and conclusions from the perspective of the ICRC, and these do not necessarily reflect the views of the participants. 4. For the ICRC, the fundamental question at the heart of ethical discussions is whether, irrespective of compliance with international law, the principles of humanity and the dictates of the public conscience can allow human decision-making on the use of force to be effectively substituted with computer- controlled processes, and life-and-death decisions to be ceded to machines. The ICRC’s concerns reflect the sense of deep discomfort over the idea of any weapon system that places the use of force beyond human control.6 And yet, important questions remain: at what point have decisions effectively, or functionally, been delegated to machines? What type and degree of human control are required, and in which circumstances, to satisfy ethical concerns? These are questions with profound implications for the future of warfare and humanity, and all States, as well as the military, scientists, industry, civil society and the public, have a stake in determining the answers. 2. The principles of humanity and the dictates of the public conscience 2.1 Ethics and the law 5. Ethics and law are intimately linked, especially where the purpose of the law – such as international humanitarian law and international human rights law – is to protect persons. This relationship can provide insights into how considerations of humanity and public conscience drive legal development. 6. The regulation of any conduct of hostilities, including regulating the choice of weapons, starts with a societal decision of what is acceptable or unacceptable behaviour, what is right and wrong. Subsequent legal restrictions are, therefore, a social construct, shaped by societal and ethical perceptions. These determinations evolve over time; what was considered acceptable at one point in history is not necessarily the case today.7 However, some codes of behaviour in warfare have endured for centuries – for example, the unacceptability of killing women and children, and of poisoning. 7. It is clear that ethical decisions by States, and by society at large, have preceded and motivated the development of new international legal constraints in warfare, and that in the face of new developments not specifically foreseen or not clearly addressed by existing law, contemporary ethical concerns can go beyond what is already codified in the law. This highlights the importance of not reducing debates about autonomous weapon systems, or other new technologies of warfare, solely to legal compliance. 2.2 The Martens Clause 8. In international humanitarian law, notions of humanity and public conscience are drawn from the Martens Clause, a provision that first appeared in the Hague Conventions of 1899 and 1907, was later incorporated in the 1977 Additional Protocols to the Geneva Conventions, and is considered customary law. It provides that, in cases not covered by existing treaties, civilians and combatants remain under the protection and authority of the principles of humanity and the dictates of the public conscience.8 The Martens Clause 6 ICRC, Statement to the Convention on Certain Conventional Weapons (CCW) Meeting of Experts on “Lethal Autonomous Weapons Systems”, 13 April 2015: https://www.icrc.org/en/document/lethalautonomous- weapons-systems-LAWS. 7 For example, among conventional weapons: expanding bullets, anti-personnel mines and cluster munitions. 8 It appears in the preamble to Additional Protocol II and in Article 1(2) of Additional Protocol I: “In cases not covered by this Protocol or by any other international agreements, civilians and combatants remain under the protection and authority of the principles of international law derived from established custom, from the principles of humanity and from dictates of public conscience.” CCW/GGE.1/2018/WP.5 6 prevents the assumption that anything that is not explicitly prohibited by relevant treaties is therefore permitted – it is a safety net for humanity. The provision is recognized as being particularly relevant to assessing new technologies and new means and methods of warfare.9 9. There is debate over whether the Martens Clause constitutes a legally-binding yardstick against which the lawfulness of a weapon must be measured, or rather an ethical guideline. Nevertheless, it is clear that considerations of humanity and public conscience have driven the evolution of international law on weapons, and these notions have triggered the negotiation of specific treaties to prohibit or limit certain weapons, as well as underlying the development and implementation of the rules of international humanitarian law more broadly.10 2.3 The public conscience in practice 10. In the development of international humanitarian law on weapons there is a strong ethical narrative to be found in the words used by States, the ICRC (mandated to uphold international humanitarian law) and civil society in raising concerns about weapons that cause, or have the potential to cause, unacceptable harm. For example, regarding weapons that cause superfluous injury or unnecessary suffering for combatants, in 1918, the ICRC, in calling for a prohibition of chemical weapons, described them as “barbaric weapons”, an “appalling method of waging war”, and appealed to States’ “feeling of humanity”.11 In advocating for a prohibition of blinding laser weapons, the ICRC appealed to the “conscience of humanity” and later welcomed the 1995 Protocol IV to the Convention on Certain Conventional Weapons (CCW) as a “victory of civilization over barbarity”.12 11. Likewise, addressing weapons that strike blindly, indiscriminately affecting civilians, the ICRC expressed an ethical revulsion over the “landmine carnage” and “appalling humanitarian consequences” of anti-personnel mines in debates leading to the prohibition of these weapons in 1997.13 The recent Treaty on the Prohibition of Nuclear Weapons, adopted in July 2017 by a group of 122 States, recognizes that the use of nuclear weapons would be “abhorrent to the principles of humanity and the dictates of public conscience”.14 The ethical underpinnings of restrictions in international humanitarian law on the use of certain weapons are not in dispute. 12. Civil society, medical, scientific and military experts, and the ICRC and other components of the International Red Cross and Red Crescent Movement, have played a key role in raising the attention of States to the unacceptable harm caused by certain weapons, such as anti-personnel mines and cluster munitions, building on evidence collected by those treating victims. Engagement in these endeavours by military veterans and religious figures, appeals to political leaders and parliamentarians, the testimony of victims and communication of concerns to the public were central to securing these prohibitions. In some debates, such as on blinding laser weapons, reflections by the military on the risks for their 9 International Court of Justice, Legality of the Threat or Use of Nuclear Weapons, Advisory Opinion, ICJ Reports, 1996, para. 78. 10 K Lawand and I Robinson, “Development of treaties limiting or prohibiting the use of certain weapons: the role of the International Committee of the Red Cross”, in R Geiss, A Zimmermann and S Haumer (eds.), Humanizing the laws of war: the Red Cross and the development of international humanitarian law, Cambridge University Press, 2017, pp. 141–184; M Veuthey, “Public Conscience in International Humanitarian Law”, in D Fleck (ed.), Crisis Management and Humanitarian Protection, Berliner Wissenschafts-Verlag, Berlin, 2004, pp. 611–642. 11 ICRC, World War I: the ICRC's appeal against the use of poisonous gases, 1918: https://www.icrc.org/eng/resources/documents/statement/57jnqh.htm. 12 L Doswald-Beck, “New Protocol on Blinding Laser Weapons”, International Review of the Red Cross, No. 312, 1996: https://www.icrc.org/eng/resources/documents/article/other/57jn4y.htm. 13 P Herby and K Lawand, “Unacceptable Behaviour: How Norms are Established”, in J Williams, S Goose and M Wareham (eds.), Banning Landmines: Disarmament, Citizen Diplomacy and Human Security, Lanham, MD: Rowman & Littlefield Publishers, 2008, p. 202. 14 UN General Assembly, Treaty on the Prohibition of Nuclear Weapons, preamble, A/CONF.229/2017/8, 7 July 2017. CCW/GGE.1/2018/WP.5 7 own soldiers were critical. All these various activities can be seen, in some way, as a demonstration of the public conscience.15 3. The ethical debate on Autonomous Weapon Systems 13. Ethical questions about autonomous weapon systems have sometimes been viewed as secondary concerns. Many States have tended to be more comfortable discussing whether new weapons can be developed and used in compliance with international law, particularly international humanitarian law, and with the assumption that the primary factors that limit the development and use of autonomous weapon systems are legal and technical. 14. However, for many experts and observers, and for some States, ethics – the “moral principles that govern a person’s behaviour or the conducting of an activity”16 – are at the heart of what autonomous weapon systems mean for the human conduct of warfare, and the use of force more broadly. It is precisely anxiety about the loss of human control over this conduct that goes beyond questions of the compatibility of autonomous weapon systems with our laws to encompass fundamental questions of acceptability to our values. 15. Ethical concerns over delegating life-and-death decisions, and reflections on the importance of the Martens Clause, have been raised in different quarters, including by: more than 30 States during CCW meetings,17 a UN Special Rapporteur at the Human Rights Council,18 Human Rights Watch19 (and the Campaign to Stop Killer Robots), the ICRC,20 the United Nations Institute for Disarmament Research (UNIDIR),21 academics and think-tanks, and, increasingly, among the scientific and technical communities.22 16. Discussions on autonomous weapon systems have generally acknowledged the necessity for some degree of human control over weapons and the use for force, whether for legal, ethical or military operational reasons (States have not always made clear for which reasons, or combination thereof).23 It is clear, however, that the points at which human control is located in the development and deployment, and exercised in the use, of a weapon with autonomy in the critical functions of selecting and attacking targets may be central to 15 K Lawand and I Robinson, op. cit. (footnote 10), 2017. 16 Oxford Dictionary of English: https://en.oxforddictionaries.com/definition/ethics. 17 Including: Algeria, Argentina, Austria, Belarus, Brazil, Cambodia, Costa Rica, Cuba, Ecuador, Egypt, France, Germany, Ghana, Holy See, India, Kazakhstan, Mexico, Morocco, Nicaragua, Norway, Pakistan, Panama, Peru, Republic of Korea, Sierra Leone, South Africa, Sri Lanka, Sweden, Switzerland, Turkey, Venezuela, Zambia and Zimbabwe. 18 Human Rights Council, Report of the Special Rapporteur on extrajudicial, summary or arbitrary executions, Christof Heyns, A/HRC/23/47, 9 April 2013. 19 Human Rights Watch, Losing Humanity: The Case against Killer Robots, 19 November 2012. 20 ICRC, Statement to CCW Meeting of Experts on “Lethal Autonomous Weapons Systems”, 13–17 April 2015: https://www.icrc.org/en/document/lethal-autonomous-weapons-systems-LAWS. 21 UNIDIR, The Weaponization of Increasingly Autonomous Technologies: Considering Ethics and Social Values, 2015. 22 Future of Life Institute, Autonomous Weapons: an Open Letter from AI & Robotics Researchers, 28 July 2015; Future of Life Institute, An Open Letter to the United Nations Convention on Certain Conventional Weapons, 21 August 2017. 23 United Nations, Report of the 2017 Group of Governmental Experts on “Lethal Autonomous Weapons Systems” (LAWS), CCW/GGE.1/2017/CRP.1, 20 November 2017, p.7: “The importance of considering LAWS “Lethal Autonomous Weapon Systems”] in relation to human involvement and the human-machine interface was underlined. The notions that human control over lethal targeting functions must be preserved, and that machines could not replace humans in making decisions and judgements, were promoted. Various related concepts, including, inter alia, meaningful and effective human control, appropriate human judgement, human involvement and human supervision, were discussed.” United Nations, Recommendations to the 2016 Review Conference Submitted by the Chairperson of the Informal Meeting of Experts, November 2016, p. 1: “V]iews on appropriate human involvement with regard to lethal force and the issue of delegation of its use are of critical importance to the further consideration of LAWS amongst the High Contracting Parties and should be the subject of further consideration”. CCW/GGE.1/2018/WP.5 8 determining whether this control is “meaningful”, “effective” or “appropriate” from an ethical perspective (and a legal one). 17. A prominent aspect of the ethical debate has been a focus on “lethal autonomy” or “killer robots” – implying weapon systems that are designed to kill or injure humans, rather than autonomous weapon systems that destroy or damage objects, which are already employed to a limited extent.24 This is despite the fact that some anti-materiel weapons can also result in the death of humans either directly (humans inside objects, such as buildings, vehicles, ships and aircraft) or indirectly (humans in proximity to objects), and that even the use of non-kinetic weapons – such as cyber weapons – can result in kinetic effects and in human casualties. Of course, autonomy in the critical functions of selecting and attacking targets is a feature that could, in theory, be applied to any weapon system. 18. Ethical discussions have also transcended the context-dependent legal bounds of international humanitarian law and international human rights law. Ethical concerns, relevant in all circumstances, have been at the centre of warnings by UN Special Rapporteur Christof Heyns that “allowing LARs Lethal Autonomous Robots] to kill people may denigrate the value of life itself”,25 and by Human Rights Watch that “fully autonomous weapons” would “cross a moral threshold” because of “the lack of human qualities necessary to make a moral decision, the threat to human dignity and the absence of moral agency”.26 3.1 Main ethical argument 19. Nevertheless, ethical arguments have been made both for and against autonomous weapon systems, reflecting, to a certain extent, the different emphases of consequentialist (results-focused) and deontological (process-focused) approaches. The primary argument for these weapons has been an assertion that they might enable better respect for both international law and human ethical values by enabling greater precision and reliability than weapon systems controlled directly by humans, and therefore would result in less adverse humanitarian consequences for civilians.27 This type of argument has been made in the past for other weapon systems, including, most recently, for armed drones, and it is important to recognize that such characteristics are not inherent to a weapon system but depend on both the design-dependent effects and the way the weapon system is used.28 20. Another ethical argument that has been made for autonomous weapon systems is that they help fulfil the duty of militaries to protect their soldiers by removing them from harm’s way. However, since this can equally apply to remote-controlled and remotely-delivered weapons, it is not a convincing argument for autonomy in targeting per se, apart from, perhaps, in scenarios where human soldiers cannot respond quickly enough to an incoming threat, such as in missile and close-in air defence. 24 See footnote 3 on existing autonomous weapon systems. Although the use of anti-materiel systems has not been without its problems and accidents – see, for example: J Hawley, Automation and the Patriot Air and Missile Defense System, Center for a New American Security (CNAS), 25 January 2017. 25 Human Rights Council, op. cit. (footnote 18), 2013, p. 20. 26 Human Rights Watch, Making the Case: The Dangers of Killer Robots and the Need for a Preemptive Ban, 9 December 2016. 27 See, for example on ethical compliance: R Arkin “Lethal Autonomous Systems and the Plight of the Non-combatant”, in AISIB Quarterly, July 2013. And on legal compliance: United States, Autonomy in Weapon Systems, Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on “Lethal Autonomous Weapon Systems”, CCW/GGE.1/2017/WP.6, 10 November 2017, pp. 3–4. 28 For example, remote-controlled armed drones with precision-guided munitions may offer the potential for greater precision and therefore less risk of indiscriminate effects. However, if the information about the target is inaccurate, targeting practices are too generalized, or protected persons or objects are deliberately, or accidentally, attacked, then the potential for precision offers no protection in itself. CCW/GGE.1/2018/WP.5 9 21. Ethical arguments against autonomous weapon systems can generally be divided into two forms: objections based on the limits of technology to function within legal constraints and ethical norms;29 and ethical objections that are independent of technological capability.30 22. Given that technology trajectories are hard to predict, it is the second category of ethical arguments that may be the most interesting for current policy debates. Do autonomous weapon systems raise any universal ethical concerns? Among the main issues in this respect are:  removing human agency from decisions to kill, injure and destroy31 – decisions to use force – leading to a responsibility gap where humans cannot uphold their moral responsibility32  undermining the human dignity of those combatants who are targeted,33 and of civilians who are put at risk of death and injury as a consequence of attacks on legitimate military targets  further increasing human distancing – physically and psychologically – from the battlefield, enhancing existing asymmetries and making the use of violence easier or less controlled.34 3.2 Human agency in decisions to use force 23. In ethical debates, there seems to be wide acknowledgement of the importance of retaining human agency35 – and associated intent – in decisions to use force, particularly in decisions to kill, injure and destroy. In other words, many take the view that “machines must not make life-and-death decisions” and “machines cannot be delegated responsibility for these decisions”.36 24. Machines and computer programs, as inanimate objects, do not think, see and perceive like humans. Therefore, some argue, it is difficult to see how human values can be respected if the “decision” to attack a specific target is functionally delegated to a machine. However, there are differing perspectives on the underlying question: at which point have decisions to use force effectively been delegated to a machine? Or, from another perspective: what limits 29 See, for example: N Sharkey, “The evitability of autonomous robot warfare”, International Review of the Red Cross, No. 886, 2012. 30 See, for example: P Asaro, “On banning autonomous weapon systems: human rights, automation, and the dehumanization of lethal decision-making”, International Review of the Red Cross, No. 886, 2012; R Sparrow, “Robots and respect: Assessing the case against Autonomous Weapon Systems”, Ethics and International Affairs, 30(1), 2016, pp. 93–116; A Leveringhaus, Ethics and Autonomous Weapon Systems, Palgrave Macmillan, UK, 2016. 31 A Leveringhaus, Ethics and Autonomous Weapon Systems, op. cit. (footnote 30), 2016. 32 See, for example: R Sparrow, “Killer robots”, Journal of Applied Philosophy, 24(1), 2007, pp. 62–77; H Roff, “Killing in War: Responsibility, Liability and Lethal Autonomous Robots”, in F Allhoff, N Evans and A Henschke (eds.), Routledge Handbook of Ethics and War: Just War Theory in the 21st Century, Routledge, UK, 2014. 33 See, for example: R Sparrow, op. cit. (footnote 30), 2016; C Heyns, “Autonomous weapons in armed conflict and the right to a dignified life: An African perspective”, South African Journal on Human Rights, Vol. 33, Issue 1, 2017, pp. 46–71. 34 A Leveringhaus, “Distance, weapons technology and humanity in armed conflict”, ICRC Humanitarian Law & Policy Blog, 6 October 2017: http://blogs.icrc.org/law-andpolicy/ 2017/10/06/distance-weapons-technology-and-humanity-in-armed-conflict. 35 N Castree, R Kitchin and A Rogers, A Dictionary of Human Geography, Oxford University Press, Oxford, 2013: “The capacity possessed by people to act of their own volition.” 36 See footnote 17 listing States that have raised core ethical concerns. For example: “Germany will certainly adhere to the principle that it is not acceptable, that the decision to use force, in particular the decision over life and death, is taken solely by an autonomous system without any possibility for a human intervention.” Statement to CCW Meeting of Experts on “Lethal Autonomous Weapon Systems”, 11–15 April 2016. CCW/GGE.1/2018/WP.5 10 on autonomy are required to retain sufficient human agency and intent in these decisions? 25. There is a parallel in this debate with landmines, which have been described as “rudimentary autonomous weapon systems”.37 When humans lay landmines they effectively remove themselves from the decision about subsequent attacks on specific people or vehicles. They may know where the landmines are placed but they do not know who, or what, will trigger them, or when they will be triggered. This could be seen as a primitive form of delegating the decision to kill and injure to a machine. 26. Some argue it is difficult to establish a clear point at which this shift in functional decision-making from human to machine happens, and human agency and intention have been eroded or lost. Rather, it may be more useful, some propose, to agree on the general principle that a minimum level of human control is required in order to retain human agency in these decisions, and then consider the way in which humans must inject themselves into the decision-making process and at what points, to ensure this control is sufficient – for example, through human supervision and the ability to intervene and deactivate; technical requirements for predictability and reliability; and operational constraints on the task the weapon is used for, the type of target, the operating environment, the timeframe of operation and the scope of movement over an area.38 3.3 Human dignity: process and results 27. Closely linked to the issue of human agency, and concerns about the delegation of decisions to use force, is human dignity. The central argument here is that it matters not just if a person is killed and injured but how they are killed and injured. Where a line has been crossed, and machines are effectively making life-and-death “decisions”, the argument is that this undermines the human dignity of those targeted, even if they are lawful targets (for example, under international humanitarian law). As Christof Heyns, then UN Special Rapporteur on extrajudicial, summary or arbitrary executions, put it: “to allow machines to determine when and where to use force against humans is to reduce those humans to objects; they are treated as mere targets. They become zeros and ones in the digital scopes of weapons which are programmed in advance to release force without the ability to consider whether there is no other way out, without a sufficient level of deliberate human choice about the matter.” 39 28. Unlike previous discussions about constraints on weapons (see Section 2.3), which have focused on their effects (whether evidence of unacceptable harm or foreseeable effects), the additional ethical concerns with autonomous weapon systems are about process as well as results. What does this method of using force reveal about the underlying attitude to human life, to human dignity? And, in that sense, these concerns are particularly relevant to the relationship between combatants in armed conflict, although they are also relevant to 37 United States Department of Defense, Department of Defense Law of War Manual, Section 6.5.9.1, Description and Examples of the Use of Autonomy in Weapon Systems, 2015, p. 328: “Some weapons may have autonomous functions. For example, mines may be regarded as rudimentary autonomous weapons because they are designed to explode by the presence, proximity, or contact of a person or vehicle, rather than by the decision of the operator.” There are different views on whether the complexity of the function delegated to a machine affects this ethical assessment. Some distinguish between an “automated function” (activation, or not, of a landmine) and an “autonomous function” with “choice” (e.g. selecting between different targets), but there are no clear lines between automated and autonomous from a technical perspective, and both can enable functional delegation of decisions. See, for example: ICRC, Autonomous weapon systems: Implications of increasing autonomy in the critical functions of weapons, op. cit. (footnote 2), 2016, p. 8. 38 ICRC, Statement to the Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on “Lethal Autonomous Weapon Systems”, op. cit. (footnote 4), 15 November 2017. 39 C Heyns, Autonomous Weapon Systems: Human rights and ethical issues, presentation to the CCW Meeting of Experts on “Lethal Autonomous Weapon Systems”, 14 April 2016. CCW/GGE.1/2018/WP.5 11 civilians, who must not be targeted, but are, nevertheless, exposed to collateral risks of death and injury from attacks on legitimate military targets. 29. For some, autonomous weapon systems conjure up visions of machines being used to kill humans like vermin, and a reduced respect for human life due to a lack of human agency and intention in the specific acts of using force. In this argument, delegating the execution of a task to a machine may be acceptable, but delegating the decision to kill or injure is not, which means applying human intent to each decision. 30. There are strong parallels with the broader societal discussion about algorithmic, and especially artificial intelligence (AI)-driven, decision-making, including military decision-making40 (see also Section 5.1). Through handing over too much of the functional decision-making process to sensors and algorithms, is there a point at which humans are so far removed in time and space from the acts of selecting and attacking targets that human decision-making is effectively substituted by computer-controlled processes? The concern is that, if the connection between the human decision to use force and the eventual consequences is too diffuse, then human agency in that decision is weakened and human dignity eroded. 31. The counter-argument to an emphasis on process is found in the primary argument for autonomous weapons systems (see Section 3.1) that they will offer better results, posing less risk to civilians by enabling the users to exercise greater precision and discrimination than with human-operated systems. However, claims about reduced risks to civilians – which remain contentious in the absence of supporting evidence – are very much context-specific, whereas ethical questions about loss of human dignity present more of a universal concern, independent of context. 4. Responsibility, accountability and transparency 32. Responsibility and accountability for decisions to use force cannot be transferred to a machine or a computer program.41 These are human responsibilities – both legal and ethical – which require human agency in the decision-making process (see Section 3). Therefore, a closely related ethical concern raised by autonomous weapon systems is the risk of erosion – or diffusion – of responsibility and accountability for these decisions. 33. One way to address this concern is to assign responsibility to the operator or commander who authorizes the activation of the autonomous weapon system (or programmers and manufacturers, in case of malfunction). This addresses the issue of legal responsibility to some extent, simply by applying a process for holding an individual accountable for the consequences of their actions.42 And this is how militaries typically address responsibility for operations using existing weapon systems, including, presumably, those with autonomy in their critical functions. 4.1 Implications of autonomy for moral responsibility 34. For the ethical debate, however, responsibility is not only a legal concept but also a moral one. Some argue that, in order for the commander or operator to uphold their moral responsibility in a decision to activate an autonomous weapon system, their intent needs to be directly linked to the eventual outcome of the resulting attack. This requires an understanding of how the weapon will function and the specific consequences of activating 40 D Lewis, G Blum and N Modirzadeh, War-Algorithm Accountability, Harvard Law School Program on International Law and Armed Conflict (HLS PILAC), Harvard University, 31 August 2016: https://pilac.law.harvard.edu/waa. 41 ICRC, Statement to the Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on “Lethal Autonomous Weapon Systems”, op. cit. (footnote 4), 15 November 2017. 42 Although there are still questions around whether a person can be criminally accountable in situations where they lack the required knowledge or intent of how the system will operate once activated, or where there is insufficient evidence to discharge the burden of proof. CCW/GGE.1/2018/WP.5 12 it in those circumstances, which is complicated by the uncertainty introduced by autonomy in targeting. Uncertainty brings a risk that the consequences of activating the weapon will not be those intended – or foreseen – by the operator (see Section 5.2), which raises both ethical and legal concerns. 35. An autonomous weapon system – since it selects and attacks targets independently (after launch or activation) – creates varying degrees of uncertainty as to exactly when, where and/or why the resulting attack will take place. The key difference between a human or remote-controlled weapon and an autonomous weapon system is that the former involves a human choosing a specific target – or group of targets – to be attacked, connecting their moral (and legal) responsibility to the specific consequences of their actions. In contrast, an autonomous weapon system self-initiates an attack: it is given a technical description, or a “signature”, of a target, and a spatial and temporal area of autonomous operation. This description might be general (“an armoured vehicle”) or even quite specific (“a certain type of armoured vehicle”), but the key issue is that the commander or operator activating the weapon is not giving instructions on a specific target to be attacked (“specific armoured vehicle”) at a specific place (“at the corner of that street”) and at a specific point in time (“now”). Rather, when activating the autonomous weapon system, by definition, the user will not know exactly which target will be attacked (“armoured vehicles fitting this technical signature”), in which place (within x square kilometres) or at which point in time (during the next x minutes/hours). Thus, it can be argued, this more generalized nature of the targeting decision means the user is not applying their intent to each specific attack. 36. The potential technical description, or signature, for an enemy combatant is both extremely broad and highly specific (e.g. combatant, fighter or civilian that is directly participating in hostilities but not one that is hors de combat or surrendering) and can vary enormously from one moment to the next. It is therefore highly doubtful that a weapon system could be programmed functionally to identify “enemy combatants”.43 But, assuming this might be possible for the sake of argument, if an anti-personnel autonomous weapon system encountered the signature of an enemy combatant it would attack when the signature matches its programming. A human decision-maker controlling a weapon system in the same circumstances still has a choice. S/he may decide to attack, or s/he may decide not to attack – even if the technical signature fits – including owing to wider ethical considerations in the specific circumstances, which may go beyond whether the combatant is a lawful target.44 (From a legal perspective, it is important to note that the principles of military necessity and humanity already require that the kind and degree of force used against lawful targets must not exceed what is necessary to accomplish a legitimate military purpose in the circumstances.)45 37. In sum, from an ethical perspective, the removal of the human intent from a specific attack weakens moral responsibility by preventing considerations of humanity. There may be a causal explanation for why these combatants were attacked (i.e. they corresponded to the target signature) but we may not be able to offer a reason, an ethical justification, for that attack (i.e. why were they attacked in the specific circumstances?). Since the process of reason-giving and justification establishes moral responsibility, and makes people feel they are treated justly, autonomous technology risks blocking this process and diminishing it. 43 This does not mean it is necessarily simple, functionally, to identify objects (e.g. vehicles, buildings), since they change status over time (between military objective and civilian object), and objects used by civilians and the military can share similar characteristics. 44 A Leveringhaus, Ethics and Autonomous Weapon Systems, op. cit. (footnote 30), 2016, pp. 92–93. 45 N Melzer, Interpretive guidance on the notion of direct participation in hostilities under international humanitarian law, ICRC, Geneva, 2016. Chapter IX: Restraints on the use of force in direct attack, p. 82: “In situations of armed conflict, even the use of force against persons not entitled to protection against direct attack remains subject to legal constraints. In addition to the restraints imposed by international humanitarian law on specific means and methods of warfare, and without prejudice to further restrictions that may arise under other applicable branches of international law, the kind and degree of force which is permissible against persons not entitled to protection against direct attack must not exceed what is actually necessary to accomplish a legitimate military purpose in the prevailing circumstances.” CCW/GGE.1/2018/WP.5 13 4.2 Transparency in human-machine interaction 38. Machine control and human control have different strengths and weaknesses. As currently understood, machines have limited decision-making capacities and limited situational awareness but can respond very quickly, and according to specific parameters (although, of course, this is a fast-developing field, especially with respect to artificial intelligence (AI) – see Section 5.1). In contrast, humans have a limited attention span and field of perception but global situational awareness of their environment, and sophisticated decision-making capacities. This difference gives rise to a number of problems in humanmachine interaction that are relevant to discussions about autonomous weapon systems, including: automation bias – where humans place too much confidence in the operation of an autonomous machine; surprises – where a human is not fully aware of how a machine is functioning at the point s/he needs to take back control; and the “moral buffer” – where the human operator shifts moral responsibility and accountability to the machine as a perceived legitimate authority.46 39. This raises additional questions about how moral responsibility and accountability can be ensured in the use of an autonomous weapon system, including whether there will be sufficient transparency in the way it operates, and its interaction with the environment, to be sufficiently understood by humans. To address this concern, a human operator may need to have continuous situational awareness during the operation of an autonomous weapon system, as well as a two-way communication link to receive information and give updated instructions to the system, if necessary, as well as sufficient time to respond or change the course of action, where necessary. 40. These types of human-machine problems are already evident in existing civilian autonomous systems. One example is the accident that resulted when the pilot of a passenger aircraft had to re-take control following a failure in the autopilot system but was not sufficiently aware of the situation to respond in the correct way.47 Other accidents have happened with car “autopilot” systems, where drivers relied too heavily on a system with limited capacity.48 And there are also parallels with autonomous financial trading systems, causing so-called “flash crashes” in ways not predictable by human traders overseeing them, and not preventable owing to the extremely short time-scales involved.49 46 M Cummings, “Automation and Accountability in Decision Support System Interface Design”, Journal of Technology Studies, Vol. XXXII, No. 1, 2006: “… decision support systems that integrate higher levels of automation can possibly allow users to perceive the computer as a legitimate authority, diminish moral agency, and shift accountability to the computer, thus creating a moral buffering effect”. 47 See, for example: R Charette, “Air France Flight 447 Crash Causes in Part Point to Automation Paradox”, IEEE Spectrum, 2012: https://spectrum.ieee.org/riskfactor/aerospace/aviation/air-franceflight- 447-crash-caused-by-a-combination-of-factors. 48 J Stewart, “People Keep Confusing Their Teslas for Self-Driving Cars”, Wired, 25 January 2018: https://www.wired.com/story/tesla-autopilot-crash-dui. 49 US Securities & Exchange Commission, Findings regarding the market events of 6 May, 2010. Reports of the staffs of the CFTC and SEC to the Joint Advisory Committee on Emerging Regulatory Issues, 30 September 2010. CCW/GGE.1/2018/WP.5 14 5. Predictability, reliability and risk 41. Unpredictability and unreliability have been raised as key issues for any legal assessment of autonomous weapon systems,50 as well as for the risks their use may pose,51 in particular for civilians. However, these factors are also closely connected to ethical questions of human agency and moral responsibility (see Sections 3 and 4). 42. One way to think about predictability and reliability in autonomous (weapon) systems is as means of connecting human agency and intent with the eventual outcome and consequences of the machine’s operation. Predictability is the ability to “say or estimate that (a specified thing) will happen in the future or will be a consequence of something”.52 Applied to an autonomous weapon system, predictability is knowledge of how it will likely function in any given circumstances of use, and the effects that will likely result. Reliability is “the quality of being trustworthy or performing consistently well”.53 In this context, reliability is knowledge of how consistently the system will function as intended, i.e. without failures or unintended effects. 43. Degrees of unpredictability and unreliability in the use of an autonomous weapon system might: be inherent to the technical design of the weapon system; arise from the nature of the environment (e.g. ‘uncluttered’ deep sea versus ‘cluttered’ populated area); and/or be due to the interaction of the weapon system with the environment. Unpredictability and unreliability in the environment may also vary over time and within a given area (depending on the nature of the environment). 44. If one recognizes the argument of the necessity for human agency and intent in decisions to use force (see Section 3) and the difficulties raised by autonomy for moral responsibility and accountability (see Section 4), it follows that the use of weapon systems that lead to unpredictable and unreliable consequences, and therefore heightened risks for civilians, will accentuate these ethical concerns. Unpredictability and unreliability, in that sense, are both legally and ethically problematic. However, predictability and reliability, in themselves, do not necessarily resolve ethical questions. For example, an autonomous weapon system might be highly predictable and reliable in attacking combatants, but it could still raise ethical concerns with respect to human agency and human dignity (see Section 3). 45. Of course, there are only ever degrees of predictability and reliability in complex software-controlled systems. Unpredictable and unreliable operations may result from a variety of factors, including: software errors and system flaws; human cognitive bias in dismissing certain possibilities; in-built algorithmic bias; 54 “normal accidents”, where there is no clear error, but a system still does not function as expected; and deliberate hacking, spoofing or cyber attacks. 46. It is also important to emphasize that nothing is one hundred per cent predictable and reliable, including non-autonomous, human-controlled, weapon systems. Although it is clear that a high degree would be demanded in safety-critical autonomous systems, such as weapon systems, questions remain about the level of predictability and reliability required to satisfy ethical (and legal) considerations. 50 N Davison, Autonomous weapon systems under international humanitarian law, op. cit. (footnote 4), 2017; ICRC, Views of the ICRC on autonomous weapon systems, op. cit. (footnote 4), 11 April 2016; W Wallach, “Predictability and Lethal Autonomous Weapons Systems (LAWS)”, in German Federal Foreign Office, Lethal Autonomous Weapons Systems: Technology, Definition, Ethics, Law & Security, 2016, pp. 295–312. 51 See, for example: P Scharre, Autonomous Weapons and Operational Risk, Center for a New American Security (CNAS), February 2016; UNIDIR, Safety, Unintentional Risk and Accidents in the Weaponization of Increasingly Autonomous Technologies, 2016. 52 Oxford Dictionary of English: https://en.oxforddictionaries.com/definition/predictability. 53 Ibid: https://en.oxforddictionaries.com/definition/reliability. 54 See, for example: A Caliskan, J Bryson and A Narayanan, “Semantics derived automatically from language corpora contain human-like biases”, Science, Vol. 356, Issue 6334, 2017, pp. 183–186; C O’Neil, Weapons of Math Destruction: How big data increases inequality and threatens democracy, Crown, New York, 2016. CCW/GGE.1/2018/WP.5 15 5.1 Artificial Intelligence (AI) and unpredictability 47. For many considering the implications of autonomous weapon systems, the key change in recent years – and a fundamental challenge for predictability – is the further development of artificial intelligence (AI), and especially AI algorithms that incorporate machine learning. In general, machine-learning systems can only be understood at a particular moment in time. The “behaviour” of the learning algorithm is determined not only by initial programming (carried out by a human) but also by the process in which the algorithm itself “learns” and develops by “experience”. This can be offline learning by training (before deployment) and/or online learning by experience (after deployment) while carrying out a task. 48. Deep learning – where an algorithm develops by learning data patterns rather than learning a specific task – further complicates the ability to understand and predict how the algorithm will function, once deployed. It can also add to the problem of biases that can be introduced into an algorithm through limitations in the data sets used to “train” it. Or a learning system may simply have learned in a way that was not intended by the developer. 49. Complicating matters further, humans’ current ability to interrogate machinelearning algorithms is limited. Such systems are often described as “back-boxes”; the inputs and outputs may be known but the process by which a system converts an input to an output is not known. This type of system can be tested to help determine its functioning in different environments. However, there are significant limits in current abilities to verify the functioning of these systems, a task that becomes harder the more actions there are in the repertoire of the system and the more complex the inputs. If a system continues to learn after being tested, then the verification and validation (checks to determine if a system will operate as intended in a given environment) are no longer meaningful. This type of autonomous system would be inherently unpredictable (owing to its technical design) and, if applied to targeting, for example, the link between human intent and eventual outcome would effectively be severed.55 50. Questions about AI and learning algorithms in weapon systems and targeting functions are no longer theoretical. As with civilian digital technology, big data are an increasingly important resource, and the focus of data exploitation and analysis efforts is on AI algorithms. For the military, this promises a capability advantage for decision-making in data-rich conflict environments. And despite the risks of unpredictability, which may conflict with military commanders’ propensity for command and control, there is significant and increasing interest among the major powers in the military applications of AI,56 including projects underway to apply machine learning to automatic target recognition and identification.57 AI systems may not even need to have a physical component to raise ethical 55 From a legal perspective, when considering the obligation of States to review new weapons before their deployment and use under Article 36 of Additional Protocol I to the Geneva Conventions, it is difficult to see how a weapon system that autonomously changes its own functioning could ever be approved, since what had been tested and verified at one point in time would not be valid for the future. See: ICRC, Autonomous weapon systems: Implications of increasing autonomy in the critical functions of weapons, op. cit. (footnote 2), 2016, p. 13. 56 See, for example: United States Department of Defense, Summer Study on Autonomy, Defense Science Board, June 2016; M Cummings, Artificial Intelligence and the Future of Warfare, Chatham House, International Security Department and US and the Americas Programme, January 2017; G Allen and T Chan, Artificial Intelligence and National Security, Harvard Kennedy School, Belfer Center for Science and International Affairs, 2017; E Kania, Battlefield Singularity. Artificial Intelligence, Military Revolution, and China’s Future Military Power, Center for a New American Security (CNAS), 2017; “Artificial Intelligence and Chinese Power”, Associated Press, 2017; “Putin: Leader in artificial intelligence will rule world”, CNBC, 4 September 2017: https://www.cnbc.com/2017/09/04/putin-leader-in-artificial-intelligence-will-rule-world.html. 57 See, for example: J Keller, DARPA TRACE program using advanced algorithms, embedded computing for radar target recognition”, Military & Aerospace Electronics, 2015: http://www.militaryaerospace.com/articles/2015/07/hpec-radar-target-recognition.html; D Lewis, N Modirzadeh and G Blum, “The Pentagon’s New Algorithmic-Warfare Team”, Lawfare, 2017: https://www.lawfareblog.com/pentagons-new-algorithmic-warfare-team. CCW/GGE.1/2018/WP.5 16 (and legal) questions if their outputs, as “decision aids,” are applied to targeting decisions, especially in the absence of cross-checking, or balancing, with other sources of information before human authorization to attack (as over-reliance on algorithmic output would diminish the meaning of the consequent human decision). However, if such AI systems are used directly to control the initiation of an attack by an autonomous weapon system, these concerns would be particularly serious. More broadly, there is growing appreciation of the risks of use, and misuse, of AI across the digital, physical and political domains, and the implications for international security.58 51. The degree of predictability and reliability of autonomous (weapon) systems affects the trust of humans in that system – especially in relation to the link between human intention and the eventual “action”, or operation, of the system – and this trust is also affected by the degree to which the operation of the system can be explained – or explain itself (e.g. with in-built “explainable AI”).59 52. There are now more and more initiatives addressing these ethical questions for AI systems in general, including the Institute of Electrical and Electronics Engineers (IEEE)’s Global Initiative on Ethics of Autonomous and Intelligent Systems, which is working on “ethically aligned design” standards for AI and autonomous systems,60 and for robotic systems, in particular.61 The Asilomar AI Principles recently developed by the Future of Life Institute are interesting in this respect. In warning against an AI arms race,62 they highlight ethical concerns raised by AI systems in general, noting the need for safety, failure transparency, responsibility of developers, alignment with human values and human control over delegation of decisions to AI systems.63 5.2 Ethics and risk 53. Unpredictability and unreliability in autonomous weapon systems also contribute to the level of risk that the use of the weapon will lead to unacceptable consequences, in particular for civilians, which raises ethical (as well as legal) issues. Since assessing risk requires an assessment of probability and consequence, machine-learning systems, for example, present immediate problems. Where there is inherent unpredictability in the functioning of a system it may not be possible to assess the probability of a certain action, and so determining risk becomes problematic. The introduction of this unpredictability into system design is therefore a significant concern in managing risk. From a purely ethical 58 Future of Humanity Institute, University of Oxford; Centre for the Study of Existential Risk, University of Cambridge; Center for a New American Security; Electronic Frontier Foundation; and OpenAI, The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation, 2018: https://maliciousaireport.com. 59 See, for example: DARPA, Explainable Artificial Intelligence (XAI): https://www.darpa.mil/program/explainable-artificial-intelligence. 60 Institute of Electrical and Electronics Engineers (IEEE), The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems, http://standards.ieee.org/develop/indconn/ec/autonomous_systems.html. 61 See, for example: Engineering and Physical Sciences Research Council (EPSRC), Principles of Robotics, https://www.epsrc.ac.uk/research/ourportfolio/themes/engineering/activities/principlesofrobotics/; J Bryson, “The meaning of the EPSRC principles of robotics”, Connection Science, Vol. 29 No. 2, 2017, pp. 130–136. 62 Future of Life Institute, Asilomar AI Principles, 2017: https://futureoflife.org/ai-principles/: “18) AI Arms Race: An arms race in lethal autonomous weapons should be avoided.” 63 Ibid. “6) Safety: AI systems should be safe and secure throughout their operational lifetime, and verifiably so where applicable and feasible. 7) Failure Transparency: If an AI system causes harm, it should be possible to ascertain why. 9) Responsibility: Designers and builders of advanced AI systems are stakeholders in the moral implications of their use, misuse, and actions, with a responsibility and opportunity to shape those implications. 10) Value Alignment: Highly autonomous AI systems should be designed so that their goals and behaviors can be assured to align with human values throughout their operation. 11) Human Values: AI systems should be designed and operated so as to be compatible with ideals of human dignity, rights, freedoms, and cultural diversity. … 16) Human Control: Humans should choose how and whether to delegate decisions to AI systems, to accomplish human-chosen objectives.” CCW/GGE.1/2018/WP.5 17 perspective, some have even argued that creating an unreasonable risk should be considered harm, and ethically wrong, even if that risk does not materialize.64 54. The level of risk also relates to the potential consequences of an unpredicted or unintended action, which will also be determined by the specific type of autonomous weapon system and the context of its use, including uses that were not originally foreseen. Some emphasize that the destructive power of the weapon system – in terms of size of munition or potential destructive effects – is an important factor in determining the level of risk, and therefore for an ethical assessment. For example, few would argue for development of autonomous nuclear weapon systems, even if predictability and reliability could be assured as extremely high. However, others are sceptical of a focus on the destructive power, since relatively low-power weapons – such as an autonomous machine-gun system – could still have serious consequences and be used to kill and injure many people (see also Section 6). In summary, while predictability and reliability may reduce the risks of unintended consequences in the operation of an autonomous weapon system, they do not, in themselves, eliminate risk. 6. Ethical issues in context 55. Another aspect to consider is whether ethical assessments of autonomous weapon systems vary according to context. In particular, do specific characteristics of an autonomous weapon system, and the way it is used, have an influence on its ethical acceptability? For example: the task the weapon is used for, the type of target, the operating environment, the timeframe of operation and the scope of movement over an area. 56. When discussing different types of autonomous weapon systems, in different scenarios and contexts, different views tend to emerge on ethical acceptability. These assessments tend to vary according to the core determinations of human agency in the decision-making process and human dignity (see Section 3), associated moral responsibility (see Section 4) and, especially, the degree of predictability and risk (see Section 5), since contextual factors can have a significant impact on the last of these. 6.1 Constraints in time and space 57. A longer timeframe and/or increased scope of movement over an area are major factors in contributing to uncertainty between the point of activation of an autonomous weapon system and the eventual attack that results. As discussed, an autonomous weapon system – since it selects and attacks targets independently (after launch or activation) – creates varying degrees of uncertainty as to exactly when, where and/or why the resulting attack will take place (see also Section 4.1).65 This is accentuated by wider temporal and spatial boundaries because of greater room for variations in the operational environment over an area, and evolution of that environment over time, both of which may affect the consequences of activation. 58. Uncertainties introduced by autonomy are clearly a problem from a legal perspective, to the extent that they may prevent the commander or operator from making judgements and taking decisions in line with their legal obligations – of distinction, proportionality and precautions – in carrying out attacks in armed conflict. However, uncertainties also raise concerns from an ethical perspective because they can decouple human agency and intent in the decision to use force from the eventual consequences, even if the resulting attack is lawful (see Section 3). 59. There are different dimensions to the issue of temporal constraints. One is the elapsed time between the point of activation of an autonomous weapon system and the point at which a resulting attack takes place. For example, there is a significant difference in the 64 C Finkelstein, “Is Risk a Harm?” University of Pennsylvania Law Review, No. 263, 2003. 65 This is in contrast to a long-range non-autonomous weapon system, such as a cruise missile, which may travel long distances, with a significant delay between launch and impact, but is intended to hit a specific target at a specific point in time. (It may also have the capacity to be manually or automatically deactivated after launch.) CCW/GGE.1/2018/WP.5 18 level of uncertainty in circumstances that may result during a ten-minute flight time versus a two-day loiter time (also depending on the operating environment). There are parallels, here, with mine warfare; a major problem with anti-personnel mines, which contributed to their indiscriminate effects and eventual prohibition, was the lack of control over the period during which they could autonomously operate. Once laid by humans, and unless fitted with self-destruct or self-neutralizing features, landmines remain activated indefinitely, and the initial user has no further control over the eventual attack and the nature of the victim. 60. Mines that stay active indefinitely also raise another time-related concern: the absence of an “off switch”. With autonomous weapon systems, the uncertainty over when, where and/or why an attack takes place could be extended indefinitely if there is no capacity to deactivate the system after launch or activation. Unless the system has an automatic selfdestruct or self-neutralizing feature (the reliability of which can also vary, as was the case with landmines), the ability to deactivate an autonomous weapon system would require a communication link with a human operator to be retained. Since changes in the operational environment may require deactivation at any point following activation, there is a strong argument for enabling constant human supervision and the ability to intervene and deactivate, as is the case with many existing autonomous weapon systems, such as counter- rocket, artillery and mortar weapons.66 61. A further aspect of the temporal issue is human reaction time. Some existing autonomous weapon systems are, by design, intended to initiate an attack quicker than is humanly possible. While speed may create a military advantage – for example, in the case of time-constrained missile and counter rocket, artillery and mortar defence – it also erodes the potential for human intervention to prevent an unlawful, unnecessary or accidental attack. Even with continuous human supervision, it may only be possible to deactivate a weapon system after a problematic attack in order to prevent further negative consequences, and whether or not this is an acceptable risk may depend on the predictability and reliability of the weapon, the operating environment, as well as the task for which it is used and the target against which it is employed. 6.2 Constraints in operating environments, tasks and targets 62. The task for which an autonomous weapon system is used and the environment in which it is used can also be significant for ethical assessments. In situations where there are fewer risks to civilians or civilian objects, some have argued there may also be fewer ethical concerns raised by autonomy – in terms of reduced human agency. For example, it has been suggested that autonomous deep-sea, anti-submarine warfare and autonomous close-in air defence at sea may be more ethically acceptable, owing to the relatively uncluttered and simple nature of the operating environments, and the reduced numbers of civilians and civilian objects, compared with populated areas on the coast or inland – and, therefore, potentially more predictable, in terms of consequences, and lower-risk.67 63. Further, there is the issue of whether an autonomous weapon system is used for defensive or offensive tasks. Some suggest there may be an ethical distinction between a “defensive” weapon system – such as a missile or counter-rocket, artillery and mortar defence weapon, or a “sentry” weapon guarding a border – and an “offensive” system, which actively searches for targets. However, others caution that the distinction between “offensive” and “defensive” is not clear operationally (and legally, the same rules apply to the use of force or conduct of hostilities), and that a weapon system introduced for a “defensive” task may later be used in an “offensive” role. 66 Such a requirement could limit the utility of autonomous weapon systems where constant communication is not feasible, such as underwater. 67 R Sparrow and G Lucas, “When Robots Rule the Waves?” Naval War College Review, 69(4), 2016, pp. 49–78. CCW/GGE.1/2018/WP.5 19 64. Perhaps the most significant contextual factor that gives rise to ethical concerns, however, is the nature of the target, and whether the weapon system only targets objects or attacks humans directly. The fundamental anxiety in the ethical discourse is about antipersonnel autonomous weapon systems, especially, it is argued, with respect to: lack of human agency and intent in decisions to use force; the loss of human dignity on the part of those combatants targeted,68 and of civilians that are put at risk as a consequence of legitimate attacks on military targets; and the implications for moral responsibility (see Sections 3 and 4). 7. Public and military perceptions 65. Although public opinion does not necessarily equal public conscience, and ethics, as a formal mode of criticism, should not be reduced to opinion polls, it is useful to explore the perspectives on autonomous weapon systems from different constituents of society – including the public, the military, and the scientific and technical communities.69 66. Public opinion may not provide evidence-based answers to ethical questions, especially when those surveyed have different understandings of the questions and the concept of an autonomous weapon system. However, opinion polls can spark debate and illustrate a significant interest in and engagement with the topic by different constituents, as well as revealing trends related to public-conscience concerns. 7.1 Opinion surveys 67. There have been several surveys of public opinion in this field.70 Many have contrasted remote-controlled armed drones with autonomous weapon systems, in order to differentiate reactions to autonomy specifically from robotic-weapons platforms in general. In 2011, Moon, Danielson and Van der Loos found greater rejection of autonomous weapon systems (81% against, 10% in favour) than of remote-controlled drones (53% against, 35% in favour) based on three major rationales: preservation of human responsibility and accountability; scepticism about the technology, and therefore risks for civilians; and assertions that humans should always make life-or-death decisions.71 68. In 2015, an Open Roboethics Initiative survey gathered the views of 1000 people from 49 different countries. It, too, found a significant rejection of autonomous weapon systems (67% said all types should be banned) and stronger views based on the type of task (85% should not be used for “offensive purposes”). The rejection of autonomous weapons was also greater in comparison with remote-controlled weapons (71% would prefer their military to use remote-controlled weapons in warfare; 60% would prefer to be attacked by remotecontrolled rather than autonomous weapons).72 69. A 2017 IPSOS poll of 11,500 respondents in 25 countries also found overall opposition to autonomous weapon systems (56% against, 24% in favour), although the poll 68 R Sparrow, “Twenty seconds to comply: Autonomous Weapon Systems and the recognition of surrender”, International Law Studies, 91, 2015, pp. 699–728. 69 R Sparrow, “Ethics as a source of law: The Martens clause and autonomous weapons”, ICRC Humanitarian Law & Policy Blog, 14 November 2017: http://blogs.icrc.org/law-andpolicy/ 2017/11/14/ethics-source-law-martens-clause-autonomous-weapons. 70 Including: L Moshkina and R Arkin, “Lethality and Autonomous Systems: The Roboticist Demographic”, IEEE International Symposium on Technology and Society, 2008; Prof. C Carpenter, US public opinion on autonomous weapons, University of Massachusetts Department of Political Science, 2013; M Horowitz, “Public opinion and the politics of the killer robots debate”, Research and Politics, January–March 2016. 71 A Moon, P Danielson and M Van der Loos, “Survey-based Discussions on Morally Contentious Applications of Interactive Robotics”, International Journal of Social Robotics, Volume 4, Issue 1, 2012, pp 77–96. 72 Open Roboethics Initiative, The Ethics and Governance of Lethal Autonomous Weapons Systems: An International Public Opinion Poll, 9 November 2015. CCW/GGE.1/2018/WP.5 20 also revealed regional variations, with the greatest opposition in Russia (69% against), Peru (67% against), Spain (66% against) and Argentina (66% against), and the least in India (31%), China (36%) and the United States (45%).73 70. While each study has its limitations, these polls reflect trends that are worth exploring further. Why do people tend to prefer attacks to be carried out by remote-controlled rather than autonomous weapon systems? How much significance is placed on reservations about the technology and its consequences, and how much on ethical concerns about human agency, human dignity and the view that machines must not take decisions on the use of force? 7.2 Contrasting military and public perceptions 71. Another 2017 survey contrasted perceptions of remote-controlled armed drones and autonomous weapon systems among the public in the United States, and civilian and military personnel of the Dutch Ministry of Defence.74 The Ministry of Defence personnel had less trust, confidence and support for the “actions” taken by autonomous weapon systems compared with remote-controlled systems but considered them equally “fair”. Respondents were, generally, more anxious about the consequences of using autonomous weapon systems, and concern about a lack of respect for human dignity was one of the main objections, when compared with human-operated drones resulting in the same consequences. In comparisons between military and public perceptions, most notable was the similar level of concern about a loss of human dignity, which may indicate some common ground among different constituents. 8. CONCLUSIONS 72. Ethics, humanity and the dictates of the public conscience are at the heart of the debate about the acceptability of autonomous weapon systems. From the ICRC’s perspective, ethics provides another avenue – alongside legal assessments and technical considerations – to help determine the necessary type and degree of human control that must be retained over weapon systems, and the use of force, and to elucidate where States must establish limits on autonomy in weapon systems. 73. Considerations of humanity and the public conscience provide ethical guidance for discussions, and there is a requirement to connect them to legal assessments via the Martens Clause – a safety net for humanity. These ethical considerations go beyond whether autonomous weapon systems are compatible with our laws to include fundamental questions of whether they are acceptable to our values. And such debates necessarily require the engagement of various constituents of society. 74. Several ethical issues appear central to establishing constraints on autonomy in weapon systems. Perhaps the most powerful ethical concerns are those that transcend context – whether during armed conflict or in peacetime – and transcend technology – whether simple or sophisticated.75 These are concerns about loss of human agency in decisions to use force – decisions to kill, injure and destroy – loss of human dignity in the process of using force, and erosion of moral responsibility for these decisions. 73 IPSOS, Three in ten Americans support using Autonomous Weapons, 7 February 2017. 74 I Verdiesen, Agency perception and moral values related to Autonomous Weapons: An empirical study using the Value-Sensitive Design approach, Masters of Science, Faculty of Technology, Policy and Management, TU Delft, 2017. 75 Although there are different views among experts on the issue of technology. Some make a distinction between “automated” and “autonomous” weapons and focus their concerns on systems controlled by complex AI algorithms rather than simpler software. Others, including the ICRC, note the lack of a clear technical distinction between the two, and argue that “all such weapons raise the same core legal and ethical questions”. See: ICRC, Autonomous weapon systems: Implications of increasing autonomy in the critical functions of weapons, op. cit. (footnote 2), 2016, p. 8. CCW/GGE.1/2018/WP.5 21 75. The importance of retaining human agency – and intent – in these decisions is one of the central ethical arguments for limits on autonomy in weapon systems. Many take the view that decisions to kill, injure and destroy must not be delegated to machines, and that humans must be present in this decision-making process sufficiently to preserve a direct link between the intention of the human and the eventual operation of the weapon system. It is not enough simply to say that “humans have developed, deployed and activated the weapon system”. There must be a direct connection between the human rationale for activation of an autonomous weapon system in the specific circumstances and the consequences of the resulting attack. But questions remain about how close this connection must be, and what form it must take. 76. Human dignity is another core ethical consideration that is linked to concerns about loss of human agency. The central argument is that it matters not just if a person is killed or injured but how they are killed or injured, and the process by which these decisions are made is as important as the results. If human agency is lacking to the extent that machines have effectively, and functionally, been delegated these decisions, then, according to this argument, it undermines the human dignity of those combatants targeted, and of civilians that are put at risk as a consequence of legitimate attacks on military targets. If human agency is retained, on the other hand, it is an acknowledgement of humanity in that decision to use force and the resulting consequences. 77. The need for human agency is also linked to moral responsibility and accountability for decisions to use force. These are human responsibilities (both ethical and legal), which cannot be transferred to inanimate machines, or computer algorithms, since it is humans that have both rights and responsibilities in relation to these decisions. From an ethical perspective, it is not sufficient only to assign legal responsibility to a commander or operator who activates an autonomous weapon system. Humans must uphold their moral responsibility, requiring not only a causal explanation but also a justification for the resulting use of force. Autonomous weapon systems complicate this justification because of the more generalized nature of the targeting decisions, which risks eroding – or diffusing – moral responsibility. 78. Predictability and reliability in using an autonomous weapon system are ways of connecting human agency and intent to the eventual consequences of the resulting attack. A lack of predictability, whether inherent to the weapon system design or due to interaction with the environment, raises serious ethical (and legal) concerns owing to a lack of foreseeability of the consequences and associated risks, in particular for civilians. 79. As weapons that self-initiate attacks, autonomous weapon systems all raise questions about predictability, owing to varying degrees of uncertainty as to exactly when, where and/or why a resulting attack will take place. However, the application of AI and, in particular, machine learning, to targeting functions accentuates this problem, raising fundamental questions of inherent unpredictability by design and heightening concerns about the loss of human agency, moral responsibility and human dignity. 80. Context also affects ethical assessments of autonomous weapon systems, owing to the impact on the predictability of the outcomes of their use, the nature of the consequences and the overall level of risk that results. Constraints on the timeframe of operation and scope of movement over an area are key factors, as are the task for which the weapon is used and the operating environment in which it is activated. 81. However, from an ethical perspective, perhaps the most important contextual factor is the type of target. Core concerns about human agency, human dignity and moral responsibility are most acute in relation to the notion of anti-personnel autonomous weapon systems that target humans directly. These concerns may be one reason – together with legal considerations and technical limitations – why the use of autonomous weapon systems to date has been constrained to anti-materiel systems,76 targeting projectiles, 76 There have been reports that some anti-personnel “sentry” weapon systems have autonomous modes. However, as far as is known to the ICRC, “sentry” weapon systems that have been deployed still CCW/GGE.1/2018/WP.5 22 vehicles, aircraft or other objects, even if these systems pose dangers to humans inside or in proximity to objects.77 8.1 An ethical basis for human control? 82. From the ICRC’s perspective, ethical considerations very much parallel the requirement for a minimum level of human control over weapon systems and the use of force, to ensure compliance with international legal obligations that govern the use of force in armed conflict and in peacetime.78 83. From an ethical viewpoint, “meaningful”, “effective” or “appropriate” human control would be the type and degree of control that preserves human agency and upholds moral responsibility in decisions to use force. This does not necessarily exclude autonomy in weapon systems, but it requires a sufficiently direct and close connection to be maintained between the human intent of the user and the eventual consequences of the operation of the weapon system in a specific attack. This, in turn, will necessitate limits on autonomy. 84. Ethical and legal considerations may demand some similar constraints on autonomy in weapon systems so that meaningful human control is maintained – in particular, with respect to: human supervision and the ability to intervene and deactivate; technical requirements for predictability and reliability (including in the algorithms used); and operational constraints on the task for which the weapon is used, the type of target, the operating environment, the timeframe of operation and the scope of movement over an area.79 85. However, the combined and interconnected ethical concerns about loss of human agency in decisions to use force, diffusion of moral responsibility and loss of human dignity could have the most far-reaching consequences, perhaps precluding the development and use of anti-personnel autonomous weapon systems, and even limiting the applications of anti-materiel systems, depending on the risks that destroying materiel targets present for human life. require human remote authorization to launch an attack (even though they may identify targets autonomously). See also footnote 3. 77 Including through accidents. See, for example, “fratricide” incidents discussed in: J Hawley, Automation and the Patriot Air and Missile Defense System, op. cit. (footnote 24), 2017. 78 N Davison, Autonomous weapon systems under international humanitarian law, op. cit. (footnote 4), 2017; M Brehm, Defending the Boundary: Constraints and Requirements on the Use of Autonomous Weapon Systems Under International Humanitarian and Human Rights Law, Geneva Academy Briefing no. 9, 1 May 2017. 79 ICRC, Statement to the Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on “Lethal Autonomous Weapon Systems”, op. cit. (footnote 4), 15 November 2017. Group of Governmental Experts on Emerging Technologies in the Area of Lethal Autonomous Weapons Systems Geneva, 25–29 March 2019 and 20–21 August 2019 Agenda item 5 Focus of work of the Group of Governmental Experts in 2019 Autonomy, artificial intelligence and robotics: Technical aspects of human control Submitted by International Committee of the Red Cross (ICRC)1 1. The International Committee of the Red Cross (ICRC) has emphasized the need to maintain human control over weapon systems and the use of force, to ensure compliance with international law and to satisfy ethical concerns. This approach has informed the ICRC’s analysis of the legal, ethical, technical and operational questions raised by autonomous weapon systems. 2. In June 2018, the ICRC convened a round-table meeting with independent experts in autonomy, artificial intelligence (AI) and robotics to gain a better understanding of the technical aspects of human control, drawing on experience with civilian autonomous systems. This report combines a summary of the discussions at that meeting with additional research, and highlights the ICRC’s main conclusions, which do not necessarily reflect the views of the participants. Experience in the civilian sector yields insights that can inform efforts to ensure meaningful, effective and appropriate human control over weapon systems and the use of force. 3. Autonomous (robotic) systems operate without human intervention, based on interaction with their environment. These systems raise such questions as “How can one ensure effective human control of their functioning?” and “How can one foresee the consequences of using them?” The greater the complexity of the environment and the task, the greater the need for direct human control and the less one can tolerate autonomy, especially for tasks and in environments that involve risk of death and injury to people or damage to property – in other words safety-critical tasks. 4. Humans can exert some control over autonomous systems – or specific functions – through supervisory control, meaning “human-on-the-loop” supervision and ability to intervene and deactivate. This requires the operator to have: • situational awareness • enough time to intervene • a mechanism through which to intervene (a communication link or physical controls) in order to take back control, or to deactivate the system should circumstances require. 1 To download the full report visit: https://www.icrc.org/en/war-and-law/weapons/ihl-and-new-technologies CCW/GGE.1/2019/WP.7 Convention on Prohibitions or Restrictions on the Use of Certain Conventional Weapons Which May Be Deemed to Be Excessively Injurious or to Have Indiscriminate Effects 20 August 2019 English only CCW/GGE.1/2019/WP.7 2 5. However, human-on-the-loop control is not a panacea, because of such humanmachine interaction problems as automation bias, lack of operator situational awareness and the moral buffer. 6. Predictability and reliability are at the heart of discussions about autonomy in weapon systems, since they are essential to achieving compliance with international humanitarian law and avoiding adverse consequences for civilians. They are also essential for military command and control. 7. It is important to distinguish between: reliability – a measure of how often a system fails; and predictability – a measure of how the system will perform in a particular circumstance. Reliability is a concern in all types of complex system, whereas predictability is a particular problem with autonomous systems. There is a further distinction between predictability in a narrow sense of knowing the process by which the system functions and carries out a task, and predictability in a broad sense of knowing the outcome that will result. 8. It is difficult to ensure and verify the predictability and reliability of an autonomous (robotic) system. Both factors depend not only on technical design but also on the nature of the environment, the interaction of the system with that environment and the complexity of the task. However, setting boundaries or imposing constraints on the operation of an autonomous system – in particular on the task, the environment, the timeframe of operation and the scope of operation over an area – can render the consequences of using such a system more predictable. 9. In a broad sense, all autonomous systems are unpredictable to a degree because they are triggered by their environment. However, developments in the complexity of software control systems – especially those based on AI and machine learning – add unpredictability in the narrow sense that the process by which the system functions is unpredictable. 10. The “black box” manner in which many machine learning systems function makes it difficult – and in many cases impossible – for the user to know how the system reaches its output. Not only are such algorithms unpredictable but they are also subject to bias, whether by design or in use. Furthermore, they do not provide explanations for their outputs, which seriously complicates establishing trust in their use and exacerbates the already significant challenges of testing and verifying the performance of autonomous systems. And the vulnerability of AI and machine learning systems to adversarial tricking or spoofing amplifies the core problems of predictability and reliability. 11. Computer vision and image recognition are important applications of machine learning. These applications use deep neural networks (deep learning), of which the functioning is neither predictable nor explainable, and such networks can be subject to bias. More fundamentally, machines do not see like humans. They have no understanding of meaning or context, which means they make mistakes that a human never would. 12. It is significant that industry standards for civilian safety-critical autonomous robotic systems – such as industrial robots, aircraft autopilot systems and self-driving cars – set stringent requirements regarding: human supervision, intervention and deactivation – or failsafe; predictability and reliability; and operational constraints. Leading developers of AI and machine learning have stressed the need to ensure human control and judgement in sensitive applications – and to address safety and bias – especially where applications can have serious consequences for people’s lives. 13. Civilian experience with autonomous systems reinforces and expands some of the ICRC’s viewpoints and concerns regarding autonomy in the critical functions of weapon systems. The consequences of using autonomous weapon systems are unpredictable because of uncertainty for the user regarding the specific target, and the timing and location of any resulting attack. These problems become more pronounced as the environment or the task become more complex, or freedom of action in time and space increases. Human-on-the-loop supervision, intervention and the ability to deactivate are absolute minimum requirements for countering this risk, but the system must be designed to allow for meaningful, timely, human intervention – and even that is no panacea. 14. All autonomous weapon systems will always display a degree of unpredictability stemming from their interaction with the environment. It might be possible to mitigate this CCW/GGE.1/2019/WP.7 3 to some extent by imposing operational constraints on the task, the timeframe of operation, the scope of operation over an area and the environment. However, the use of software control based on AI – and especially machine learning, including applications in image recognition – brings with it the risk of inherent unpredictability, lack of explainability and bias. This heightens the ICRC’s concerns regarding the consequences of using AI and machine learning to control the critical functions of weapon systems and raises questions about its use in decision-support systems for targeting. 15. This review of technical issues highlights the difficulty of exerting human control over autonomous (weapon) systems and shows how AI and machine learning could exacerbate this problem exponentially. Ultimately it confirms the need for States to work urgently to establish limits on autonomy in weapon systems. 16. It reinforces the ICRC’s view that States should agree on the type and degree of human control required to ensure compliance with international law and to satisfy ethical concerns, while also underlining its doubts that autonomous weapon systems could be used in compliance with international humanitarian law in all but the narrowest of scenarios and the simplest of environments. GE.21-13672(E) Group of Governmental Experts on Emerging Technologies in the Area of Lethal Autonomous Weapons System Geneva, 3-13 August, 24 September-1 October and 2-8 December 2021 Agenda Item 5 Focus of work of the Group of Governmental Experts in 2021 Proposal for consensus recommendations in relation to the clarification, consideration and development of aspects of the normative and operational framework Submitted by the International Committee of the Red Cross I. The ICRC's concerns about autonomous weapon systems 1. Autonomous weapon systems select and apply force to targets without human intervention. After initial activation or launch by a person, an autonomous weapon system self-initiates or triggers a strike in response to information from the environment received through sensors and on the basis of a generalized "target profile". This means that the user does not choose, or even know, the specific target(s) and the precise timing and/or location of the resulting application(s) of force. 2. The use of autonomous weapon systems entails risks due to the difficulties in anticipating and limiting their effects. This loss of human control and judgement in the use of force and weapons raises serious concerns from humanitarian, legal and ethical perspectives. 3. The process by which autonomous weapon systems function: • brings risks of harm for those affected by armed conflict, both civilians and combatants, as well as dangers of conflict escalation • raises challenges for compliance with international law, including international humanitarian law, notably, the rules on the conduct of hostilities for the protection of civilians • raises fundamental ethical concerns for humanity, in effect substituting human decisions about life and death with sensor, software and machine processes. II. The ICRC's recommendations to States for the regulation of autonomous weapon systems 4. The International Committee of the Red Cross (ICRC) has, since 2015, urged States to establish internationally agreed limits on autonomous weapon systems to ensure civilian protection, compliance with international humanitarian law, and ethical acceptability. CCW/GGE.1/2021/WP.6 Convention on Prohibitions or Restrictions on the Use of Certain Conventional Weapons Which May Be Deemed to Be Excessively Injurious or to Have Indiscriminate Effects 27 September 2021 English only CCW/GGE.1/2021/WP.6 2 5. With a view to supporting current efforts to establish international limits on autonomous weapon systems that address the risks they raise, including efforts by the Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Emerging Technologies in the Area of Lethal Autonomous Weapons Systems to develop consensus recommendations in relation to the clarification, consideration and development of aspects of the normative and operational framework, the ICRC recommends that States adopt new legally binding rules.1 In particular: (a) Unpredictable autonomous weapon systems should be expressly ruled out, notably because of their indiscriminate effects. This would best be achieved with a prohibition on autonomous weapon systems that are designed or used in a manner such that their effects cannot be sufficiently understood, predicted and explained. (b) In light of ethical considerations to safeguard humanity, and to uphold international humanitarian law rules for the protection of civilians and combatants hors de combat, use of autonomous weapon systems to target human beings should be ruled out. This would best be achieved through a prohibition on autonomous weapon systems that are designed or used to apply force against persons. (c) In order to protect civilians and civilian objects, uphold the rules of international humanitarian law and safeguard humanity, the design and use of autonomous weapon systems that would not be prohibited should be regulated, including through a combination of: • limits on the types of target, such as constraining them to objects that are military objectives by nature • limits on the duration, geographical scope and scale of use, including to enable human judgement and control in relation to a specific attack • limits on situations of use, such as constraining them to situations where civilians or civilian objects are not present • requirements for human–machine interaction, notably to ensure effective human supervision, and timely intervention and deactivation. 6. The ICRC’s position and its recommendations to States are based on its analyses of associated humanitarian, legal, ethical, technical and military implications of autonomous weapon systems, insights published in a series of reports, and regular engagement with States and experts at the CCW and bilaterally.2 These inform the ICRC’s recommendations on the specific limits on autonomous weapon systems that are needed to ensure civilian protection, compliance with international humanitarian law and ethical acceptability. 7. The normative limits put forward by the ICRC are informed by views expressed by many High Contracting Parties to the CCW, and other stakeholders, on the types of measures that can contribute to ensuring human control, involvement or judgement, and on the need for new legally binding rules on autonomous weapon systems. More specifically, a number of High Contracting Parties, and other stakeholders, support the prohibition of certain autonomous weapon systems and the placement of constraints or requirements on other autonomous weapon systems. 8. The ICRC is convinced that these limits should take the form of new legally binding rules that specifically regulate autonomous weapon systems. These rules should clarify how existing rules of international law, including international humanitarian law, constrain the 1 ICRC, ICRC Position on Autonomous Weapon Systems and Background Paper, Geneva, 12 May 2021, https://www.icrc.org/en/document/icrc-position-autonomous-weapon-systems. 2 Most recently: ICRC, Statement of the ICRC to Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems, Geneva, 21–25 September 2020; ICRC, Commentary on the “Guiding Principles” of the CCW GGE on “Lethal Autonomous Weapons Systems”, Geneva, July 2020; and V. Boulanin, N. Davison, N. Goussac, and M. Peldán Carlsson, Limits on Autonomy in Weapon Systems: Identifying Practical Elements of Human Control, ICRC & SIPRI, June 2020. CCW/GGE.1/2021/WP.6 3 design and use of autonomous weapon systems, and supplement the legal framework where needed, including to address wider humanitarian risks and fundamental ethical concerns raised by autonomous weapon systems. 9. Considering the speed of development in autonomous weapon systems’ technology and use, it is critical that internationally agreed limits be established in a timely manner. Beyond new legal rules, these limits may also include common policy standards and good practice guidance, which can be complementary and mutually reinforcing. To this end, and within the scope of its mandate and expertise, the ICRC stands ready to work in collaboration with States and other stakeholders at international and national levels, including representatives of High Contracting Parties to the CCW and their armed forces, the scientific and technical community, industry and civil society. CHECK AGAINST DELIVERY Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 9–13 April 2018, Geneva Statement of the International Committee of the Red Cross (ICRC) Characterization of the systems under consideration in order to promote a common understanding on concepts and characteristics relevant to the objectives and purposes of the Convention Mr Chair Purpose of characterisation The purpose of discussing characteristics may be to: · to identify a specific category of weapon systems to be regulated, or · to identify a broad category, within which a specific group should be regulated. The ICRC’s view is that the second approach will provide more effective way of identifying “autonomous weapons systems”, and tailoring regulatory or policy responses to address any legal and ethical issues. The advantages of this approach include that it: · acknowledges the reality that some autonomous weapon systems are already in use · enables experience with existing weapons to inform legal and ethical assessments, and determinations of “meaningful” or “effective” human control, and · avoids prejudging regulatory responses at the outset. It is important that discussions are reality-based, and draw on technical, operational and legal evidence from existing weapon systems with autonomy in their critical functions. Autonomy in critical functions is the key characteristic The ICRC has characterised autonomous weapon systems broadly as Any weapon system with autonomy in its critical functions. That is, a weapon system that can select and attack targets without human intervention. After launch or activation by a human operator, the weapon system – though its sensors, programming (software algorithms) and connected weapon(s) – takes on the targeting functions that would normally be controlled by humans. In other words, the weapon system self-initiates the attack. This encompasses any weapon system that can independently select and attack targets, including some existing weapons (see below), as well as potential future systems. 2 It is autonomy in the critical functions that distinguishes autonomous weapon systems from all other weapon systems and that are central to legal obligations, ethical concerns, and humanitarian consequences. Considering only “full system autonomy” is too narrow, since a weapon may have autonomy in targeting functions without having system-level autonomy. Other autonomous functions – take-off and landing, navigation, flying or driving, and control of sensors – can be excluded, as can autonomous robotic systems that are not weaponised. Lethality is not a relevant factor. It is the use of force that triggers legal obligations under international humanitarian law (and international human rights law). Technical sophistication is not a key characteristic Technical sophistication is not the defining characteristic of whether a weapon is autonomous, rather it is whether the weapon system self-initiates the attack. Therefore, notions of “automated” and “autonomous” weapons are interchangeable because they raise the same legal, ethical and humanitarian questions. This is why the ICRC intentionally includes both “dumb” and “intelligent” autonomous weapon systems. A weapon could be very simple and “unintelligent” in its design, but highly autonomous in its critical functions (e.g. a machine-gun that is triggered by a motion or heat sensor). In fact, a “dumb” autonomous weapon systems could even raise greater legal concerns, and lead to worse humanitarian consequences. In addition, predictability in programming of a weapon system does not necessarily equal predictability in consequences. Autonomous weapon systems all raise questions about predictability, owing to varying degrees of uncertainty as to exactly when, where and/or why a resulting attack will take place. Autonomy in existing and future systems Autonomy in the critical functions is found in existing anti-materiel weapons that attack objects (e.g. counter rocket, artillery and mortar systems; vehicle “active protection” systems; and some loitering munitions). However, autonomy in the critical functions is a feature that could, in theory, be applied to any weapon system, including existing remote controlled weapon systems. There is a trend towards increasing autonomy in the wide range of robotic weapon platforms in the air, on land and at sea. Future developments could include shifts: · from anti-materiel to anti-personnel systems · from static (fixed) to mobile systems, that “search” for targets over an area · from use in armed conflict to use in law enforcement operations, and · use of autonomous target identification systems as “decision aids” not directly connected to weapon systems. Thank you. CHECK AGAINST DELIVERY Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 9–13 April 2018, Geneva Statement of the International Committee of the Red Cross (ICRC) The International Committee of the Red Cross (ICRC) is pleased to contribute its views to this second meeting of the Group of Governmental Experts on “Lethal Autonomous Weapon Systems”. The ICRC wishes to acknowledge and thank the Chair for leading these discussions and wish him success in his efforts this week. Building on the valuable work done to date, this meeting provides an opportunity to consider in more detail the complex challenges posed by autonomous weapon systems. We are hopeful that the meeting will advance progress towards a common understanding of the characteristics of these weapons. Setting a broad scope of the discussion will be important in this regard. In the ICRC’s view, the purpose of identifying the characteristics of the systems under consideration should be to identify the features that distinguish autonomous weapon systems from those controlled directly by humans, including remote-­controlled weapons. The objective at the outset need not be to define systems of concern. From the ICRC’s perspective, the focus must be on the functions of weapon systems that are most relevant to legal obligations and ethical concerns within the scope of the Convention on Certain Conventional Weapons, namely autonomy in the critical functions of selecting and attacking targets. Autonomy in other functions (such as movement or navigation) would not in our view be relevant to the discussions. Experiences with existing weapon systems with autonomy in their critical functions should be harnessed for a greater understanding of the legal and ethical issues raised by autonomy in weapon systems more generally, and of the nature of human control over the use of force that must be retained for legal compliance and ethical acceptability. For the ICRC, it is crucial that discussions draw on real-­world technologies and near-­term developments of autonomy in weapon systems. To do otherwise would risk overlooking incremental developments in autonomy that raise concerns and that may require legal and policy responses from the international community. The degree of human control over weapon systems – not their technological sophistication – should be the yardstick in these discussions. It is humans – not machines, computer programs or weapon systems – who apply the law and are obliged to respect it. International humanitarian law (IHL) requires that those who plan, decide upon and carry out attacks make certain judgements in applying the norms when launching an attack. Ethical considerations parallel this requirement – demanding that human agency and intention be retained in decisions to use force. Humans therefore bear responsibilities in the programming, development, activation and operational phases of autonomous weapon systems. Mindful of the potential adverse humanitarian consequences of the loss of human control over weapons and the use of force, the ICRC has posited that a minimum level of human control is necessary from both a legal and ethical perspective. In the view of the ICRC, a weapon system beyond human control would be unlawful by its very nature. But beyond these so-­called “fully autonomous weapon systems”, there is a need consider the full range of risks associated with weapon systems that have autonomy in their critical functions. The ethical concerns around loss of human agency in decisions to use force, diffusion of moral responsibility and loss of human dignity could have far-­reaching consequences, perhaps precluding the development and use of anti-­personnel autonomous weapon systems, and even limiting the application of anti-­matériel systems, depending on the risks that destroying matériel targets present for human life. The ICRC continues to urge all States in this meeting to elaborate what “meaningful” or “effective” human control entails in practice. States must also address fundamental concerns about weapon systems that may introduce inherent unpredictability, such as those employing artificial intelligence (AI) machine-­learning algorithms. The affirmations by CCW States Parties in this Group that IHL is both relevant and applicable to any emerging weapon technology, including autonomous weapon systems, is heartening. The ICRC welcomes efforts to improve implementation of IHL, including through enhancing mechanisms to review the legality of new weapons. Conducting legal reviews of weapon systems with autonomy in their critical functions can raise challenges, in particular regarding standards of predictability and reliability. The ICRC encourages the exchange of information and experiences about these processes, and views such efforts as complementary to the work of the Group of Governmental Experts. This being said, the ICRC remains convinced that the overall purpose of this Group should be to agree limits on autonomy in weapon systems. As noted by the ICRC in previous meetings on this topic, technological developments that remove or reduce direct human control over weapon systems are threatening to outpace international deliberations, and States must therefore approach this task with some urgency. A “human-­centred” approach must guide the identification of limits to autonomy in weapon systems and of possible options to address “autonomous weapon systems of concern”. We look forward to elaborating further on our views during the thematic sessions. Thank you. Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 25-29 March 2019, Geneva Statement of the International Committee of the Red Cross (ICRC) Agenda item 5(a) – An exploration of the potential challenges posed by emerging technologies in the area of lethal autonomous weapon systems to international humanitarian law Thank you, Mr Chair, for bringing this element of our discussions to the fore, particularly in view of the historic link between the Convention on Certain Conventional Weapons and the fundamental rules of international humanitarian law. In the ICRC’s view, the challenges posed to IHL by autonomous weapon systems are inextricable from the issue of the human element in the use of force (which will be discussed this afternoon under agenda item 5(b)). For the ICRC it is clear that the law is addressed to humans, and the relevant legal obligations under international humanitarian law (IHL) – notably the rules of distinction, proportionality and precautions in attack – rest with those who plan, decide on, and carry out attacks. As many of the States who have spoken so far have pointed out, it is humans, not machines, that comply with and implement the law, and it is humans who will be held accountable for any violations of IHL. These legal obligations, and associated judgements, cannot be transferred to a machine, computer program, or weapon system. As a result, combatants will require a level of human control over weapon systems and the use of force so that they can make context-specific legal judgements in specific attacks. It is the loss of human control over the use of force inherent in the use of autonomy in the critical functions of weapon systems that poses a challenge to IHL, particularly the rules regulating the conduct of hostilities. Addressing this challenge necessitates a close examination of the rules to determine exactly what they require, whether existing law is sufficiently clear or whether there is a need to clarify IHL or to develop new rules, or standards. For this reason, the ICRC welcomes the opportunity of this morning’s session to speak about the rules of IHL that demand a level of human control. In this afternoon’s session, we will discuss the key elements of human control required by IHL as we see them. IHL rules demand context-based decisions Mr Chair, the demands by IHL for context-specific legal judgments by those who plan and decide on attacks limits the degree of autonomy that is permitted under exiting IHL. In other words, limitations on autonomy in the critical functions of weapon systems can be deduced from existing IHL rules. The rules of distinction, proportionality and precautions require complex assessments based on the circumstances prevailing at the time of the decision to attack, but also during an attack. 2 For example, the rule on distinction refers to the obligations on parties to an armed conflict to always distinguish between civilians and civilian objects on the one hand, and military objectives on the other, and not to direct attacks against the former. Related is the prohibition on indiscriminate attacks. Both of these rules demand that parties characterise as military objectives the person and/or objects at whom an attack is intended to be directed. Such characterisations are, by their nature, qualitative and variable. By way of illustration, the definition of the term ‘military objective’ – set out in article 52(2) of Additional Protocol I – requires an assessment of whether the object by its nature, location, purpose or use ‘makes an effective contribution to military action and whose partial or total destruction, capture or neutralisation, in the circumstances ruling at the time, offers a definite military advantage’. Assuming that an attack is planned to be directed at a lawful target, the rule of proportionality would prohibit such an attack where it may be expected to cause incidental harm that would be excessive compared to the concrete and direct military advantage anticipated. The assessment of proportionality is generally measured against what a reasonable person would conclude in the circumstances, making use of available information. This rule provides a further example of the qualitative and variable nature of the legal judgments required by IHL. The obligation to take feasible precautions in attack relies on these same qualitative judgements, and generally requires those who plan or decide on attacks to take constant care to spare the civilian population, individual civilians and civilian objects, including when choosing between several possible targets, or when choosing their means and methods of warfare. To conclude, Mr Chair, adherence to IHL rules of distinction, proportionality and precautions clearly requires evaluative assessments based on knowledge of context, including the environment of use and the expected effects of the weapon. These context-based assessments must be made by humans. Military lawyers themselves reject the suggestion that the proportionality calculus, for example, can be reduced to objective indicators, insisting that their evaluations are dependent on the circumstances of particular situations and the good faith of military commanders. Context-based decisions demand human control It’s important to add, Mr Chair, that assessments of distinction, proportionality and precautions made by combatants must be reasonably proximate in time to the attack (or “strike”). Where these assessments form part of planning assumptions, these assumptions must have continuing validity until the execution of the attack in order to comply with IHL. This mean that when using autonomous weapon systems commanders must retain a level of human control over weapon systems sufficient to allow them to make context-specific legal judgments in carrying out attacks in armed conflict. In other words, they must retain a level of human control that is ‘meaningful’ (or ‘substantive’, ‘appropriate’, or ‘effective’, or ‘appropriate levels of human judgment’). 3 Human control at different stages Keeping in mind the IHL requirements for context-based decisions made in reasonable proximity to the attack, and the human control that these requirements demand, we now wish to turn to the different forms that human control can take during the development, activation, and operation of an autonomous weapon system: 1. Development and testing (‘development stage’); 2. Decision by the commander, or operator, to activate the weapon system (‘activation stage’); and 3. the operation of the weapon system during which it independently selects and attacks targets (‘operation stage’). Human control at all three stages, in design (development stage) and in use (activation and operation stages), is essential for compliance with IHL. The International Panel on the Regulation of Autonomous Weapons (iPRAW) has reached similar conclusions on the need for “control by design” and “control in use”. Ensuring human control in use, at the activation and operation stages, is the most important for compliance with IHL rules on the conduct of hostilities. Human control in design, at the development stage, provides a means to set and test control measures that will ensure human control in use. Human control at the development stage alone – control in design – will not be sufficient to ensure compliance with IHL in the use of an autonomous weapon system for attacks in armed conflict given the inherently variable and unpredictable nature of real-world operational environments. Human control, and judgement, for compliance with IHL must be proximate to the use of force in a specific attack, and cannot be substituted with software control. In the view of the ICRC, concepts of “human control in the wider loop” and the use of autonomy to “effectuate the intention of commanders and the operators of weapons systems” do not adequately capture the requirement for human control under IHL. Elements of human control Mr Chair, having addressed the legal basis for human control, this afternoon we will discuss the three key and inter-related elements of human control as we see them, namely: 1. human supervision and the ability to interevent/deactivate 2. predictability and reliability, and 3. operational constraints. To be clear, Mr Chair, and to repeat what we said at the outset of this intervention, the use of autonomous weapon systems brings a loss of human control that entails serious risks for protected persons in armed conflict (both civilians and combatants no longer fighting) and of violations of IHL. This unique characteristic of autonomous weapon systems, raises difficulties in the interpretation and application of IHL rules and, ultimately, it raises the question of whether existing law is sufficiently clear or whether there is a need to clarify IHL or to develop new rules, or standards. This is a question we will return to in this afternoon’s session. Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 25-29 March 2019, Geneva Statement of the International Committee of the Red Cross (ICRC) under agenda item 5(b) further consideration of the human element in the use of lethal force; aspects of human-machine interaction in the development, deployment and use of emerging technologies in the area of lethal autonomous weapon systems In our last intervention, the ICRC explained the legal basis for human control, that is, the limits on autonomy that can be deduced from IHL rules. In the ICRC’s view, ethical considerations also demand human control over weapon systems and the use of force. As you know, the ICRC is convinced that human intention and agency – the human actor – must be sufficiently retained in decisions to use force. Moral responsibility for these decisions cannot be delegated to a machine, no more than legal responsibility can. With the legal and ethical bases of human control in mind, the ICRC wishes to dedicate its intervention this afternoon to the key elements of human control demanded by legal and ethical considerations (noting that ethical concerns might demand additional limits on autonomy – an issue we have spoken about before). These key elements are: 1. Human supervision and the ability to intervene and deactivate 2. Predictability and reliability, and 3. Operational constraints. 1. Human supervision, and ability to intervene and deactivate Human supervision, and the ability to adapt to changing circumstances, is essential to ensure compliance with IHL, including when carrying out an attack with a weapon system with autonomy in its critical functions. This requires supervision of both the weapon system and the target area, in other words situational awareness. Generally speaking, constant human supervision of the weapon system and the target area may be required, so that the operator has sufficient information and understanding about the operation of the weapon system, the environment of use, and the interaction of the two, over the given time period and geographical area. This information is necessary for making the context-based legal judgments required by IHL. A physical and/or communication link that permits adjustment of the engagement criteria and the ability to cancel the attack, as well as sufficient time for such intervention, is necessary if the supervision is to serve its purpose of ensuring compliance with IHL. Without human supervision – and ability to intervene and deactivate – it is difficult to envisage how operators/commanders could take into account changes in the situation and thereby exercise the legal judgements required by IHL in carrying out attacks. IHL requires the attacker to maintain awareness of, and adapt to, continuously changing circumstances, even after the decision to attack. A party to the conflict must do everything feasible to cancel or suspend an attack if it becomes apparent that the target is not a military objective, or that the attack may be expected to cause incidental loss of civilian life, injury to civilians, damage to civilian objects, or a combination thereof which would be excessive in relation to the concrete and direct military advantage anticipated (API article 57(2)(b), CIHL rule 19). The practical effect of this rule is that weapon systems must remain under human supervision, and must permit the user to, where feasible, cancel, suspend or modify attacks, up until the execution of the attack (or halting of the attack). Feasibility means what is practicable or practically possible, taking into account all the circumstances at the time, including humanitarian and military considerations. The existence of autonomy in the critical functions of a weapon system may render precautionary measures unfeasible. Precautions will not always be precluded when using an autonomous weapon system, but the ICRC is interested to hear from States about how they see the interaction between autonomy and feasibility and precautions. In the ICRC’s view, the use of an autonomous weapon system that does not permit the taking of precautions such as cancelling or suspending an attack – in situations where there is a reasonable likelihood that the circumstances will change enough to render an attack unlawful – would likely be unlawful. 2. Predictability and reliability Autonomous weapon systems, since they self-initiate attacks, all raise concerns about unpredictability, owing to varying degrees of uncertainty about location, timing and/or nature of the subsequent machine-initiated attack. Predictability (knowledge of the consequences of use) and reliability (likelihood of failure) are dependent on the: 1 weapon system design (including the software and algorithms that control the system); task it is used for; nature of the environment where it is used; and interaction between the system and the environment. All autonomous weapon systems, which operate based on interaction with their environment, raise questions about human control and predictability. The greater the complexity of the environment and complexity of the task, the greater the need for human control and the less tolerance of autonomy, from both a legal and ethical perspective. 1 Predictability is the ability to “say or estimate that (a specified thing) will happen in the future or will be a consequence of something”. Applied to an autonomous weapon system, predictability is knowledge of how it will function in the circumstances of use, and the effects that will result. Reliability is “the quality of being trustworthy or performing consistently well”. In this context, reliability is knowledge of how consistently the machine will function as intended, e.g. without failures or unintended effects. Humans can exert some control over autonomous systems through “human on the loop” interaction with them. However, this is not a panacea due to human-machine interaction problems, such as automation bias, over-trust in the system, or lack of operator awareness of the system state at the time of intervention. Further, there are challenges in quantifying the level of predictability (and reliability) needed to ensure a level of human control, or judgement, sufficient for IHL compliance (and ethical acceptability). Testing also raises unique challenges since it is not possible to test all possible environmental inputs to an autonomous system. In order to comply with IHL, those who plan or decide on an attack using an autonomous weapon system must understand its capabilities and limitations in the given circumstances, in order to determine whether it will perform lawfully in the given circumstances. This also requires knowledge of the environment over time (see above on human supervision). In general, IHL demands a high level of certainty about the prevailing circumstances and the effect of the chosen means and methods of warfare. One example of this is the rule on the loss of civilian protection in attack. During an attack, doubt as to status of a person must be resolved in favour of treating the individual as a civilian. It is challenging to pinpoint exactly where IHL lines will be crossed with respect to predictability and reliability. The ICRC would be interested to hear from States what levels of predictability/reliability are demanded by IHL. Indeed, the absence of clarity on this issue is one of the reasons that we have called for States to set internationally agreed limits on autonomy in weapon systems. A weapon system that is unpredictable by design would be unlawful by its nature. Let us explain what we mean by that. Unpredictability of a weapon system’s interaction with its environment will inevitably create risks for protected persons and objects in that environment. There are fundamental concerns about autonomy in critical functions controlled by machinelearning algorithms, since these are generally unpredictable in their functioning, not transparent (i.e. they are “black boxes” and their functioning cannot be explained), and their performance therefore cannot be verified during testing. Not only are such algorithms unpredictable, but they can also introduce bias, whether by design or through bias in the data used to “train” (develop) the algorithms. This kind of ‘unpredictability by design’ would raise concerns in all situations, as the person who plans or decides on an attack cannot have reasonable certainty about the effects of the weapon. Any weapon systems with autonomy in critical functions whose software can set its own goals, or learn, change or adapt its functioning after deployment, would be inherently unpredictable, and therefore beyond human control and unlawful under IHL. This is because any assessment that the user has made at the moment of activating the weapon to ensure compliance with IHL would immediately become invalid after deployment. Self-organising swarms used as autonomous weapons could raise similar concerns due to their inherent unpredictability of their behaviour in the environment. 3. Operational constraints Operational parameters and constraints are important for human control in particular the: task the weapon is used for; types of targets it attacks, type of force (and effects) it employs; operating environment; duration of autonomous operation (time-frame); and scope of movement over and area (mobility). An autonomous weapon system must be capable of being used – and must be used – in accordance with existing rules of IHL - notably the rules of distinction, proportionality and precautions in attack, which require complex context-based assessments by commanders based on the circumstances prevailing at the time of the attack. As the ICRC explained in our intervention this morning, these assessments must be reasonably proximate in time to the attack (or “strike”) to comply with these rules. Autonomy in the critical functions of weapons complicates the ability of the commander, to make these context specific judgements since, upon activation, they do not know the timing, location and nature of the subsequent attack(s) that the weapon will self-initiate. The lawfulness of use relies on the continuing validity of IHL assessments and planning assumptions made at the point of activation. Whether an autonomous weapon system will operate within the constraints of IHL once activated will depend on the technical performance of the specific weapon, especially its predictability and reliability (see above), and the specific circumstances and environment of use. Predictability in the consequences of its use will depend not only on the technical design of the system, but on variations in the environment over time and the interaction of the system with that environment, taking into account the task it is used for. The more variable the environment of use, or the longer the timespan between the human decision to activate the weapon and the eventual use of force initiated by the weapon system in response to the environment, the greater the risk of IHL violations. The risk that IHL might be violated can be reduced by manipulating operational parameters like the environment in which the weapon system is to operate, the mobility of the weapon system in space and the time-frame of its operation, in order to increase predictability in the consequences of its use. The more predictable (non-dynamic) the environment, and the more highly constrained the system is in time and space, the greater predictability in the consequences of activating the system. The nature of the spatial/temporal limitations required by IHL is one on which further clarity would be useful. Reaching agreement on the kinds of operational constraints necessary as part of human control for IHL compliance (and ethical acceptability) could be a part of States’ efforts to set limits on autonomy in weapon systems. However, given the high degree of unpredictability of most real world conflict environments, it is likely that operational constraints alone will only help avoid an unacceptable risk of IHL violations in the narrowest of circumstances, and will generally not be sufficient to ensure IHL compliance with IHL in carrying out an attack with an autonomous weapon system. Mr Chair, this intervention has already been rather long, so we will conclude there and look forward to contributing to the discussions in the coming days. Thank you. 1 Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 25-29 March 2019, Geneva Statement of the International Committee of the Red Cross (ICRC) under agenda item 5(e) Possible options for addressing the humanitarian and international security challenges posed by emerging technologies in the area of lethal autonomous weapon systems in the context of the objectives and purposes of the Convention without prejudicing policy outcomes and taking into account past, present and future proposals Thank you Mr. Chairperson, As we have discussed this week, autonomous weapon systems raise concerns about loss of human control over the use of force, which could have serious humanitarian consequences, in terms of adverse consequences for both civilians and combatants in armed conflict. As the ICRC expressed yesterday, these unique characteristics of autonomous weapon systems – namely the loss of the ability of combatants to exercise the context-specific judgments required of international humanitarian law (IHL) rules, and the loss of human agency and diffusion of moral responsibility in decisions to use force -- raise challenges for IHL compliance and for ethical acceptability and, therefore, raise the question of whether new internationally agreed policies, standards or rules are needed. The common ground on which States can build, and have been building in their interventions this week, is their agreement in the 2018 GGE that human responsibility – or control – must be retained over weapon systems and the use of force. As the ICRC has called for in its Working Paper on the “Element of Human Control” submitted to the CCW Meeting of High Contracting Parties last November (CCW/MSP/2018/WP.3), this “human-centred” approach must guide the development of limits on autonomy in weapon systems, and in particular the practical elements of human control over the critical functions of weapon systems needed for legal compliance and ethical acceptability. The Provisional Programme of Work lists a number of options that have been proposed for addressing the humanitarian and international security challenges posed by autonomous weapon systems. These options, which are “not necessarily mutually exclusive”, include: a legally binding instrument; a political declaration; guidelines, principles or codes of conduct; and improving implementation of existing legal requirements, including national legal reviews of new weapons. As the ICRC stated in its Working Paper, all of these approaches share the same need to develop common understandings of the type and degree of human control necessary in practice to ensure compliance with IHL, and ethical acceptability. This effort must be driven by the necessity to preserve human judgement and responsibility in targeting decisions, where human supervision, predictability and context are important factors. 2 Working on the parameters of a positive obligation for human control will also enable States to identify “autonomous weapon systems of concern” that fall outside of effective human control, and would therefore be unlawful and/or ethically unacceptable. Insofar as the sufficiency of existing law – and in particular of IHL – is concerned, it is clear, as the ICRC submitted again yesterday, that existing IHL rules – in particular distinction, proportionality and precautions required in attack – already provide some limits to autonomy in weapon systems, insofar as they establish responsibilities for combatants, who must retain the ability to make the contextspecific assessments required of IHL rules during the operation of the weapon system. We described the IHL limits in more detail in our statement. However, it is also clear that existing IHL rules – while already limiting autonomy in weapons to a certain degree – do not provide all the answers. Key questions include:  What is the type and degree of human control over weapon systems, including the level of human supervision and spatial and temporal constraints on their operation, required to ensure compliance with IHL rules?  What are the minimum levels of predictability (or maximum levels of unpredictability) that are tolerable in the functioning of weapon systems, to ensure compliance with IHL rules? Debates in the GGE this week and in the last two years have shown that there are serious questions whether existing IHL is sufficiently clear, or whether there is a need to clarify IHL or to develop new rules – i.e. new law. This again points to the urgent need to, at minimum, reach common understandings on the practical elements of human control. The ICRC has welcomed the attention that this discussion has brought to improving and developing legal review processes. Effective legal reviews are critical to ensuring that a State’s armed forces comply with IHL in light of rapid technological developments. Greater transparency in how States interpret and apply their obligation to carry out legal reviews of new weapons to new technologies can help to identify good practices and thus assist States seeking to comply with their legal review obligations, particularly when reviewing autonomous weapon systems, whose unique characteristics can make legal reviews more difficult. But the bottom line is that while robust legal reviews remain essential, they are not a substitute for States working towards internationally agreed limits on autonomy in weapon systems. Moreover, the limits dictated by ethical considerations may go beyond those found in existing IHL rules. In particular, ethical concerns have been expressed about the loss of human agency in decisions to use force, diffusion of moral responsibility and loss of human dignity are most acutely felt with autonomous weapon systems that present risks for human life, and especially with the notion of antipersonnel systems, that is, those designed to target humans directly. As a result, public conscience may demand limits or prohibitions on particular types of autonomous weapons – such as anti-personnel systems – and/or their use in certain environments – such as where civilians and civilian objects are present. Perhaps these ethical boundaries are already evident with existing autonomous weapons, which generally only target objects and not humans, and are used in very narrow circumstances for defensive 3 purposes in environments where there are few civilians. And even in these narrow circumstances, human supervision and ability to intervene and deactivate is retained. Regarding a proposed ban on so-called “fully autonomous weapon systems” -- such as weapons that are unsupervised, unpredictable and unconstrained in time and space (that is, beyond any human control) -- the ICRC notes that many States are of the view that such weapons would almost certainly be unlawful by their very nature (a view that it shares), and many have expressly stated they have no intention of developing and using such weapons. Yet the ongoing militarization and weaponization of autonomous technologies demands a standard of human control that is relevant to current and emerging developments as well as to future technological and operational developments. Most States appear to agree that the yardstick against which the legal and ethical acceptability of all existing and future weapon systems with autonomy in their critical functions must be assessed is meaningful (effective or significant) human control, or put another way, ensuring appropriate levels of human judgment in decisions to use force. However, the question remains: what would be the practical elements (or criteria) of this positive obligation of human control? In terms of articulating a standard of human control, and therefore of limits on autonomy in weapon systems and the use of force, could a requirement for human control (responsibility) take one of these two possible approaches?  A requirement for direct or remote human control over all weapons i.e. that would generally exclude autonomous weapon systems, with specific exceptions? Or  A requirement for meaningful / effective / substantive human control or judgment over all weapons with autonomy in their critical functions, with specific prohibitions? In terms of the type and degree of human control that would be needed to comply with IHL and satisfy ethical concerns:  Must there be constant human supervision, with the ability to intervene and de-activate, for the duration of the weapon’s operation? And what would be the exceptions, if any, to this requirement?  Must there be a requirement that the human operator be able to predict, with a high degree of certainty, that the weapon will attack a specific target at a specific point in time, and its effects? And what would be the exceptions, if any, to such requirement?  What are the standards of reliability of weapons with autonomy in their critical functions, also as regards testing?  Assuming that autonomous weapons that are designed to select and attack materiel targets (objects) are acceptable, what operational constraints apply, in particular to the environment in which they operate (e.g. populated or unpopulated area), the duration (time-limit) of their operation? What would be the spatial and temporal limits that would be applied to mobile (as opposed to stationary) systems? The ICRC believes that these key questions can help guide efforts to identify a standard of human control that is clear, robust and practical, and that withstands the test of time. 4 Thank you. CCW Meeting of Experts on “LAWS”, 17 April 2015 Closing statement by the International Committee of the Red Cross (ICRC) Thank you Mr. Chairman, The ICRC very much welcomes broad agreement of the need for continued discussions on autonomous weapon systems at the CCW next year. We remain of the view that incrementally increasing autonomy in weapons systems – specifically in the critical functions of identifying, selecting, and attacking targets – raises fundamental questions about human control over the use of force. In our view, future discussions at the CCW should address: whether these developments may affect the ability of parties to armed conflicts to respect international humanitarian law; and whether these developments are acceptable under the principles of humanity and the dictates of public conscience. Discussions this week have highlighted, once again, broad agreement on the need to retain human control over the critical functions of weapon systems. To move forward, there is now a need for States to address where the limits should lie, in order to ensure that meaningful, adequate and effective human control is maintained over weapon systems. The work of the CCW in the next year should focus on this objective. Mr. Chairman, The ICRC welcomes the wide recognition by States that legal reviews at the domestic level of autonomous weapon systems, as with any new weapon, are required under Article 36 of Additional Protocol I to the Geneva Conventions or under customary international humanitarian law. We wish to recall that this recognition is not new to States party to the CCW. Indeed, past Review Conferences have recalled the importance of legal reviews of new weapons in connection with the CCW’s work. We encourage States to share their experience and outcomes of legal reviews of existing weapons with autonomy in their critical functions. In this respect, we also welcome proposals on sharing good practices for legal review mechanisms in order to improve implementation of the said legal obligations. However, as we have stated previously, these efforts to encourage implementation of national legal reviews are not a substitute for States party to the CCW to consider possible policy and other options at the international level to address the legal and ethical limits to autonomy in weapon systems. Rather these two approaches are very much complimentary. 1 ICRC statement to concluding session CCW Expert Meeting on "Lethal Autonomous Weapon Systems" 16 May 2014 Thank you Mr. President. As the ICRC has previously stated here and elsewhere, the development of weapons with increasingly autonomous functions – i.e. with autonomy in the "critical functions" of identifying and attacking (using force) against targets – has profound implications for the future of warfare and indeed of humanity. We are therefore very pleased that the delegations that have taken the floor in this session seem to agree that discussions on this issue in the CCW should continue through 2015. We would like to highlight a few key issues that have emerged this week, which could help frame future discussions on autonomous weapon systems. Regarding the overall scope of the discussions, we are convinced of the need to ground the discussions in how weapon technology is developing rather than attempting to predict the future. In particular, there is a need to look at increasing autonomy in weapon systems, and specifically at autonomy in the critical functions of acquiring, tracking, selecting and attacking targets. This is an incremental process and not a sudden development, which gives us an opportunity to look carefully at this trend and consider the implications before developments come to fruition. At some point on an incremental process of increasing autonomy in the critical functions of weapon systems, human control may no longer be meaningful. As we mentioned in our opening statement, we believe the crucial aspect is human control over the use of force, and what constitutes meaningful, appropriate and responsible human control over the critical functions of weapon systems. Where humans are so far removed in time and space from control over the weapon system, the human decision-making process on the use of force may in effect be substituted with machine decision-making. Regarding the "Martens Clause", which several delegations have raised in their interventions in relation to the acceptability of autonomous weapons, we believe it is useful to recall that the "Martens Clause" refers to a provision found in several international humanitarian law treaties, starting with the preambles of Hague Conventions II of 1899 and IV of 1907. It's modern incarnation is Article 2(1) of Additional Protocol I of the Geneva Conventions, which reads: In cases not covered by international humanitarian law], civilians and combatants remain under the protection and authority of the principles of international law derived from established custom, from the principles of humanity and from dictates of public conscience. (It is also found in the preamble of Additional Protocol II.) The International Court of Justice, in its 1996 Advisory Opinion on the Legality of the Threat or Use of Nuclear Weapons, affirmed that the applicability of the Martens Clause "is not to be doubted" and that it had "proved to be an effective means of addressing rapid evolution in military technology. 2 The Court also found that the Martens Clause represent customary international law – in other words, it is legally binding on all States, not just those that are party to the treaties I have just mentioned. Therefore, a weapon that is not covered by existing rules of international humanitarian law would be considered contrary to the Martens Clause if it is determined per se to contravene the principles of humanity or the dictates of public conscience. Admittedly, some dispute this interpretation. But it is useful to recall the interpretation of the ICRC Commentary to the Additional Protocols: "the Martens clause prevents the assumption that anything which is not explicitly prohibited by the relevant treaties is therefore permitted". Furthermore, "it should be seen as a dynamic factor proclaiming the applicability of the principles mentioned regardless of subsequent developments of types of situation or technology". In other words, at minimum the Martens Clause provides that the acceptability of new technologies of warfare can be judged against the principles of humanity and the dictates of public conscience. Mr. President, Many delegations this week stressed the importance of legal reviews of autonomous weapons. The ICRC welcomes the wide recognition of the need to carry out thorough legal reviews of the new technologies of warfare they are developing or acquiring, including weapons that have autonomy in their critical functions. Such legal review must determine whether weapons with autonomy in their critical functions are capable of being used, in some or all circumstances (to use the terms of Article 36 of Additional Protocol I) in accordance with international humanitarian law. They must also take into account the compatibility of the weapon with the principles of humanity and the dictates of public conscience (the "Martens Clause"). In the context of discussions in the framework of the CCW, we encourage States to be as transparent as possible in sharing national experiences of legal reviews of weapons with autonomous features. Lessons from the review of autonomy in existing weapons could provide a guiding framework for legal reviews of weapons with increasing levels of autonomy in their critical functions. At the same time, it is clear that the issues and questions raised by autonomous weapon systems cannot be addressed solely through national processes. Multilateral discussions, such as those that have taken place here, must continue in order to address the issues identified this week. Thank you. Convention on Certain Conventional Weapons (CCW) Meeting of Experts on Lethal Autonomous Weapons Systems (LAWS), 13-16 May 2014, Geneva Statement of the International Committee of the Red Cross (ICRC) 13 April 2015 Thank you Mr Chairman. The International Committee of the Red Cross (the ICRC) is pleased to contribute its views to this second CCW Meeting of Experts on “Lethal Autonomous Weapon Systems”. The CCW, which is grounded in international humanitarian law (IHL), provides an important framework to further our understanding of the technical, legal, ethical and policy questions raised by the development and use of autonomous weapon systems in armed conflicts. This week will provide an opportunity to build on last year’s meeting to develop a clearer understanding of the defining characteristics of autonomous weapon systems and their current state of development, so as to identify the best approaches to addressing the legal and ethical concerns raised by this new technology of warfare. We will have the opportunity to comment in more detail during the thematic sessions but at the outset we would like to highlight a few key points on which the ICRC believes attention should be focused this week. We first wish to recall that the ICRC is not at this time calling for a ban, nor a moratorium on “autonomous weapon systems”. However, we are urging States to consider the fundamental legal and ethical issues raised by autonomy in the ‘critical functions’ of weapon systems before these weapons are further developed or deployed in armed conflicts. We also wish to stress that our thinking about this complex subject continues to evolve as we gain a better understanding of current and potential technological capabilities, of the military purposes of autonomy in weapons, and of the resulting legal and ethical issues raised. To ensure a focussed discussion, the ICRC believes that it will be important to have a clearer common understanding of what is the object of the discussion, and in particular of what CHECK AGAINST DELIVERY 2 constitutes an autonomous weapon system. Without engaging in a definition exercise, there is a need to set some boundaries for the discussion. As the ICRC proposed at last year’s CCW Meeting of Experts, an autonomous weapon system is one that has autonomy in its ‘critical functions’, meaning a weapon that can select (i.e. search for or detect, identify, track) and attack (i.e. intercept, use force against, neutralise, damage or destroy) targets without human intervention. We have suggested that it would be useful to focus on how autonomy is developing in these ‘critical functions’ of weapon systems because these are the functions most relevant to ‘targeting decision-making’, and therefore to compliance with international humanitarian law, in particular its rules on distinction, proportionality and precautions in attack. Autonomy in the critical functions of selecting and attacking targets also raise significant ethical questions, notably when force is used autonomously against human targets. The ICRC believes that it would be most helpful to ground discussions on autonomous weapon systems on current and emerging weapon systems that are pushing the boundaries of human control over the critical functions. Hypothetical scenarios about possible developments far off in the future may be inevitable when discussing a new and continuously evolving technology, but there is a risk that by focussing exclusively on such hypothetical scenarios, we will neglect autonomy in the critical functions of weapon systems that actually exist today, or that are currently in development and intended for deployment in the near future. From what we understand, many of the existing autonomous weapon systems have autonomous ‘modes’, and therefore only operate autonomously for short periods. They also tend to be highly constrained in the tasks they are used for, the types of targets they attack, and the circumstances in which they are used. Most existing systems are also overseen in real-time by a human operator. However, future autonomous weapon systems could have more freedom of action to determine their targets, operate outside tightly constrained spatial and temporal limits, and encounter rapidly changing circumstances. The current pace of technological change and military interest in autonomy for weapon systems lend urgency to the international community’s consideration of the legal and ethical implications of these weapons. As the ICRC has stressed in the past, closer examination of existing and emerging autonomous weapon systems may provide useful insights regarding what level of autonomy and human control may be considered acceptable or unacceptable, and under which CHECK AGAINST DELIVERY 3 circumstances, from a legal and ethical standpoint. In our view, this would allow for more informed deliberations. Mr. Chairman, Based on what we have heard this morning and on the discussions that took place last year in the CCW and elsewhere, there appears to be broad agreement among States on the need to retain human control over the critical functions of weapon systems. States should now turn their attention to agreeing a framework for determining what makes human control of a weapon meaningful or adequate. Discussions should focus on the types of controls that are required, in which situations, and at which stages of the process – programming, deployment and/or targeting (selecting and attacking a target). It is also not disputed that autonomous weapons intended for use in armed conflict must be capable of being used in accordance with international humanitarian law (IHL), in particular its rules of distinction, proportionality and precautions in attack. Indeed, weapons with autonomy in their critical functions are not being developed in a ‘legal vacuum’, they must comply with existing law. Based on current and foreseeable robotics technology, it is clear that compliance with the core rules of IHL poses a formidable technological challenge, especially as weapons with autonomy in their critical functions are assigned more complex tasks and deployed in more dynamic environments than has been the case until now. Based on current and foreseeable technology, there are serious doubts about the ability of autonomous weapon systems to comply with IHL in all but the narrowest of scenarios and the simplest of environments. In this respect, it seems evident that overall human control over the selection of targets and use of force against them will continue to be required. In discussions this week, we encourage States that have deployed, or are currently developing, weapon systems with autonomy in their critical functions, to share their experience of how they are ensuring that these weapons can be used in compliance with IHL, and in particular to share the limits and conditions imposed on the use of weapons with autonomous functions, including in terms of the required level of human control. Lessons learned from the legal review of autonomy in the critical functions of existing and emerging weapon systems could help to provide a guiding framework for future discussions. CHECK AGAINST DELIVERY 4 In this respect, the ICRC welcomes the wide recognition of the obligation for States to carry out legal reviews of any new technologies of warfare they are developing or acquiring, including weapons with autonomy in some or all of their critical functions. CCW Meetings of States Parties and Review Conferences have in the past recalled the importance of legal reviews of new weapons, which are a legal requirement for States party to Additional Protocol I to the Geneva Conventions. The ICRC encourages States that have not yet done so to establish weapons review mechanisms and stands ready to advise States in this regard. In this respect, States may wish to refer to the ICRC’s Guide to the Legal Review of New Weapons, Means and Methods of Warfare. Finally Mr. Chairman, the ICRC wishes to again emphasise the concerns raised by autonomous weapon systems under the principles of humanity and the dictates of public conscience. As we have previously stated, there is a sense of deep discomfort with the idea of any weapon system that places the use of force beyond human control. In this respect, we would like this week to hear the views delegations on the following crucial question for the future of warfare, and indeed for humanity: would it be morally acceptable, and if so under what circumstances, for a machine to make life and death decisions on the battlefield without human intervention? We will be pleased to elaborate on our views further during the thematic sessions. Thank you. ICRC statement, CCW GGE “LAWS”, Monday 25 March 2019 CHECK AGAINST DELIVERY 1 Agenda Item 5 (c): CHARACTERISATION. The importance of critical functions • ICRC has characterised autonomous weapon systems broadly as: “Any weapon system with autonomy in its critical functions. That is, a weapon system that can select and attack targets without human intervention.” After initial activation by a human operator, the weapon system – though its sensors, software (programming / algorithms) and connected weapon(s) – takes on the targeting functions that would normally be controlled by humans. • Autonomy in these “critical functions” of selecting and attacking targets is central to humanitarian, legal, and ethical considerations within the scope of the Convention on Certain Conventional Weapons (CCW). It is these functions: that result in injury, damage and destruction to persons or objects in armed conflict; that are governed by international humanitarian law (IHL) rules on the conduct of hostilities; and that raise ethical questions about the role of humans in life and death decisions. • The key distinction, in our view, from non-­‐autonomous weapons is that the machine self-­‐initiates an attack. • With reference to the earlier discussion today of military applications, it may be useful to distinguish between: o Weapon systems where the human choses the specific target at a particular point in time and at a specific location, for example missiles and other munitions with guidance functions; and o (Autonomous) weapon systems, where the exact timing, location and/or nature of the attack not known to the user since it is self-­‐initiated by the weapon, which is – in-­‐turn – triggered by its environment. • As regards the latter, autonomy in critical functions is a feature that could be applied to any weapon system, especially robotic weapon systems (e.g. in the air, on land or at sea), and it is already found to a limited extent in some existing weapons, such as air defence systems, “active protection” systems and some loitering weapons. • In the ICRC’s understanding, notions of “automated” and “autonomous” weapon systems are not easily distinguishable from a technical perspective. But more importantly, they not easily distinguishable from a humanitarian, legal and ethical perspective. • In the ICRC’s view, the core issue is ensuring meaningful/effective/sufficient/appropriate human control over decisions to select and attack targets, independent of the technical sophistication of the weapon system. Agenda Item 5 (d): MILITARY APPLICATIONS. Lessons for human control from autonomy in existing weapon systems • Linking characterisation to discussions this morning on military applications, experiences with existing weapons can highlight: o On one hand, the potential humanitarian consequences, legal questions and ethical concerns introduced by autonomy in critical functions and; o On the other hand, the practical lessons for human control measures that need to be taken to mitigate these risks, and to ensure compliance with IHL, and ethical acceptability. ICRC statement, CCW GGE “LAWS”, Monday 25 March 2019 CHECK AGAINST DELIVERY 2 • The ICRC explored these issues in an international expert meeting it convened with a group of 20 States in 2016, and also proposed some elements of human control in its working paper to the CCW Meeting of High Contracting Parties in November 2018. • In terms of lessons for human control measures, for example with existing air defence weapons, which have autonomous modes, States using such systems have highlighted the need for human control in three key aspects: 1) Human supervision, and ability to intervene and deactivate From what the ICRC understands, and from explanations today and in previous GGE sessions with presentations by military experts, these systems are under constant supervision by a human operator with the ability – through a physical and/or communication link – to intervene and deactivate the system at any time. o As we understand, many such systems retain the ability, even with incoming projectiles, for a human operator to visually verify the projectile on screen and decide to cancel the attack if necessary. 2) Predictability and reliability It would have to be assumed that existing systems have been tested with and meet high standards of predictability and reliability for their intended use, including the intended environment of use. In that sense, it would be beneficial for States to share the standards they use in making such assessments of predictability and reliability, including as part of national legal reviews. 3) Operational constraints Operational constraints are clearly very important for human control, for increasing predictability in the consequences of use, and for compliance with IHL, in particular: • Limits on the task. These systems are used for a single, relatively simple, task to defend against incoming projectiles. • Limits on the targets. These systems are only used against materiel targets, such as projectiles and military aircraft, vehicles and drones. They are not used to target humans (personnel) directly. • Limits on the environment. These systems are only used in highly constrained and relatively simple environments, where there are few or no civilians or civilian objects present. As the ICRC understands, specific measures are also taken to monitor the environment and ensure civilian objects to not enter the area. • Time frame of autonomous operation. In these systems, the autonomous mode is only activated for short periods, and can be switched back to manual mode at any time. • Scope of movement over an area. These systems are fixed in place, at a perimeter, or on a ship or armoured vehicle, thereby limiting their range and effects. • Other existing weapons with automated or autonomous critical functions illustrate some of the difficulties of ensuring human control and ensuring compliance with IHL. In particular, where they are unsupervised, unpredictable, or insufficiently constrained in time and space. • For example, the use of a loitering weapon with autonomy in critical functions creates unpredictability for the user: o Due to the long time scale of autonomous operation (up to several hours) and large area of operation (up to hundreds of square kilometres), there is a high level of unpredictability ICRC statement, CCW GGE “LAWS”, Monday 25 March 2019 CHECK AGAINST DELIVERY 3 for the user as to the timing and location of the subsequent attack, which raises questions for IHL compliance. o This unpredictability is even greater where the system is not supervised by the human operator with the ability to intervene and deactivate, should the attack need to be cancelled during that period. o ICRC would like to hear more from military experts familiar with the operation of these types of systems. • Another example of a much older and simpler technology, where unpredictability and lack of human control has raised concerns, is the mine: o The consequences of use of anti-­‐personnel landmines starkly illustrate the serious dangers to civilians from weapons, which are triggered by their environment, and where the human operator has insufficient knowledge – at the point of use or activation – about the subsequent timing, (location) and nature of the attack that may result. Recognition of the resulting indiscriminate effects on civilians led to the prohibition of anti-­‐personnel landmines by the majority of States. Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 3–13 August 2021, Geneva Statement of the International Committee of the Red Cross The International Committee of the Red Cross (ICRC) welcomes the resumption of work by the Group of Governmental Experts (GGE) at this critical moment in multilateral deliberations on autonomous weapon systems, and with less than five months to go until the Review Conference of the Convention. The ICRC appreciates the efforts of the chair of the GGE, Ambassador Marc Pecsteen de Buytswerve of Belgium, to solicit proposals for consensus recommendations of the Review Conference on the “normative and operational framework”. It is the ICRC’s view that an urgent and effective international response is needed to address the serious risks posed by autonomous weapon systems, as highlighted by many states and civil society organizations over the past decade. These risks stem from the process by which autonomous weapon systems function. It is the ICRC’s understanding that these weapons, after initial activation, select and apply force to targets without human intervention, in the sense that they are triggered by their environment based on a “target profile”, which serves as a generalized approximation of a type of target. The user of an autonomous weapon system does not choose the specific target, nor the precise time or place that force is applied. This process risks the loss of human control over the use of force and it is the source of the humanitarian, legal, and ethical concerns. These concerns are significant when one considers that autonomy in the critical functions of selecting and applying force could be integrated into any weapon system. The central challenge with autonomous weapon systems resides in the difficulty of anticipating and limiting their effects. From a humanitarian perspective, they risk harming those affected by armed conflict, both civilians and combatants hors de combat, and they increase the risk of conflict escalation. From a legal perspective, they challenge the ability of persons who must apply the rules of international humanitarian law (IHL) during the planning, decision and execution of attacks to comply with their obligations. From an ethical perspective, this process of functioning risks effectively substituting human decisions about life and death with sensor, software and machine processes. This raises ethical concerns that are especially acute when autonomous weapon systems are used to target persons directly. Today, autonomous weapon systems are highly constrained in their use; they are mostly used against certain types of military objects, for limited periods of time, in restricted areas where civilians are not present and with close human supervision. However, current trends in the expanded development and use of autonomous weapon systems exacerbate core concerns dramatically. In particular, there is military interest in their use against a wider range of targets, over larger areas and for longer periods of time, in urban areas where civilians would be most at risk and with reduced human supervision and capacity for intervention and deactivation. 2 Worryingly, the use of artificial intelligence and machine learning software to control the critical functions of selecting and applying force is being increasingly explored, which would exacerbate the already difficult task that users have in anticipating and limiting the effects of an autonomous weapon system. Against this background, the High Contracting Parties to the CCW have a responsibility and an opportunity to make progress in clarifying, considering and developing the normative and operational framework for autonomous weapon systems. Given this opportunity, the ICRC offered recommendations to all states on 12 May 2021, which were also submitted to the chair of the GGE on 11 June 2021. The ICRC recommends that states adopt new, legally binding rules to regulate autonomous weapon systems to ensure that sufficient human control and judgement is retained in the use of force. It is the ICRC’s view that this will require prohibiting certain types of autonomous weapon systems and strictly regulating all others. First, unpredictable autonomous weapon systems should be expressly ruled out, notably because of their indiscriminate effects. This would best be achieved with a prohibition on autonomous weapon systems that are designed or used in a manner such that their effects cannot be sufficiently understood, predicted and explained. Secondly, the use of autonomous weapon systems to target human beings should be ruled out. This would best be achieved through a prohibition on autonomous weapon systems that are designed or used to apply force against persons directly as opposed to against objects. Thirdly, the design and use of non-prohibited autonomous weapon systems should be regulated, including through a combination of limits on the types of target, such as constraining them to objects that are military objectives by nature; limits on the duration, geographical scope and scale of use, including to enable human judgement and control in relation to a specific attack; limits on situations of use, such as constraining them to situations where civilians or civilian objects are not present; and imposing a requirement for human–machine interaction, notably to ensure effective human supervision and timely intervention and deactivation. It is the ICRC’s understanding that these proposed prohibitions and restrictions are in line with current military practice in the use of autonomous weapon systems. While these recommendations offer clear, principled and pragmatic guidance on where to draw the boundaries of what is acceptable in light of humanitarian, legal and ethical concerns, they are not ready-made treaty text and the finer details of these limits will require further elaboration by states. It is encouraging, therefore, that there is an increasing convergence of views among states that certain autonomous weapon systems should be prohibited or otherwise excluded from development and use, and that others should be regulated or otherwise limited in their development and use. These proposals reflect widely held views: a recognition of the need to ensure human control and judgement in the use of force; an acknowledgement that ensuring such control and judgement requires effective limits on the design and use of autonomous weapon systems; and an increasing confidence that such limits can be articulated at international level. 3 It is also encouraging that many states have shown a readiness to clarify how IHL already constrains autonomous weapon systems and some have proposed the further sharing of current military practice. The ICRC is convinced that international limits on autonomous weapon systems must take the form of new, legally binding rules. New rules are required because of the seriousness of the risks, the necessity to clarify how existing IHL rules apply and the need to develop and strengthen the legal framework in line with ethical and rule of law issues and humanitarian considerations. International law must continue to evolve in order to uphold and strengthen protections in the face of evolving military technology and practice. The High Contracting Parties to the CCW, which is a framework Convention anchored in IHL, acknowledge the “need to continue the codification and progressive development of the rules of international law applicable in armed conflict”, as they have affirmed in the Convention’s preamble. Considering current military developments in how autonomous weapon systems are being used and deployed, the ICRC urges the High Contracting Parties to the CCW to take action now towards the adoption of new rules. Building on the extensive and in-depth work of the CCW and this GGE over the past eight years, there is an opportunity to shape an international response that will effectively strengthen protections for those affected by armed conflict, uphold the legal obligations and moral responsibilities of persons conducting conflict and safeguard our shared humanity. It is an opportunity that the High Contracting Parties must seize and focus on in their work during this meeting. Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 27-31 August 2018, Geneva Statement of the International Committee of the Red Cross (ICRC) under agenda item 6(d) Thank you Mr. Chair. At the outset, let me, on behalf of the ICRC, thank the many delegations that have taken the floor under this important agenda item to share their views. The ICRC appreciates just how far the discussions in the CCW on autonomous weapon systems have come in the last five years. Rapid developments in military applications of robotics and artificial intelligence technologies lend urgency to these ongoing discussions and to achieving progress in identifying and building on common ground, with the aim of ensuring that decisions to kill, injure and destroy remain with humans, and that such critical decisions are not effectively left to sensors and software. Autonomous weapon systems raise concerns about loss of human control of the use of force, which could have serious consequences for both civilians and combatants in armed conflict. The ICRC’s view is that these characteristics of autonomous weapon systems raise unique issues and challenges for legal compliance and for humanity and, in turn, raise the question of whether new internationally agreed policies, standards or rules are needed. The meetings of this Group of Governmental Experts (GGE) in April, and again this week, were encouraging in the general agreement among all States that human control must be maintained over weapon systems and the use of force. This “humancentred” approach should now guide the further work of this GGE to set limits on autonomy in weapon systems and ensure human control is maintained. We’ve heard several proposal put forward by States on how to achieve this goal, including: o the development of new law, in particular the negotiation of a legally binding instrument (a Protocol) to either prohibit autonomous weapon systems, or establish a positive obligation of human control; o a political declaration, reaffirming the applicability of international law (in particular, international humanitarian law (IHL)) and stating human control must be maintained; and o improved implementation and best practices for national legal reviews of new weapons. Independent of the policy response chosen, all approaches share the same requirement for core substantive work, namely that States must determine the type and degree of human control in the use of weapon systems with autonomy in their critical functions that is needed to ensure compliance with international law, in particular IHL, and ethical acceptability. In the ICRC’s view, focused work by States will help determine what (minimum/ sufficient /effective/meaningful/appropriate) human control means in practice. It should be driven by the necessity to preserve human judgement and responsibility in targeting decisions. This approach will enable States at the CCW to: 1) Specify a positive obligation for human control over weapon systems and the use of force, in line with legal obligations and ethical considerations; 2) establish internationally agreed limits on autonomy in weapon systems, that address legal, ethical and humanitarian concerns; and 3) identify “autonomous weapon systems of concern” that fall outside these limits, and would therefore be unlawful and/or unacceptable. The advantage of this approach, which focusses on a positive obligation of human control, in contrast to a negative approach (a prohibition), is that we would avoid getting stuck with debates about definitions and technical characteristics. Focussing on a positive obligation of human control would also, in our submission, result in a standard that withstands the test of time (as you have called for Mr. Chair), as it is focussed on the needed level of human control over weapon systems and the use of force, rather than on technological characteristics. The ICRC is encouraged that an increasing number of States are supporting this approach. Thank you. ICRC statement, CCW GGE on “LAWS”, 27-31 August 2018 2 Convention on Certain Conventional Weapons (CCW) Meeting of Experts on Lethal Autonomous Weapons Systems (LAWS) 11-15 April 2016, Geneva Views of the International Committee of the Red Cross (ICRC) on autonomous weapon system 11 April 2016 Introduction Views on autonomous weapon systems, including those of the International Committee of the Red Cross (ICRC), continue to evolve as a better understanding is gained of current and potential technological capabilities, the military purpose of autonomy in weapon systems, and the resulting questions for compliance with international humanitarian law (IHL) and ethical acceptability. Expert discussions of the last three years in the framework of the Convention on Certain Conventional Weapons (CCW) and in meetings convened by the ICRC and other organisations have been crucial to enhancing this understanding. The ICRC held a second expert meeting on ‘Autonomous weapon systems: Implications of increasing autonomy in the critical functions of weapons’ from 15-16 March 2016 in Versoix, Switzerland.1 Representatives of 20 States together with individual experts and representations of the United Nations and civil society organisations participated in the meeting. The ICRC will soon publish a summary report of the meeting. In the meantime, as a contribution to ongoing discussions in the CCW, this paper highlights some of the key issues on autonomous weapon systems from the perspective of the ICRC, and in the light of discussions at its recent expert meeting. 1. Definitions The ICRC has defined autonomous weapon systems as: “Any weapon system with autonomy in its critical functions. That is, a weapon system that can select (i.e. search for or detect, identify, track, select) and attack (i.e. use force against, neutralize, damage or destroy) targets without human intervention.” The advantage of this broad definition, which encompasses some existing weapon systems, is that it enables real-world consideration of weapons technology to assess what may make certain existing weapon systems acceptable – legally and ethically – and which emerging technology developments may raise concerns under international humanitarian law (IHL) and under the principles of humanity and the dictates of the public conscience. Some prefer a narrow definition more closely linked to identifying the type of weapon systems of greatest concern. In addition, some narrow definitions distinguish between “highly automated” and “autonomous” weapons. However, many experts emphasise that there is not such a clear difference from a technical perspective, and the core legal and ethical questions remain the same. 1 For a report of the first meeting see: ICRC (2014) Autonomous weapon systems technical, military, legal and humanitarian aspects, Report of an Expert Meeting held 26-28 March 2014 (published November 2014), https://www.icrc.org/en/download/file/1707/4221-002-autonomous-weapons-systems-full-report.pdf 2 In the view of the ICRC it is useful to start with a broad scope since lessons from experiences with existing weapon systems with autonomy in their critical functions can inform current discussions on emerging technology. 2. Autonomy in existing weapons There are weapon systems in use today which can select and attack targets without human intervention. After activation by a human operator it is the weapon system, through its sensors and computer programming, which selects a target and launches an attack. One broad group are anti-material defensive weapons used to protect vehicles, facilities or areas from incoming attacks with missiles, rockets, mortars or other projectiles. These include missile and rocket defence weapons and vehicle “active protection” weapons. The ability to effectively control these weapons and the use of force seems to require certain operational constraints including: limits on the task carried out (i.e. a single function to defend against incoming projectiles); limits on the targets (i.e. primarily objects and vehicles); controls over the operational environment (e.g. limitations on the geographical area and time frame of autonomous operation); and procedures for human intervention to deactivate the weapon, i.e. to cease its operation. Some offensive weapon systems, including certain missiles and torpedoes also have a level of autonomy in selecting and attacking targets after launch. Many of these weapons are fired into a particular target area, after which on-board sensors and programming take over to autonomously select and attack a specific target object or person within that area. Some have more freedom of action in time and/or space, and therefore greater autonomy. These include, in particular, loitering munitions that search for targets over a wide geographical area for long time periods, and encapsulated torpedo weapons that remain stationary underwater over long time periods but can carry out attacks autonomously. The trend in the development of missiles appears to be increasing autonomy with respect to movement in time and space. Indications are that future developments could also include increasing adaptability of these weapon systems to their environment. The ability to effectively control these weapons and the use of force may depend on a number of factors (as with defensive systems discussed above) including: limits on the task carried out; the ability of the system to discriminate targets; controls over the operational environment, such as limitations in time and space; and the ability for humans to communicate with the weapon system, for example to deactivate it. The latter is particularly difficult for underwater systems. There are other anti-personnel weapons that may be capable of autonomously selecting and attacking targets, such as so called “sentry” weapons used to defend facilities and borders. However, the systems in use today apparently remain under remote control for initiation of attacks. In sum, autonomy for selecting and attacking targets in existing weapon systems is limited by the operational parameters described above. Moreover, the technical characteristics and performance of existing weapon systems, combined with the operational parameters of their use, provide a certain degree of predictability of the outcomes of using these weapon systems. This predictability may be lost as autonomous weapon systems are used for more complex tasks or deployed in more dynamic environments than has been the case until now. 3. Emerging technology and future autonomous weapons Although it is difficult to foresee the future development of autonomous weapon systems, it is clear there are a number of military drivers for increased autonomy, including enabling: increased mobility of robotic/unmanned weapon systems or platforms; operation of these systems in “communications denied” environments; shorter decision-making times between identifying and attacking a target; increased performance over remotely operated systems; and operation of increased numbers of robotic/unmanned weapon systems by fewer operators. Interest in autonomous weapon systems could also develop in different ways among non-State armed groups. The general trend in civilian robotics is towards supervised autonomy, where robotic systems are increasingly autonomous while human operators retain oversight and often the ability to intervene. 3 The degree of autonomy in a robotic system is related to the level of human intervention in its operation, both in terms of the degree of human intervention and the stages at which the intervention is made. Even so, machines can and do effectively take decisions that have been delegated to them by humans through their computer programming, and without the need to be “conscious” or to have human-like levels of intelligence. There are a number of developments that might make increasingly autonomous weapon systems become less predictable. These include: increased mobility, meaning the weapon system would encounter more varied environments over greater time periods; increased adaptability, such as systems that set their own goals or change their functioning in response to the environment (e.g. a system that defends itself against an attack) or even incorporate learning algorithms; and increased interaction of multiple weapon systems in self-organising swarms. In addition to decreasing the predictability of the weapon system, these developments could raise related problems for the validity of testing to ensure reliability. For example, autonomous weapon systems that could set their own goals, or even “learn” and adapt their functioning, would by their nature be unpredictable. Highly mobile autonomous weapon systems would also create problems of predictability with respect to the target of specific attacks, especially if a system moved over a wide area or carried out multiple attacks. 4. Legal and ethical implications of increasing autonomy in weapon systems It is clear that IHL rules on the conduct of hostilities are addressed to the parties to an armed conflict, more specifically to the human combatants and fighters, who are responsible for respecting them, and will be held accountable for violations. These obligations cannot be transferred to a machine. Still, in practical terms, the question remains, what limits are needed on autonomy in weapon systems to ensure compliance with IHL. Control exercised by human beings can take various forms and operate at different stages of the “life cycle” of an autonomous weapon system, including: 1) the development of the weapon system, including its programming; 2) the deployment and use of the weapon system, including the decision by the commander or operator to use or activate the weapon system; and 3) the operation of the weapon system during which it selects and attacks targets. It is clear that human control is exerted in the development and deployment stages of the weapon’s “life cycle”. It is in stage three, however, when the weapon is in operation – when it autonomously selects and attacks the target(s) – that the important question arises as to whether human control in the first two stages is sufficient to overcome minimal or no human control at this last stage, from a legal, ethical and military-operational standpoint (see also section 5 below). The assessment of whether an autonomous weapon system can be used in compliance with IHL may depend on the specific technical characteristics and performance of the weapon system and the intended and expected circumstances of its use. Certain technical characteristics and their interaction with different operational parameters could significantly affect this assessment, including:  The task the weapon system carries out;  The type of target the weapon system attacks;  The environment in which the weapon system operates;  The movement of weapon system in space;  The time-frame of operation of the weapon system;  The adaptability of the weapon system, i.e. its ability to adapt its behaviour to changes in its environment, to determine its own functions and to set its own goals;  Degree of reliability of the weapon system, i.e. robustness to failures and vulnerability to malfunction or hacking; and  Potential for human supervision and intervention to deactivate the weapon system. The combination of these technical characteristics and performance of the weapon system with the operational parameters of its use are critical to determining the foreseeable effects of the weapon – in other words, the predictability of the outcomes of using the weapon – and therefore in determining whether it can be used in conformity with IHL rules. 4 Indeed, deploying a weapon system whose effects are wholly or partially unpredictable would create a significant risk that IHL will not be respected. The risks may be too high to allow use of the weapon, or else mitigating the risks may require limiting or even obviating the weapons’ autonomy. In this respect, the last factor in the list above – human intervention – could be considered as a risk mitigation factor. The level of risk resulting from a decrease in predictability of the weapon system may be influenced by a number of factors, such as the environment in which the weapon is used. Predicting the outcome of using autonomous weapon systems may become increasingly difficult as the weapon systems become more complex or are given more freedom of action in their operations. For example, the legal assessment of an autonomous weapon system that carries out a single task against a limited type of target in a simple (uncluttered) environment, and that is stationary and limited in the duration of its operation (e.g. some existing missile and rocket defence systems) may conclude that there is an acceptable level of predictability, allowing for responsibility and accountability of the human operator. However, the conclusion may be very different regarding an autonomous weapon system that carries out multiple tasks (or is adaptable) against different types of targets in a complex (cluttered) environment, and that is mobile over a wide area and/or operating for a long duration. For the purposes of an assessment under IHL there is no legal distinction between an offensive attack and a defensive one, as they both constitute attacks under the law. Nevertheless, the distinction between defensive and offensive weapon systems may be of greater relevance from a militaryoperational or ethical perspective. The obligation to carry out legal reviews of new weapons under article 36 of Additional Protocol I to the Geneva Conventions is important to ensure that a State's armed forces are capable of conducting hostilities in accordance with its international obligations. The above challenges for IHL compliance will need to be carefully considered by States when carrying out legal reviews of any autonomous weapon system they develop or acquire. As with all weapons, the lawfulness of a weapon with autonomy in its critical functions depends on its specific characteristics, and whether, given those characteristics, it can be employed in conformity with the rules of IHL in all of the circumstances in which it is intended and expected to be used. The ability to carry out such a review entails fully understanding the weapon’s capabilities and foreseeing its effects, notably through testing. Yet foreseeing such effects may become increasingly difficult if autonomous weapon systems were to become more complex or to be given more freedom of action in their operations, and therefore become less predictable. Questions arise as to how IHL’s “targeting rules” (e.g. the rules of proportionality and precautions in attack) are considered in reviewing weapons. Where it is the weapon itself that takes on the targeting functions, the legal review would demand a very high level of confidence that the weapon is capable of carrying out those functions in compliance with IHL. An additional challenge for reviewing the legality of an autonomous weapon system is the absence of standard methods and protocols for testing and evaluation to assess the performance of these weapons, and the possible risks associated with their use. Questions arise regarding: How is the reliability (e.g. risk of malfunction or vulnerability to cyber-attack) and predictability of the weapon tested? What level of reliability and predictability are considered to be necessary? The legal review procedure faces these and other practical challenges to assess whether an autonomous weapon system will perform as anticipated in the intended or expected circumstances of use. Although there are different views on the adequacy of national legal reviews of new weapons for ensuring IHL compliance of autonomous weapon systems, especially given the low level of implementation among States, this mechanism remains a critical measure for States to ensure respect for IHL. In any case, efforts to strengthen national legal review processes should be seen as complementary and mutually reinforcing of CCW discussions at the international level. Some have raised concerns that use of autonomous weapon systems may lead to an “accountability gap” in case of violations of IHL. Others are of the view that no such gap would ever exist as there will always be a human involved in the decision to deploy the weapon to whom responsibility could be attributed. 5 Under IHL and international criminal law, the limits to control over, or the unpredictability of, an autonomous weapon system could make it difficult to find individuals involved in the programming and deployment of the weapon liable for serious violations of IHL. They may not have the knowledge or intent required for such a finding, owing to the fact that the machine can select and attack targets independently. Programmers might not have knowledge of the concrete situations in which at a later stage the weapon system might be deployed and in which IHL violations could occur. On the other hand, a programmer who intentionally programmes an autonomous weapon to commit war crimes would certainly be criminally liable. Likewise, a commander would be liable for deciding to use an autonomous weapon system in an unlawful manner, for example deploying in a populated area an anti-personnel autonomous weapon that is incapable of distinguishing civilians from combatants. In addition, a commander who knowingly decides to deploy an autonomous weapon whose performance and effects he/she cannot predict may be held criminally responsible for any serious violations of IHL that ensue, to the extent that his/her decision to deploy the weapon is deemed reckless under the circumstances. Overall, as long as there will be a human involved in the decision to deploy the weapon to whom responsibility could be attributed, there might not be an accountability gap. Under the law of State responsibility a State could be held liable for violations of IHL caused by the use of any autonomous weapon system. Indeed under general international law governing the responsibility of States, they would be held responsible for internationally wrongful acts, such as violations of IHL committed by their armed forces using autonomous weapon systems. A State would also be responsible if it were to use an autonomous weapon system that it has not, or has inadequately, been tested or reviewed prior to deployment. Autonomous weapon systems also raise ethical concerns that deserve careful consideration. The fundamental question at the heart of concerns, and irrespective of whether they can be used in compliance with IHL, is whether the principles of humanity and the dictates of public conscience would allow machines to make life-and-death decisions in armed conflict without human involvement. The debates of recent years among States, experts, civil society and the public have shown that there is a sense of deep discomfort with the idea of any weapon system that places the use of force beyond human control. The question remains, however, what degree of human control is required, and in which circumstances, in light of ethical considerations? Is it sufficient for a human being to program an autonomous weapon system according to certain parameters and then make the decision to deploy it in a particular context? Or is it necessary that a human being bring his or her judgment to bear also on each individual attack? If the weapon autonomously uses force against a human target, what ethical considerations would this entail? 5. Human control The notion of human control has become the overarching issue in the debates on autonomous weapon systems. There is broad agreement that human control over weapon systems and the use of force must be retained, although less clarity on whether this is for legal, ethical, military operational, and/or policy reasons, and what makes it “meaningful”, “appropriate” or “effective”. From the ICRC’s perspective, a focus on the role of the human in the targeting process and the humanmachine interface could provide a fruitful avenue for increasing understanding of concerns that may be raised by autonomous weapon systems, rather than a purely technical focus on the ‘level of autonomy’ of weapon systems. A certain level of human control over attacks is inherent in, and required to ensure compliance with, the IHL rules of distinction, proportionality and precautions in attack. Considering more closely what these requirements are could help determine the boundaries of what is acceptable under IHL with respect to autonomy in the critical functions of selecting and attacking targets. There are already a number of considerations that have been suggested, which provide avenues for future work to establish these requirements, including:  Predictability of the weapon system in its intended or expected circumstances of use;  Reliability of the weapon system in its intended or expected circumstances of use; 6  Human intervention in the functioning of the weapon system during its development, deployment and use;  Knowledge and accurate information about the functioning of the weapon system and the context of its intended or expected use; and  Accountability for the functioning of the weapon system following its use. Many of the technical characteristics and operational parameters that are relevant to assessing compliance with IHL (see section 4) are also important factors for determining the requisite human control over the use of force, as well as relevant for military-operational or ethical considerations. For example, human control over existing autonomous weapon systems (see section 2) is largely governed by technical and operational constraints on the functioning of the system (e.g. limited tasks and targets, limits in space and time, physical controls over the environment, and human supervision and ability to deactivate). The military have a clear interest in maintaining human control of weapon systems, both to ensure compliance with IHL and to ensure that the commander has control over a given military operation. Rules of engagement associated with particular weapon systems are a primary way in which operational control is exerted over weapon systems and the use of force, and are therefore an important element in ensuring human control over the weapon system and the use of force. Deeper consideration of the elements constituting human control and understanding human-machine interaction is needed to help determine the boundaries necessary to ensure that human control is maintained over weapon systems and the use of force. As outlined above, there are already concerns that autonomous weapon systems which could adapt or change their functioning, and those that ‘hunt’ for targets over wide areas could raise serious questions about human control and IHL compliance due to the lack of predictability on when and where the use of force and specific attacks would take place. 6. The way forward There are at least three broad approaches that States could take to address the legal and ethical questions raised by autonomous weapon systems. The first regards strengthening national mechanisms for legal review and implementation of IHL to ensure any new weapons, including autonomous weapon systems, can be used in compliance with IHL. The second is for States to develop a definition of “lethal autonomous weapon systems” in terms of the weapon systems that may be problematic from a legal and/or ethical perspective with a view to establishing specific limits on autonomy in weapon systems. The third approach is for States to develop the parameters of human control in light of the specific requirements under IHL and ethical considerations (principles of humanity and the dictates of public conscience), thereby establishing specific limits on autonomy in weapon systems. Both the second and third approaches recognise that international consideration is needed of the limits of autonomy in weapons systems to ensure legal compliance and ethical acceptability. Ultimately these two approaches might lead to the same end point in terms of identifying weapon systems requiring possible regulation or prohibition. From the ICRC’s perspective, the third approach focussing on human control and the human-machine interaction could be an effective way forward for the CCW, with efforts to strengthen national legal reviews to be pursued in parallel as a mutually reinforcing initiative. The framework of human control provides a useful baseline from which common understandings can be developed among States, and through which boundaries or limits on autonomy in weapon systems can be established. This is consistent with the broad agreement among States, experts and other stakeholders that there is a need to maintain human control over weapon systems and the use of force in view of legal obligations, military operational requirements, and ethical considerations. + + + CHECK AGAINST DELIVERY Convention on Certain Conventional Weapons (CCW) Meeting of Experts on Lethal Autonomous Weapons Systems (LAWS) 11-15 April 2016, Geneva Statement of the International Committee of the Red Cross (ICRC) 11 April 2016 The International Committee of the Red Cross (ICRC) is pleased to contribute its views to this third CCW Meeting of Experts on “Lethal Autonomous Weapon Systems”. Discussions at the previous two CCW meetings of experts, as well as expert meetings convened by the ICRC and other organizations, have underscored the significant legal, ethical and societal questions raised by weapon systems that can select and attack targets without human intervention. Although views on this complex subject continue to evolve, including those of the ICRC, discussions have indicated broad agreement that, for legal, ethical or military-operational reasons, human control over weapon systems and the use of force must be retained. The ICRC has called on States to set limits on autonomy in weapon systems to ensure they are used in accordance with international humanitarian law (IHL) and within the bounds of what is acceptable under the principles of humanity and the dictates of public conscience. As an additional contribution and further to the first ICRC meeting held in March 2014, the ICRC convened a second meeting of international experts (“Autonomous weapon systems: Implications of increasing autonomy in the critical functions of weapons”) on 15-16 March 2016. Representatives of 20 States together with individual experts and representatives of the United Nations and civil society organizations participated in the meeting. The aim of the meeting was to link the real-world development of autonomy in the critical functions of weapon systems more closely to consideration of the legal and ethical implications of, and potential policy approaches to, autonomous weapon systems. A summary report of the ICRC’s expert meeting will be published shortly. In the meantime, as a contribution to this CCW meeting, we are circulating a short paper highlighting the ICRC’s perspective on some of the key issues raised at our expert meeting. In particular, we would like to highlight the following points:  Definitions: The ICRC has proposed that “autonomous weapon systems” is an umbrella term encompassing any weapon system that has autonomy in the critical functions of selecting and attacking targets. We wish to stress that the purpose of this working definition is to promote better understanding of the issue and to help frame related discussions. The advantage of such a broad definition is that it enables consideration to be given to experience of existing weapons systems with autonomy in their critical functions and to lessons learned. This could facilitate the process of determining the boundaries of what is acceptable under IHL and the dictates of public conscience.  Existing weapons: Some weapon systems in use today can select and attack targets without human intervention. The ability to effectively control these weapons and the use of force seems to be closely linked to their predictability and reliability, as well as to strict operational constraints with respect to the task carried out, the targets attacked, the operational environment, the geographical space and time of operation, the scope to enable human oversight of the operation of the weapon system, and the human ability to deactivate it if need be. 2  Emerging technology: There is a likelihood that increasingly autonomous weapon systems could become less predictable, particularly in case of increased mobility, increased adaptability and/or increased interaction of multiple systems (as swarms). The loss of predictability regarding the outcomes of using an autonomous weapon may point to the loss of human control over that weapon’s operation, with human decision-making over the use of force being replaced by machine processes. Therefore, significant questions may arise regarding compliance of such a weapon with IHL and its acceptability under the dictates of public conscience.  Legal reviews: The ICRC welcomes the recognition by States of the importance of reviewing new weapons to ensure their compatibility with international law, and stresses that efforts to strengthen national legal review processes are complementary and mutually reinforcing of discussions at the international level, in particular in the CCW meetings of experts. However, the legal review faces certain practical challenges regarding the assessment of whether an autonomous weapon system will perform as anticipated in the intended or expected circumstances of use. Two particular questions are raised: How is the predictability and reliability of the weapon assessed? What level of predictability and reliability is considered necessary?  The principles of humanity and the dictates of public conscience: Debates among States, experts, civil society and the general public have shown that there is a sense of deep discomfort with the idea of any weapon system that places the use of force beyond human control. Given the extreme gravity of the consequences and regardless of whether or not such systems are lawful, humanity may very well insist that decisions to kill or to destroy must continue to be taken by humans.  Human control: The notion of human control is the overarching issue in this debate. Whether for legal, ethical or military-operational reasons, there is broad agreement on the need for human control over weapons and the use of force. However, it remains unclear whether human control at the stages of the development and the deployment of an autonomous weapon system is sufficient to overcome minimal or no human control at the stage of the weapon system’s operation – that is, when it independently selects and attacks targets. There is now a need to determine the kind and degree of human control over the operation of weapon systems that are deemed necessary to comply with legal obligations and to satisfy ethical and societal considerations.  The way forward: Since the need to maintain human control –- whether “meaningful”, “appropriate” or “effective” – over weapon systems and the use of force is consistent with legal obligations, military operational requirements and ethical considerations, the ICRC encourages States to use it as a framework for ongoing CCW discussions. Human control and consideration of humanmachine interaction may provide a useful baseline from which common understandings can be developed among States, and through which limits on autonomy in weapon systems can be agreed. The ICRC urges CCW States Parties to agree at the Review Conference in December that future work focus on determining where these limits on autonomy in weapon systems should lie. Weapon systems that operate with autonomy in their critical functions already exist and new advances are constantly being made. There is therefore a need for States – and indeed for all stakeholders – to address this issue with a sense of responsibility and urgency, so as to ensure that technological developments do not outpace our legal and ethical deliberations. We encourage delegations to read the paper we have circulated, which goes into greater detail on these points. We will be pleased to elaborate further on our views during the thematic sessions. Thank you. Convention on Certain Conventional Weapons (CCW) Meeting of Experts on Lethal Autonomous Weapons Systems (LAWS) 11-15 April 2016, Geneva Views of the International Committee of the Red Cross (ICRC) on autonomous weapon system 11 April 2016 Introduction Views on autonomous weapon systems, including those of the International Committee of the Red Cross (ICRC), continue to evolve as a better understanding is gained of current and potential technological capabilities, the military purpose of autonomy in weapon systems, and the resulting questions for compliance with international humanitarian law (IHL) and ethical acceptability. Expert discussions of the last three years in the framework of the Convention on Certain Conventional Weapons (CCW) and in meetings convened by the ICRC and other organisations have been crucial to enhancing this understanding. The ICRC held a second expert meeting on ‘Autonomous weapon systems: Implications of increasing autonomy in the critical functions of weapons’ from 15-16 March 2016 in Versoix, Switzerland.1 Representatives of 20 States together with individual experts and representations of the United Nations and civil society organisations participated in the meeting. The ICRC will soon publish a summary report of the meeting. In the meantime, as a contribution to ongoing discussions in the CCW, this paper highlights some of the key issues on autonomous weapon systems from the perspective of the ICRC, and in the light of discussions at its recent expert meeting. 1. Definitions The ICRC has defined autonomous weapon systems as: “Any weapon system with autonomy in its critical functions. That is, a weapon system that can select (i.e. search for or detect, identify, track, select) and attack (i.e. use force against, neutralize, damage or destroy) targets without human intervention.” The advantage of this broad definition, which encompasses some existing weapon systems, is that it enables real-world consideration of weapons technology to assess what may make certain existing weapon systems acceptable – legally and ethically – and which emerging technology developments may raise concerns under international humanitarian law (IHL) and under the principles of humanity and the dictates of the public conscience. Some prefer a narrow definition more closely linked to identifying the type of weapon systems of greatest concern. In addition, some narrow definitions distinguish between “highly automated” and “autonomous” weapons. However, many experts emphasise that there is not such a clear difference from a technical perspective, and the core legal and ethical questions remain the same. 1 For a report of the first meeting see: ICRC (2014) Autonomous weapon systems technical, military, legal and humanitarian aspects, Report of an Expert Meeting held 26-28 March 2014 (published November 2014), https://www.icrc.org/en/download/file/1707/4221-002-autonomous-weapons-systems-full-report.pdf 2 In the view of the ICRC it is useful to start with a broad scope since lessons from experiences with existing weapon systems with autonomy in their critical functions can inform current discussions on emerging technology. 2. Autonomy in existing weapons There are weapon systems in use today which can select and attack targets without human intervention. After activation by a human operator it is the weapon system, through its sensors and computer programming, which selects a target and launches an attack. One broad group are anti-material defensive weapons used to protect vehicles, facilities or areas from incoming attacks with missiles, rockets, mortars or other projectiles. These include missile and rocket defence weapons and vehicle “active protection” weapons. The ability to effectively control these weapons and the use of force seems to require certain operational constraints including: limits on the task carried out (i.e. a single function to defend against incoming projectiles); limits on the targets (i.e. primarily objects and vehicles); controls over the operational environment (e.g. limitations on the geographical area and time frame of autonomous operation); and procedures for human intervention to deactivate the weapon, i.e. to cease its operation. Some offensive weapon systems, including certain missiles and torpedoes also have a level of autonomy in selecting and attacking targets after launch. Many of these weapons are fired into a particular target area, after which on-board sensors and programming take over to autonomously select and attack a specific target object or person within that area. Some have more freedom of action in time and/or space, and therefore greater autonomy. These include, in particular, loitering munitions that search for targets over a wide geographical area for long time periods, and encapsulated torpedo weapons that remain stationary underwater over long time periods but can carry out attacks autonomously. The trend in the development of missiles appears to be increasing autonomy with respect to movement in time and space. Indications are that future developments could also include increasing adaptability of these weapon systems to their environment. The ability to effectively control these weapons and the use of force may depend on a number of factors (as with defensive systems discussed above) including: limits on the task carried out; the ability of the system to discriminate targets; controls over the operational environment, such as limitations in time and space; and the ability for humans to communicate with the weapon system, for example to deactivate it. The latter is particularly difficult for underwater systems. There are other anti-personnel weapons that may be capable of autonomously selecting and attacking targets, such as so called “sentry” weapons used to defend facilities and borders. However, the systems in use today apparently remain under remote control for initiation of attacks. In sum, autonomy for selecting and attacking targets in existing weapon systems is limited by the operational parameters described above. Moreover, the technical characteristics and performance of existing weapon systems, combined with the operational parameters of their use, provide a certain degree of predictability of the outcomes of using these weapon systems. This predictability may be lost as autonomous weapon systems are used for more complex tasks or deployed in more dynamic environments than has been the case until now. 3. Emerging technology and future autonomous weapons Although it is difficult to foresee the future development of autonomous weapon systems, it is clear there are a number of military drivers for increased autonomy, including enabling: increased mobility of robotic/unmanned weapon systems or platforms; operation of these systems in “communications denied” environments; shorter decision-making times between identifying and attacking a target; increased performance over remotely operated systems; and operation of increased numbers of robotic/unmanned weapon systems by fewer operators. Interest in autonomous weapon systems could also develop in different ways among non-State armed groups. The general trend in civilian robotics is towards supervised autonomy, where robotic systems are increasingly autonomous while human operators retain oversight and often the ability to intervene. 3 The degree of autonomy in a robotic system is related to the level of human intervention in its operation, both in terms of the degree of human intervention and the stages at which the intervention is made. Even so, machines can and do effectively take decisions that have been delegated to them by humans through their computer programming, and without the need to be “conscious” or to have human-like levels of intelligence. There are a number of developments that might make increasingly autonomous weapon systems become less predictable. These include: increased mobility, meaning the weapon system would encounter more varied environments over greater time periods; increased adaptability, such as systems that set their own goals or change their functioning in response to the environment (e.g. a system that defends itself against an attack) or even incorporate learning algorithms; and increased interaction of multiple weapon systems in self-organising swarms. In addition to decreasing the predictability of the weapon system, these developments could raise related problems for the validity of testing to ensure reliability. For example, autonomous weapon systems that could set their own goals, or even “learn” and adapt their functioning, would by their nature be unpredictable. Highly mobile autonomous weapon systems would also create problems of predictability with respect to the target of specific attacks, especially if a system moved over a wide area or carried out multiple attacks. 4. Legal and ethical implications of increasing autonomy in weapon systems It is clear that IHL rules on the conduct of hostilities are addressed to the parties to an armed conflict, more specifically to the human combatants and fighters, who are responsible for respecting them, and will be held accountable for violations. These obligations cannot be transferred to a machine. Still, in practical terms, the question remains, what limits are needed on autonomy in weapon systems to ensure compliance with IHL. Control exercised by human beings can take various forms and operate at different stages of the “life cycle” of an autonomous weapon system, including: 1) the development of the weapon system, including its programming; 2) the deployment and use of the weapon system, including the decision by the commander or operator to use or activate the weapon system; and 3) the operation of the weapon system during which it selects and attacks targets. It is clear that human control is exerted in the development and deployment stages of the weapon’s “life cycle”. It is in stage three, however, when the weapon is in operation – when it autonomously selects and attacks the target(s) – that the important question arises as to whether human control in the first two stages is sufficient to overcome minimal or no human control at this last stage, from a legal, ethical and military-operational standpoint (see also section 5 below). The assessment of whether an autonomous weapon system can be used in compliance with IHL may depend on the specific technical characteristics and performance of the weapon system and the intended and expected circumstances of its use. Certain technical characteristics and their interaction with different operational parameters could significantly affect this assessment, including:  The task the weapon system carries out;  The type of target the weapon system attacks;  The environment in which the weapon system operates;  The movement of weapon system in space;  The time-frame of operation of the weapon system;  The adaptability of the weapon system, i.e. its ability to adapt its behaviour to changes in its environment, to determine its own functions and to set its own goals;  Degree of reliability of the weapon system, i.e. robustness to failures and vulnerability to malfunction or hacking; and  Potential for human supervision and intervention to deactivate the weapon system. The combination of these technical characteristics and performance of the weapon system with the operational parameters of its use are critical to determining the foreseeable effects of the weapon – in other words, the predictability of the outcomes of using the weapon – and therefore in determining whether it can be used in conformity with IHL rules. 4 Indeed, deploying a weapon system whose effects are wholly or partially unpredictable would create a significant risk that IHL will not be respected. The risks may be too high to allow use of the weapon, or else mitigating the risks may require limiting or even obviating the weapons’ autonomy. In this respect, the last factor in the list above – human intervention – could be considered as a risk mitigation factor. The level of risk resulting from a decrease in predictability of the weapon system may be influenced by a number of factors, such as the environment in which the weapon is used. Predicting the outcome of using autonomous weapon systems may become increasingly difficult as the weapon systems become more complex or are given more freedom of action in their operations. For example, the legal assessment of an autonomous weapon system that carries out a single task against a limited type of target in a simple (uncluttered) environment, and that is stationary and limited in the duration of its operation (e.g. some existing missile and rocket defence systems) may conclude that there is an acceptable level of predictability, allowing for responsibility and accountability of the human operator. However, the conclusion may be very different regarding an autonomous weapon system that carries out multiple tasks (or is adaptable) against different types of targets in a complex (cluttered) environment, and that is mobile over a wide area and/or operating for a long duration. For the purposes of an assessment under IHL there is no legal distinction between an offensive attack and a defensive one, as they both constitute attacks under the law. Nevertheless, the distinction between defensive and offensive weapon systems may be of greater relevance from a militaryoperational or ethical perspective. The obligation to carry out legal reviews of new weapons under article 36 of Additional Protocol I to the Geneva Conventions is important to ensure that a State's armed forces are capable of conducting hostilities in accordance with its international obligations. The above challenges for IHL compliance will need to be carefully considered by States when carrying out legal reviews of any autonomous weapon system they develop or acquire. As with all weapons, the lawfulness of a weapon with autonomy in its critical functions depends on its specific characteristics, and whether, given those characteristics, it can be employed in conformity with the rules of IHL in all of the circumstances in which it is intended and expected to be used. The ability to carry out such a review entails fully understanding the weapon’s capabilities and foreseeing its effects, notably through testing. Yet foreseeing such effects may become increasingly difficult if autonomous weapon systems were to become more complex or to be given more freedom of action in their operations, and therefore become less predictable. Questions arise as to how IHL’s “targeting rules” (e.g. the rules of proportionality and precautions in attack) are considered in reviewing weapons. Where it is the weapon itself that takes on the targeting functions, the legal review would demand a very high level of confidence that the weapon is capable of carrying out those functions in compliance with IHL. An additional challenge for reviewing the legality of an autonomous weapon system is the absence of standard methods and protocols for testing and evaluation to assess the performance of these weapons, and the possible risks associated with their use. Questions arise regarding: How is the reliability (e.g. risk of malfunction or vulnerability to cyber-attack) and predictability of the weapon tested? What level of reliability and predictability are considered to be necessary? The legal review procedure faces these and other practical challenges to assess whether an autonomous weapon system will perform as anticipated in the intended or expected circumstances of use. Although there are different views on the adequacy of national legal reviews of new weapons for ensuring IHL compliance of autonomous weapon systems, especially given the low level of implementation among States, this mechanism remains a critical measure for States to ensure respect for IHL. In any case, efforts to strengthen national legal review processes should be seen as complementary and mutually reinforcing of CCW discussions at the international level. Some have raised concerns that use of autonomous weapon systems may lead to an “accountability gap” in case of violations of IHL. Others are of the view that no such gap would ever exist as there will always be a human involved in the decision to deploy the weapon to whom responsibility could be attributed. 5 Under IHL and international criminal law, the limits to control over, or the unpredictability of, an autonomous weapon system could make it difficult to find individuals involved in the programming and deployment of the weapon liable for serious violations of IHL. They may not have the knowledge or intent required for such a finding, owing to the fact that the machine can select and attack targets independently. Programmers might not have knowledge of the concrete situations in which at a later stage the weapon system might be deployed and in which IHL violations could occur. On the other hand, a programmer who intentionally programmes an autonomous weapon to commit war crimes would certainly be criminally liable. Likewise, a commander would be liable for deciding to use an autonomous weapon system in an unlawful manner, for example deploying in a populated area an anti-personnel autonomous weapon that is incapable of distinguishing civilians from combatants. In addition, a commander who knowingly decides to deploy an autonomous weapon whose performance and effects he/she cannot predict may be held criminally responsible for any serious violations of IHL that ensue, to the extent that his/her decision to deploy the weapon is deemed reckless under the circumstances. Overall, as long as there will be a human involved in the decision to deploy the weapon to whom responsibility could be attributed, there might not be an accountability gap. Under the law of State responsibility a State could be held liable for violations of IHL caused by the use of any autonomous weapon system. Indeed under general international law governing the responsibility of States, they would be held responsible for internationally wrongful acts, such as violations of IHL committed by their armed forces using autonomous weapon systems. A State would also be responsible if it were to use an autonomous weapon system that it has not, or has inadequately, been tested or reviewed prior to deployment. Autonomous weapon systems also raise ethical concerns that deserve careful consideration. The fundamental question at the heart of concerns, and irrespective of whether they can be used in compliance with IHL, is whether the principles of humanity and the dictates of public conscience would allow machines to make life-and-death decisions in armed conflict without human involvement. The debates of recent years among States, experts, civil society and the public have shown that there is a sense of deep discomfort with the idea of any weapon system that places the use of force beyond human control. The question remains, however, what degree of human control is required, and in which circumstances, in light of ethical considerations? Is it sufficient for a human being to program an autonomous weapon system according to certain parameters and then make the decision to deploy it in a particular context? Or is it necessary that a human being bring his or her judgment to bear also on each individual attack? If the weapon autonomously uses force against a human target, what ethical considerations would this entail? 5. Human control The notion of human control has become the overarching issue in the debates on autonomous weapon systems. There is broad agreement that human control over weapon systems and the use of force must be retained, although less clarity on whether this is for legal, ethical, military operational, and/or policy reasons, and what makes it “meaningful”, “appropriate” or “effective”. From the ICRC’s perspective, a focus on the role of the human in the targeting process and the humanmachine interface could provide a fruitful avenue for increasing understanding of concerns that may be raised by autonomous weapon systems, rather than a purely technical focus on the ‘level of autonomy’ of weapon systems. A certain level of human control over attacks is inherent in, and required to ensure compliance with, the IHL rules of distinction, proportionality and precautions in attack. Considering more closely what these requirements are could help determine the boundaries of what is acceptable under IHL with respect to autonomy in the critical functions of selecting and attacking targets. There are already a number of considerations that have been suggested, which provide avenues for future work to establish these requirements, including:  Predictability of the weapon system in its intended or expected circumstances of use;  Reliability of the weapon system in its intended or expected circumstances of use; 6  Human intervention in the functioning of the weapon system during its development, deployment and use;  Knowledge and accurate information about the functioning of the weapon system and the context of its intended or expected use; and  Accountability for the functioning of the weapon system following its use. Many of the technical characteristics and operational parameters that are relevant to assessing compliance with IHL (see section 4) are also important factors for determining the requisite human control over the use of force, as well as relevant for military-operational or ethical considerations. For example, human control over existing autonomous weapon systems (see section 2) is largely governed by technical and operational constraints on the functioning of the system (e.g. limited tasks and targets, limits in space and time, physical controls over the environment, and human supervision and ability to deactivate). The military have a clear interest in maintaining human control of weapon systems, both to ensure compliance with IHL and to ensure that the commander has control over a given military operation. Rules of engagement associated with particular weapon systems are a primary way in which operational control is exerted over weapon systems and the use of force, and are therefore an important element in ensuring human control over the weapon system and the use of force. Deeper consideration of the elements constituting human control and understanding human-machine interaction is needed to help determine the boundaries necessary to ensure that human control is maintained over weapon systems and the use of force. As outlined above, there are already concerns that autonomous weapon systems which could adapt or change their functioning, and those that ‘hunt’ for targets over wide areas could raise serious questions about human control and IHL compliance due to the lack of predictability on when and where the use of force and specific attacks would take place. 6. The way forward There are at least three broad approaches that States could take to address the legal and ethical questions raised by autonomous weapon systems. The first regards strengthening national mechanisms for legal review and implementation of IHL to ensure any new weapons, including autonomous weapon systems, can be used in compliance with IHL. The second is for States to develop a definition of “lethal autonomous weapon systems” in terms of the weapon systems that may be problematic from a legal and/or ethical perspective with a view to establishing specific limits on autonomy in weapon systems. The third approach is for States to develop the parameters of human control in light of the specific requirements under IHL and ethical considerations (principles of humanity and the dictates of public conscience), thereby establishing specific limits on autonomy in weapon systems. Both the second and third approaches recognise that international consideration is needed of the limits of autonomy in weapons systems to ensure legal compliance and ethical acceptability. Ultimately these two approaches might lead to the same end point in terms of identifying weapon systems requiring possible regulation or prohibition. From the ICRC’s perspective, the third approach focussing on human control and the human-machine interaction could be an effective way forward for the CCW, with efforts to strengthen national legal reviews to be pursued in parallel as a mutually reinforcing initiative. The framework of human control provides a useful baseline from which common understandings can be developed among States, and through which boundaries or limits on autonomy in weapon systems can be established. This is consistent with the broad agreement among States, experts and other stakeholders that there is a need to maintain human control over weapon systems and the use of force in view of legal obligations, military operational requirements, and ethical considerations. + + + Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 13–17 November 2017, Geneva Statement of the International Committee of the Red Cross (ICRC) The International Committee of the Red Cross (ICRC) welcomes this first meeting of the Group of Governmental Experts on “Lethal Autonomous Weapons Systems”. With the High Contracting Parties to the Convention on Certain Conventional Weapons moving on to more formal discussions there is now an expectation that States will identify and build on common ground, such as the broad agreement that human control must be retained over weapon systems and the use of force. The ICRC’s view is that States must now work to establish limits on autonomy in weapon systems to ensure compliance with international law and to satisfy ethical concerns. In determining these limits, the ICRC is convinced that the focus must remain on obligations and responsibilities of humans in decisions to use force. For this reason, it has proposed that States assess the type and degree of human control required in the use of autonomous weapon systems – broadly defined as weapons with autonomy in their critical functions of selecting and attacking targets – to ensure compliance with international law, and acceptability under the principles of humanity and the dictates of the public conscience From the perspective of international humanitarian law, it is clear that the rules on the conduct of hostilities are addressed to those who plan, decide upon, and carry out an attack. These rules, which apply to all attacks regardless of the means or methods employed, give rise to obligations for human combatants, who are responsible for respecting them. These legal obligations, and accountability for them, cannot be transferred to a machine, a computer program, or a weapon system. In the ICRC’s view, compliance with these legal obligations would require that combatants retain a minimum level of human control over the use of weapon systems to carry out attacks in armed conflict. An examination of the way in which human control can be exerted over autonomous weapon systems (as broadly defined by the ICRC) points to the following key elements of human control to ensure legal compliance: predictability; human supervision and ability to intervene; and various operational restrictions, including on tasks, types of targets, the operating environment, time frame of operation, and scope of movement. Meaningful, effective or appropriate human control also requires that the operator have sufficient information on and understanding of the weapon system and operating environment, and the interaction between them. 2 These elements of human control are needed to link the decisions of the human commander or operator – which must comply with international humanitarian law and other applicable provisions of international law – to the outcome of a specific attack using the weapon system. The need for some degree of human control indicates that there will be limits to lawful levels of autonomy in weapon systems under international humanitarian law. The need for human control also raises concerns about the technical aspects of weapon system design that may lead to unpredictability. In particular, the use of artificial intelligence (AI) machine-learning algorithms in targeting would raise fundamental legal concerns to the extent that their functioning and outcomes would be inherently unpredictable. In recent months the ICRC has further evaluated the ethical aspects of autonomous weapon systems; it convened a small round-table of experts in August 2017 and has built on the outcomes of the expert meetings held with States in 2014 and 2016. The principles of humanity and the dictates of the public conscience provide moral guidance for these discussions, and support the ICRC’s call to set limits on autonomy in weapon systems. Perhaps the most significant ethical issues raised by autonomous weapon systems are those which transcend both the context of their deployment – whether in armed conflict or in peacetime – and the technology involved – whether simple or sophisticated. These concerns focus on the loss of human agency and responsibility in decisions to kill and destroy. Since moral responsibility for decisions to kill and destroy cannot be delegated to machines, meaningful, effective or appropriate human control – from an ethical point of view – would be the type and degree of control that preserves human agency and responsibility in these decisions. With increasing autonomy in weapon systems, a point may be reached where humans are so far removed in time and space from the acts of selecting and attacking targets that human decision-making is effectively substituted with computer-controlled processes, and life-anddeath decisions ceded to machines. This raises profound ethical questions about the role and responsibility of humans in the use of force and the taking of human life, which go beyond questions of compliance with international humanitarian law. With respect to the public conscience, there is a sense of deep discomfort with the idea of any weapon system that places the use of force beyond human control. With these legal and ethical questions in mind, and conscious of rapid technological developments in the fields of robotics and artificial intelligence and their current application in weapon systems, the ICRC urges all States present at this Group of Governmental Experts to work on establishing limits on autonomy in weapon systems, so that humans remain fully responsible for decisions to use force. Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 27-31 August 2018, Geneva Statement of the International Committee of the Red Cross (ICRC) under agenda item 6(b) Thank you, Mr Chair. The ICRC is encouraged to hear the emphasis and general agreement among delegations that human control must be maintained over weapon systems and the use of force. The ICRC would like to take this opportunity to recall several points we raised in our statements to the April Group of Governmental Experts on the legal and ethical basis for human control. Further we would like to offer some insights from a small meeting of technical experts we convened in June 2018 on the technical aspects of human control. Firstly, on the legal basis for human control, the ICRC is clear that it is humans who apply international law and are obliged to respect it. International humanitarian law rules on the conduct of hostilities require that those who plan, decide upon and carry out attacks make certain judgements when launching an attack. It follows that human combatants will need to retain a level of control over weapon systems and the use of force so that they can make context-specific legal judgements – of distinction, proportionality and precautions – in specific attacks. Legal concerns will arise where the design and/or the use of a weapon system with autonomy in its critical functions prevents the commander or operator from making the necessary judgements required by international humanitarian law. Secondly, in the ICRC’s view, ethical considerations also demand human control over weapon systems and the use of force. From an ethical perspective, (minimum/sufficient/effective/meaningful /appropriate) human control would be the type and degree that preserves human agency and upholds moral responsibility in decisions to use force. Mr Chair, In June 2018, ICRC convened a small meeting of independent experts in civilian artificial intelligence (AI), robotics and autonomous systems to better understand the technical aspects of human control. A report will be published in due course but two broad themes emerged: The first theme was that all autonomous robotic systems, which operate based on interaction with their environment, raise questions about human control and predictability. The greater the complexity of the environment and complexity of the task, the greater the need for human control and the less tolerance of autonomy. Humans can exert some control over autonomous systems through “human on the loop” interaction with them. However, this is not a panacea due to human-machine interaction problems, such as automation bias, over-trust in the system, or lack of operator awareness of the system state at the time of intervention. ICRC statement, CCW GGE on “LAWS”, 27-31 August 2018. 2 Further, quantifying the level of predictability (and reliability) required to ensure (minimum/sufficient /effective/meaningful/appropriate) human control, or judgement, is very difficult. Testing also raises unique challenges since it is not possible to test all possible environmental inputs to an autonomous system. However, setting boundaries – or operational constraints – in the operation of an autonomous robotic system – for example, on the task, time-frame of operation, scope of movement over an area, and operating environment – can contribute to increasing predictability. The second key theme that emerged from discussions was that AI - and especially machine learning – algorithms bring a new dimension of unpredictability to autonomous robotic systems. This is due both to the lack of transparency (“black-box” nature) in how the algorithms function, and the changing of their functioning over time. Not only are such algorithms unpredictable, but they can also introduce bias, whether by design or through bias in the data used to “train” (develop) the algorithms. This has reinforced the ICRC’s concerns about the unpredictability of machine learning algorithms, and the consequences if applied to weapon systems and the critical functions of selecting and attacking targets. Mr Chair, In closing, we would like to draw attention to the comments of the ICRC’s President published today, which stressed the importance of retaining human control over weapon systems and the use of force. He notes that, in some ways, recognising the need for human control is the easy part. The more difficult question is: how much and what type? It is this issue where the ICRC believes this Group of Governmental Experts should focus its work over the coming days, weeks and months. ICRAC Statement on the human control of weapons systems Mr Chairperson, We have been very pleased with this mornings session as states begin to contemplate a move towards policies on the human control of weapons systems. On a pedantic note that we cannot talk about the meaningful human control of LAWS as that would make them no longer and autonomous weapon. In the view of ICRAC, the control of weapons systems is more nuanced than can be captured by terms such as in-the-loop, on-the-loop, the broader loop, looping-the loop, human oversight, and appropriate human judgement. In this way we agree strongly with the statement made by Brazil and several others in this session who believe that the devil is in the detail. For human control to be meaningful we need to examine how humans interact with machines and understand the types of human-machine biases that can occur in the selection of legitimate targets. Lessons should be learned from 30 years of research on human supervisory control of machinery and more than 100 years of research on the psychology of human reasoning. This combination of this work can help us to design human-machine interfaces that allow weapons to be controlled in a manner that is fully compliant with international law and the principle of humanity. First, there should be a focus on what the human operator MUST do in the targeting cycle. This is control by use which is governed by targeting rules under International Humanitarian Law and International Human Rights Law. Further, international law rules that apply after the use of weapons – such as those that relate to human responsibility – must be satisfied. Second, the design of weapon systems must render them INCAPABLE of operating without meaningful human control. This is control by design, which is governed by international weapons law. In terms of international weapons law, if the weapon system, by its design, is incapable of being sufficiently controlled in terms of the law, then such a weapon is illegal per se. Ideally the following three conditions should be followed for the control of weapons systems: 1. a human commander (or operator) will have full contextual and situational awareness of the target area for each and every attack and is able to perceive and react to any change or unanticipated situations that may have arisen since planning the attack. 2. there will be active cognitive participation in every attack with sufficient time for deliberation on the nature of any target, its significance in terms of the necessity and appropriateness of attack, and likely incidental and possible accidental effects of the attack and 3. there will be a means for the rapid suspension or abortion of every attack. Thank you Mr Chairperson CHECK AGAINST DELIVERY Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 9–13 April 2018, Geneva Statement of the International Committee of the Red Cross (ICRC) Possible options for addressing the humanitarian and international security challenges posed by emerging technologies in the area of lethal autonomous weapons systems in the context of the objectives and purposes of the Convention without prejudging policy outcomes and taking into account past, present and future proposals Thank you, Mr Chair, and for ably guiding our work this week. The ICRC is pleased with how discussions have progressed, and appreciate the constructive engagement of delegations, though clearly much work remains to be done if common understandings are to be reached on key issues by the GGE’s next meeting in August. Discussions on “policy options” should be grounded on three truisms: Firstly, all new technologies of warfare must fit with existing law, not the other way around. Any weapon with autonomy in its critical functions of selecting and attacking targets must be capable of being used, and must be used, in accordance with international humanitarian law (IHL). This is undisputed. However, the unique implications of these weapons – namely the risk of loss of human control, and loss of human agency and intent, in targeting decisions -­-­ raise unique challenges for legal compliance and for humanity, which in turn raise the question of whether new internationally agreed policies, standards or rules are needed. Secondly, “policy options” must be informed by reality – the reality of how autonomy in is being employed in weapon systems today and may be applied in the foreseeable future, and the evidence of the weapons’ capabilities and limitations, their relative risks and benefits. Experience with existing weapon systems with autonomy in their critical functions can provide insights here. Thirdly, the further weaponization of autonomy is not inevitable, it is a choice. And in making that choice, we must put humans – and their legal obligations and moral responsibilities – first, not technology. Mr Chair It is ICRC’s view is that human control must be retained over weapon systems and the use of force and, therefore, the way forward should be determine the type and degree of human control necessary – in the use of weapon systems with autonomy in their critical functions – to ensure compliance with international (humanitarian) law, and ethical acceptability. With general agreement that “meaningful”, “effective” or “appropriate” human control must be retained, this approach will enable States to 2 • establish meaningful, and internationally agreed, limits on autonomy in weapon systems that address legal, ethical and humanitarian concerns, and • identify the specific characteristics of “autonomous weapon systems of concern”. The criteria for the minimum level of human control should be driven by the necessity to preserve human judgement and responsibility in targeting decisions, where predictability and human supervision are important factors. This may be seen as a positive obligation to ensure human control over weapon systems and the use of force, in line with legal obligations and ethical considerations. And, at this stage, the ICRC welcomes efforts to reach common understandings on the type and degree of human control required. Thank you. ICRC commentary on the ‘Guiding Principles’ of the CCW GGE on ‘Lethal Autonomous Weapons Systems’ The ‘Guiding Principles’ agreed by the Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts (GGE) on ‘Emerging Technologies in the Area of Lethal Autonomous Weapons Systems’ provide a useful basis for orientating the future work of States towards agreeing an effective ‘normative and operational framework’ to address autonomous weapon systems.1 This commentary groups the Guiding Principles under three main themes that, in the view of the ICRC, deserve States’ focused attention. 1. International humanitarian law limits the development and use of autonomous weapon systems It was affirmed that international law, in particular the United Nations Charter and International Humanitarian Law (IHL) as well as relevant ethical perspectives, should guide the continued work of the Group. Noting the potential challenges posed by emerging technologies in the area of lethal autonomous weapons systems to IHL, the following were affirmed, without prejudice to the result of future discussions The ICRC welcomes States’ unequivocal affirmation that both international law and ethical perspectives should guide the work of the GGE. The development and use of autonomous weapon systems is limited by international law, in particular the general rules of IHL governing the choice of means and methods of warfare and the specific treaty and customary rules prohibiting or restricting certain weapons.2 Additional constraints may derive from ethical considerations, including from the principles of humanity and the dictates of public conscience.3 (a) International humanitarian law continues to apply fully to all weapons systems, including the potential development and use of lethal autonomous weapons systems IHL regulates the conduct of armed conflict and seeks to limit its effects. It protects people not taking part in hostilities (such as civilians) and those who are no longer doing so (such as wounded or surrendered combatants). During an armed conflict, IHL governs the use of weapons, means and methods of warfare in the conduct of hostilities, including autonomous weapon systems. Outside armed conflict, the use of weapons is primarily governed by international human rights law, which is applicable at all times. In the view of the ICRC, autonomous weapon systems raise challenges for compliance with IHL. The rules on the conduct of hostilities, notably the rules of distinction, proportionality and precautions in attack, already set limits on the use of autonomous weapon systems, although many legal questions require clarification, and ethical concerns may demand limits that go beyond those found in existing law.4 The key question is not whether IHL applies to autonomous weapon systems in armed conflict, but how IHL is applied, that is, how IHL rules are and should be interpreted and implemented in practice, and whether new legally binding rules, policy standards or best practices are needed.5 1 United Nations, Report of the 2019 session of the Group of Governmental Experts on Emerging Technologies in the Area of Lethal Autonomous Weapons Systems. CCW/GGE.1/2019/3, 25 September 2019, Annex IV. 2 ICRC, International Humanitarian Law and the Challenges of Contemporary Armed Conflicts. Report to the 33rd International Conference of the Red Cross and Red Crescent, October 2019, Section 2 B, pp. 29–31. 3 Convention on Prohibitions or Restrictions on the Use of Certain Conventional Weapons Which May Be Deemed to be Excessively Injurious or to have Indiscriminate Effects, as amended on 21 December 2001, Preamble; ICRC, Ethics and autonomous weapons systems: An ethical basis for human control? Working Paper for the CCW GGE on LAWS, Geneva, 9-13 April 2018, CCW/GGE.1/2018/WP.5, 29 March 2018. 4 ICRC, International Humanitarian Law and the Challenges of Contemporary Armed Conflicts, 2019, op. cit. 5 ICRC, States must address concerns raised by autonomous weapons, Statement to the Meeting of the High Contracting Parties to the CCW, 14 November 2019; ICRC, International Humanitarian Law and the Challenges of Contemporary Armed Conflicts, 2019, op. cit.; ICRC, Statement under agenda item 5e. CCW 2 (e) In accordance with States’ obligations under international law, in the study, development, acquisition, or adoption of a new weapon, means or method of warfare, determination must be made whether its employment would, in some or all circumstances, be prohibited by international law States Parties to 1977 Additional Protocol I to the Geneva Conventions have a legal obligation to conduct legal reviews of new weapons.6 In the ICRC’s view, the requirement to carry out legal reviews also flows from the obligation to ensure respect for IHL. Besides these legal requirements, all States have an interest in assessing the lawfulness of new weapons.7 Effective legal reviews are critical to ensuring that a State’s armed forces comply with IHL in light of rapid technological developments. However, in the view of the ICRC, they are not sufficient alone to address the concerns raised by autonomous weapon systems given the complex legal and ethical questions involved, which require common understandings at the international level.8 Implementation of legal reviews of autonomous weapon systems raises practical challenges and questions, especially given the difficulties in foreseeing the likely consequences of use of autonomous weapon systems.9 In conducting reviews, particular attention should be given to measures needed to ensure human control over weapons and the use of force. (h) Consideration should be given to the use of emerging technologies in the area of lethal autonomous weapons systems in upholding compliance with IHL and other applicable international legal obligations (f) When developing or acquiring new weapons systems based on emerging technologies in the area of lethal autonomous weapons systems, physical security, appropriate non-physical safeguards (including cyber-security against hacking or data spoofing), the risk of acquisition by terrorist groups and the risk of proliferation should be considered (g) Risk assessments and mitigation measures should be part of the design, development, testing and deployment cycle of emerging technologies in any weapons systems Under IHL, all parties to armed conflict have a legal obligation to respect and ensure respect for IHL. This entails a duty to ensure that all weapons, means and methods of warfare, including autonomous weapon systems, are capable of being used, and are in fact used, in compliance with IHL and with other applicable international legal obligations (Guiding Principle (h)). These obligations, as well as additional obligations for States Parties to specific treaties, also demand consideration in the transfer of weapons (Guiding Principle (f)). Risk assessments and mitigation measures during the design, development, testing and deployment of new weapons may be required to ensure compliance with these legal obligations (Guiding Principle (g)), including as part of obligations to conduct legal reviews of new weapons (Guiding Principle (e)). 2. Human control is central to the legal compliance and ethical acceptability of autonomous weapon systems (c) Human-machine interaction, which may take various forms and be implemented at various stages of the life cycle of a weapon, should ensure that the potential use of weapons systems based on emerging technologies in the area of lethal autonomous weapons systems is in compliance with applicable international law, in particular IHL. In determining the quality and extent of human-machine interaction, a range of factors should be considered including the operational context, and the characteristics and capabilities of the weapons system as a whole For the ICRC, Guiding Principle (c) – together with Guiding Principles (b) and (d) – reflect the main risks posed by autonomous weapon systems: loss of human control over weapons and the use of force; GGE on LAWS, Geneva, 25–29 March 2019; Boulanin, V., Davison, N., Goussac, N. and Peldán Carlsson, M., Limits on Autonomy in Weapon Systems, ICRC & SIPRI, June 2020, Chapter 4, Recommendations 3 & 4. 6 ICRC, A Guide to the Legal Review of New Weapons, Means and Methods of Warfare, January 2006. 7 ICRC, International Humanitarian Law and the Challenges of Contemporary Armed Conflicts, 2019, op. cit., Section 2.E, pp. 34-35. 8 ICRC, Statement under agenda item 5e, op. cit. 9 Ibid. 3 diffusion, or abdication, of human responsibility for the consequences of their use; and practical challenges in ensuring accountability for violations of international law that may result. Measures pertaining to human control, responsibility and accountability – including but not limited to measures concerning ‘human-machine interaction’ and implemented throughout weapon development and use – are critical to ensuring compliance with applicable international law, in particular IHL, as well as ethical acceptability.10 Based on humanitarian, legal, and ethical considerations, as well as military operational realities, a recent report co-published by the ICRC and SIPRI proposes a combination of three types of control measures on autonomous weapon systems needed to satisfy legal obligations and ethical considerations: 1) controls on weapon parameters; 2) controls on the environment of use; and 3) controls through human-machine interaction.11 These measures should be considered in the use of autonomous weapon systems, as well as in their study, research, development and acquisition (Guiding Principle (e)).12 These types of control measures can inform internationally agreed limits on autonomous weapon systems, whether in the form of new legally binding rules, policy standards or best practices:13 • Controls on weapon parameters can inform limits on types of autonomous weapon systems including the targets they are used against, as well as limits on their duration and geographical scope of operation, and requirements for deactivation and fail-safe mechanisms. • Controls on the environment can inform limits on the situations and locations in which autonomous weapon systems may be used, notably, in terms of the presence and density of civilians and civilian objects. • Controls through human-machine interaction can inform requirements for human supervision, and ability to intervene and deactivate autonomous weapon systems, and requirements for predictable and transparent functioning. (b) Human responsibility for decisions on the use of weapons systems must be retained since accountability cannot be transferred to machines. This should be considered across the entire life cycle of the weapons system (d) Accountability for developing, deploying and using any emerging weapons system in the framework of the CCW must be ensured in accordance with applicable international law, including through the operation of such systems within a responsible chain of human command and control Legal obligations under IHL rules on the conduct of hostilities must be fulfilled by those persons who plan, decide on, and carry out military operations. It is humans, not machines, that comply with and implement these rules, and it is humans who can be held accountable for violations. Whatever the machine, computer program, or weapon system used, individuals and parties to conflicts remain responsible for their effects.14 Nevertheless, the way in which autonomous weapon systems function – i.e. independently selecting and applying force to targets without human intervention -- raises questions about the about the practical possibility of holding parties to conflict and individuals legally accountable for the consequences of their use, including for violations of IHL.15 10 ICRC, International Humanitarian Law and the Challenges of Contemporary Armed Conflicts, 2019, op. cit.; ICRC, Statement under agenda item 5a. CCW GGE on LAWS, Geneva, 25–29 March 2019; ICRC, Statement under agenda item 5c, CCW GGE on LAWS, Geneva, 25–29 March 2019; ICRC, The Element of Human Control, Working Paper for the Meeting of High Contracting Parties to the CCW 21-23 November 2018, Geneva, CCW/MSP/2018/WP.3, 19 November 2018; ICRC, Autonomy, artificial intelligence and robotics: Technical aspects of human control. Working Paper for the CCW GGE on LAWS, Geneva, 20-21 August 2019, CCW/GGE.1/2019/WP.7, 20 August 2019; ICRC, Ethics and autonomous weapons systems: An ethical basis for human control?, op. cit.; ICRC, Autonomous Weapon Systems: Implications of Increasing Autonomy in the Critical Functions of Weapons, August 2016; ICRC, Autonomous Weapon Systems: Technical, Military, Legal and Humanitarian Aspects, March & November 2014. 11 Boulanin, V., Davison, N., Goussac, N. and Peldán Carlsson, M., Limits on Autonomy in Weapon Systems, op. cit. Chapter 4, Recommendation 1. 12 Ibid., Chapter 4, Recommendation 5. 13 Ibid., Chapter 4, Recommendation 2. 14 ICRC, International Humanitarian Law and the Challenges of Contemporary Armed Conflicts, 2019, op. cit. 15 ICRC, International Humanitarian Law and the Challenges of Contemporary Armed Conflicts. Report to the 32nd International Conference of the Red Cross and Red Crescent, October 2015, Section VII I) ii), p. 46. 4 The rules on the conduct of hostilities – notably the rules of distinction, proportionality and precautions in attack – require complex assessments based on the circumstances prevailing at the time of the decision to attack, and during an attack. Commanders or operators must retain a level of human control over weapon systems sufficient to allow them to make context-specific judgments to apply the law in carrying out attacks.16 From an ethical perspective, human control is required to preserve human agency and uphold moral responsibility in decisions to use force. This requires a sufficiently direct and close connection to be maintained between the human intent of the user and the eventual consequences of the operation of the weapon system in a specific attack. Weapons, as inanimate objects, do not have moral agency and nor can they meaningfully be held responsible or accountable.17 Measures aimed at ensuring human control, responsibility and accountability are outlined under Guiding Principle (c). 3. Towards an effective multilateral response to autonomous weapon systems (k) The CCW offers an appropriate framework for dealing with the issue of emerging technologies in the area of lethal autonomous weapons systems within the context of the objectives and purposes of the Convention, which seeks to strike a balance between military necessity and humanitarian considerations In the view of the ICRC, the CCW – and the GGE on ‘Emerging Technologies in the Area of Lethal Autonomous Weapons Systems’ – offers an appropriate framework to address the risks posed by autonomous weapon systems falling within its scope. This is without prejudice to consideration of such risks in other relevant fora. In light of humanitarian, legal and ethical concerns, the ICRC reiterates its call to States at the GGE to urgently agree international limits on autonomous weapon systems. Rapid technological advances and military-doctrinal developments in a number of States indicate that the window for preventive action is fast closing.18 As Guiding Principles (b), (c) and (d) imply, an effective policy response to the risks posed by autonomous weapon systems requires consideration of what ‘quality and extent’ of human control is necessary and how ‘human responsibility’ and ‘accountability’ are ensured. Measures aimed at ensuring human control, responsibility and accountability can inform international limits on autonomous weapon systems. (i) In crafting potential policy measures, emerging technologies in the area of lethal autonomous weapons systems should not be anthropomorphized Legal obligations and ethical responsibilities rest with humans. Weapons, as inanimate objects, do not hold such obligations or responsibilities, and it should not be implied that they do (see also Guiding Principle (b)). (j) Discussions and any potential policy measures taken within the context of the CCW should not hamper progress in or access to peaceful uses of intelligent autonomous technologies Even the strictest measures taken to address the concerns raised by autonomous weapon systems can be crafted so as not to hamper progress in or access to relevant technologies for peaceful purposes. For example, the CCW Protocol IV prohibition of blinding laser weapons has not hampered progress in laser technology, nor have the Biological Weapons Convention or the Chemical Weapons Convention hampered progress in the peaceful uses of biology and chemistry. Geneva, July 2020 16 ICRC, International Humanitarian Law and the Challenges of Contemporary Armed Conflicts, op. cit. 17 ICRC, Ethics and autonomous weapons systems: An ethical basis for human control?, op. cit.; Boulanin, V., Davison, N., Goussac, N. and Peldán Carlsson, M., Limits on Autonomy in Weapon Systems, op. cit., Chapter 2, Section III. 18 ICRC, States must address concerns raised by autonomous weapons, op. cit.; ICRC, Statement under agenda item 5e, op. cit. Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 21-25 September 2020, Geneva Statement of the International Committee of the Red Cross The International Committee of the Red Cross (ICRC) welcomes the efforts of the High Contracting Parties to the Convention on Certain Conventional Weapons, as well as the Chair of the Group of Governmental Experts on Lethal Autonomous Weapons Systems, Ljupčo Jivan Gjorgjinski, to make progress in their important work, under difficult circumstances. The ICRC also appreciates the valuable work done by the outgoing Chair, Ambassador Janis Karklins, in collecting and summarizing commentaries by the High Contracting Parties on the 11 points of agreement, the Guiding Principles adopted at the end of 2019. His paper on commonalities illustrates the substantive common ground shared by States and other stakeholders. Autonomous weapon systems, as the ICRC understands them, select and apply force to targets without human intervention. To varying degrees, the user of the weapon will know neither the specific target nor the exact timing and location of the attack that will result. This raises serious concerns from a humanitarian, legal and ethical perspective, in particular the risk of losing human control over the use of force. The ICRC’s own commentary on the Guiding Principles focused on three overarching issues: - International humanitarian law regulates and limits the development and use of autonomous weapon systems in armed conflict. - Human control is critical to ensuring compliance with applicable international law, including international humanitarian law, as well as ethical acceptability. Specific measures to ensure human control pertain to weapon parameters, the environment of use, and human-machine interaction. - Internationally agreed limits on autonomous weapon systems are needed, whether in the form of new, legally binding rules, policy standards or best practices, to respond to the pressing humanitarian, legal and ethical concerns raised during deliberations in Geneva over the past seven years. It is clear to the ICRC that it is necessary to establish limits on the types of autonomous weapons used and the situations in which they are used, and these limits can be informed by measures to ensure human control. It is encouraging that many national submissions and the joint submissions by groups of States stressed similar points, which testifies to a growing convergence of views on these substantive issues. In the ICRC’s analysis, there is agreement on: the need to maintain human control, involvement or judgement and the rationale behind that need, including reducing the risk of harm to civilians, upholding legal obligations and ethical principles, and ensuring human responsibility and accountability. There is also a convergence of views on the types of measures that can contribute to ensuring human control, involvement or judgement, which include limits on tasks and target sets or profiles, temporal and spatial restrictions on the operation of the weapon, as well as requirements relating to human situational awareness, and intervention and deactivation capacities. Significantly, in his paper on commonalities, the Chair finds that further work is required to “determine the type and extent of human involvement or control necessary to ensure compliance with applicable law, notably international humanitarian law, and respond to ethical concerns.” In this respect, the ICRC 2 would also like to draw attention to a joint ICRC-SIPRI report published in June 2020, Limits on Autonomy in Weapon Systems: Identifying Practical Elements of Human Control, which sets out five recommendations for a way forward. The ICRC is grateful to the governments of the Netherlands, Sweden and Switzerland for supporting this independent study. The ICRC looks forward to engaging in greater depth on these issues during the course of discussions this week. Geneva, 9 - 13 April 2018 Item 6 of the provisional agenda Other matters Ethics and autonomous weapon systems: An ethical basis for human control? Submitted by the International Committee of the Red Cross (ICRC) Executive Summary In the view of the International Committee of the Red Cross (ICRC), human control must be maintained over weapon systems and the use of force to ensure compliance with international law and to satisfy ethical concerns, and States must work urgently to establish limits on autonomy in weapon systems. In August 2017, the ICRC convened a round-table meeting with independent experts to explore the ethical issues raised by autonomous weapon systems and the ethical dimension of the requirement for human control. This report summarizes discussions and highlights the ICRC’s main conclusions. The fundamental ethical question is whether the principles of humanity and the dictates of the public conscience can allow human decision-making on the use of force to be effectively substituted with computer-controlled processes, and life-and-death decisions to be ceded to machines. It is clear that ethical decisions by States, and by society at large, have preceded and motivated the development of new international legal constraints in warfare, including constraints on weapons that cause unacceptable harm. In international humanitarian law, notions of humanity and public conscience are drawn from the Martens Clause. As a potential marker of the public conscience, opinion polls to date suggest a general opposition to autonomous weapon systems – with autonomy eliciting a stronger response than remotecontrolled systems. Ethical issues are at the heart of the debate about the acceptability of autonomous weapon systems. It is precisely anxiety about the loss of human control over weapon systems and the use of force that goes beyond questions of the compatibility of autonomous weapon systems with our laws to encompass fundamental questions of acceptability to our values. A prominent aspect of the ethical debate has been a focus on autonomous weapon systems that are designed to kill or injure humans, rather than those that destroy or damage objects, which are already employed to a limited extent. The primary ethical argument for autonomous weapon systems has been resultsoriented: that their potential precision and reliability might enable better respect for both international law and human ethical values, resulting in fewer adverse humanitarian consequences. As with other weapons, such characteristics would depend on both the designdependent effects and the way the weapons were used. A secondary argument is that they CCW/GGE.1/2018/WP.5 Group of Governmental Experts of the High Contracting Parties to the Convention on Prohibitions or Restrictions on the Use of Certain Conventional Weapons Which May Be Deemed to Be Excessively Injurious or to Have Indiscriminate Effects 29 March 2018 English only CCW/GGE.1/2018/WP.5 2 would help fulfil the duty of militaries to protect their own forces – a quality not unique to autonomous weapon systems. While there are concerns regarding the technical capacity of autonomous weapons systems to function within legal and ethical constraints, the enduring ethical arguments against these weapons are those that transcend context – whether during armed conflict or in peacetime – and transcend technology – whether simple or sophisticated. The importance of retaining human agency – and intent – in decisions to use force, is one of the central ethical arguments for limits on autonomy in weapon systems. Many take the view that decisions to kill, injure and destroy must not be delegated to machines, and that humans must be present in this decision-making process sufficiently to preserve a direct link between the intention of the human and the eventual operation of the weapon system. Closely linked are concerns about a loss of human dignity. In other words, it matters not just if a person is killed or injured but how they are killed or injured, including the process by which these decisions are made. It is argued that, if human agency is lacking to the extent that machines have effectively, and functionally, been delegated these decisions, then it undermines the human dignity of those combatants targeted, and of civilians that are put at risk as a consequence of legitimate attacks on military targets. The need for human agency is also linked to moral responsibility and accountability for decisions to use force. These are human responsibilities (both ethical and legal), which cannot be transferred to inanimate machines, or computer algorithms. Predictability and reliability in using an autonomous weapon system are ways of connecting human agency and intent to the eventual consequences of an attack. However, as weapons that self-initiate attacks, autonomous weapon systems all raise questions about predictability, owing to varying degrees of uncertainty as to exactly when, where and/or why a resulting attack will take place. The application of AI and machine learning to targeting functions raises fundamental questions of inherent unpredictability. Context also affects ethical assessments. Constraints on the time-frame of operation and scope of movement over an area are key factors, as are the task for which the weapon is used and the operating environment. However, perhaps the most important factor is the type of target, since core ethical concerns about human agency, human dignity and moral responsibility are most acute in relation to the notion of anti-personnel autonomous weapon systems that target humans directly. From the ICRC’s perspective, ethical considerations parallel the requirement for a minimum level of human control over weapon systems and the use of force to ensure legal compliance. From an ethical viewpoint, “meaningful”, “effective” or “appropriate” human control would be the type and degree of control that preserves human agency and upholds moral responsibility in decisions to use force. This requires a sufficiently direct and close connection to be maintained between the human intent of the user and the eventual consequences of the operation of the weapon system in a specific attack. Ethical and legal considerations may demand some similar constraints on autonomy in weapon systems, so that meaningful human control is maintained – in particular, with respect to: human supervision and the ability to intervene and deactivate; technical requirements for predictability and reliability (including in the algorithms used); and operational constraints on the task for which the weapon is used, the type of target, the operating environment, the timeframe of operation and the scope of movement over an area. However, the combined and interconnected ethical concerns about loss of human agency in decisions to use force, diffusion of moral responsibility and loss of human dignity could have the most far-reaching consequences, perhaps precluding the development and use of anti-personnel autonomous weapon systems, and even limiting the applications of antimateriel systems, depending on the risks that destroying materiel targets present for human life. CCW/GGE.1/2018/WP.5 3 Contents Page 1. Introduction ............................................................................................................. 4 2. The principles of humanity and the dictates of the public conscience .................... 5 2.1 Ethics and the law .................................................................................. 5 2.2 The Martens Clause ................................................................................ 5 2.3 The public conscience in practice ........................................................... 6 3. The ethical debate on Autonomous Weapon Systems ............................................. 7 3.1 Main ethical arguments .......................................................................... 8 3.2 Human agency in decisions to use force ................................................ 9 3.3 Human dignity: process and results ....................................................... 10 4. Responsibility, accountability and transparency ..................................................... 11 4.1 Implications of autonomy for moral responsibility ................................ 11 4.2 Transparency in human-machine interaction ......................................... 13 5. Predictability, reliability and risk ............................................................................ 14 5.1 Artificial Intelligence (AI) and unpredictability ..................................... 15 5.2 Ethics and risk ........................................................................................ 16 6. Ethical issues in context .......................................................................................... 17 6.1 Constraints in time and space ................................................................. 17 6.2 Constraints in operating environments, tasks and targets ....................... 18 7. Public and military perceptions ............................................................................... 19 7.1 Opinion surveys...................................................................................... 19 7.2 Contrasting military and public perceptions ........................................... 20 8. Conclusions ............................................................................................................. 20 8.1 An ethical basis for human control? ....................................................... 22 CCW/GGE.1/2018/WP.5 4 1. Introduction 1. Since 2011, the ICRC has been engaged in debates about autonomous weapon systems, holding international expert meetings with States and independent experts in March 20141 and March 2016,2 and contributing to discussions at the United Nations Convention on Certain Conventional Weapons (CCW) since 2014. 2. The ICRC’s position is that States must establish limits on autonomy in weapon systems to ensure compliance with international humanitarian law and other applicable international law, and to satisfy ethical concerns. It has called on States to determine where these limits should be placed by assessing the type and degree of human control required in the use of autonomous weapon systems (broadly defined as weapons with autonomy in their critical functions of selecting and attacking targets)3 for legal compliance and ethical acceptability.4 3. As part of continuing reflections, the ICRC convened a two-day round-table meeting with independent experts to consider the ethical issues raised by autonomous weapon systems and the ethical dimension of the requirement for human control over weapon systems and the use of force.5 This report summarizes discussions at the meeting, 1 ICRC, Autonomous weapon systems: Technical, military, legal and humanitarian aspects, 2014 – report of an expert meeting: https://www.icrc.org/en/document/report-icrc-meeting-autonomousweapon- systems-26-28-march-2014. 2 ICRC, Autonomous weapon systems: Implications of increasing autonomy in the critical functions of weapons, 2016 – report of an expert meeting: https://www.icrc.org/en/publication/4283-autonomousweapons- systems. 3 The ICRC’s working definition of an autonomous weapon system is: “Any weapon system with autonomy in its critical functions. That is, a weapon system that can select (i.e. search for or detect, identify, track, select) and attack (i.e. use force against, neutralize, damage or destroy) targets without human intervention.” This definition encompasses a limited number of existing weapons, such as: anti-materiel weapon systems used to protect ships, vehicles, buildings or areas from incoming attacks with missiles, rockets, artillery, mortars or other projectiles; and some loitering munitions. There have been reports that some anti-personnel “sentry” weapon systems have autonomous modes. However, as far as is known to the ICRC, “sentry” weapon systems that have been deployed still require human remote authorization to launch an attack (even though they may identify targets autonomously). See: ICRC, Autonomous weapon systems: Implications of increasing autonomy in the critical functions of weapons, op. cit. (footnote 2), 2016, pp. 11–12. 4 ICRC, Statement to the Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on “Lethal Autonomous Weapon Systems”, 15 November 2017: https://www.icrc.org/en/document/expert-meeting-lethal-autonomous-weapons-systems; N Davison, “Autonomous weapon systems under international humanitarian law”, in Perspectives on Lethal Autonomous Weapon Systems, United Nations Office for Disarmament Affairs (UNODA) Occasional Papers No. 30, November 2017: https://www.un.org/disarmament/publications/occasionalpapers/unoda-occasional-papers-no-30- november-2017; ICRC, Views of the ICRC on autonomous weapon systems, 11 April 2016: https://www.icrc.org/en/document/views-icrc-autonomous-weapon-system. 5 The event was entitled “Ethics and autonomous weapon systems: An ethical basis for human control?” and was held at the Humanitarium, International Committee of the Red Cross (ICRC), Geneva, on 28 and 29 August 2017. With thanks to the following experts for their participation: Joanna Bryson (University of Bath, UK); Raja Chatila (Institut des Systèmes Intelligents et de Robotique, France); Markus Kneer (University of Zurich, Switzerland); Alexander Leveringhaus (University of Oxford, UK); Hine-Wai Loose (United Nations Office for Disarmament Affairs, Geneva); AJung Moon (Open Roboethics Institute, Canada); Bantan Nugroho (United Nations Office for Disarmament Affairs, Geneva); Heather Roff (Arizona State University, USA); Anders Sandberg (University of Oxford, UK); Robert Sparrow (Monash University, Australia); Ilse Verdiesen (Delft University of Technology, Netherlands); Kerstin Vignard (United Nations Institute for Disarmament Research); Wendell Wallach (Yale University, US); and Mary Wareham (Human Rights Watch). The ICRC was represented by: Kathleen Lawand, Neil Davison and Anna Chiapello (Arms Unit, Legal Division); Fiona Terry (Centre for Operational Research and Experience); and Sasha Radin (Law and Policy Forum). Report prepared by Neil Davison, ICRC. CCW/GGE.1/2018/WP.5 5 supplemented by additional research. The report highlights key themes and conclusions from the perspective of the ICRC, and these do not necessarily reflect the views of the participants. 4. For the ICRC, the fundamental question at the heart of ethical discussions is whether, irrespective of compliance with international law, the principles of humanity and the dictates of the public conscience can allow human decision-making on the use of force to be effectively substituted with computer- controlled processes, and life-and-death decisions to be ceded to machines. The ICRC’s concerns reflect the sense of deep discomfort over the idea of any weapon system that places the use of force beyond human control.6 And yet, important questions remain: at what point have decisions effectively, or functionally, been delegated to machines? What type and degree of human control are required, and in which circumstances, to satisfy ethical concerns? These are questions with profound implications for the future of warfare and humanity, and all States, as well as the military, scientists, industry, civil society and the public, have a stake in determining the answers. 2. The principles of humanity and the dictates of the public conscience 2.1 Ethics and the law 5. Ethics and law are intimately linked, especially where the purpose of the law – such as international humanitarian law and international human rights law – is to protect persons. This relationship can provide insights into how considerations of humanity and public conscience drive legal development. 6. The regulation of any conduct of hostilities, including regulating the choice of weapons, starts with a societal decision of what is acceptable or unacceptable behaviour, what is right and wrong. Subsequent legal restrictions are, therefore, a social construct, shaped by societal and ethical perceptions. These determinations evolve over time; what was considered acceptable at one point in history is not necessarily the case today.7 However, some codes of behaviour in warfare have endured for centuries – for example, the unacceptability of killing women and children, and of poisoning. 7. It is clear that ethical decisions by States, and by society at large, have preceded and motivated the development of new international legal constraints in warfare, and that in the face of new developments not specifically foreseen or not clearly addressed by existing law, contemporary ethical concerns can go beyond what is already codified in the law. This highlights the importance of not reducing debates about autonomous weapon systems, or other new technologies of warfare, solely to legal compliance. 2.2 The Martens Clause 8. In international humanitarian law, notions of humanity and public conscience are drawn from the Martens Clause, a provision that first appeared in the Hague Conventions of 1899 and 1907, was later incorporated in the 1977 Additional Protocols to the Geneva Conventions, and is considered customary law. It provides that, in cases not covered by existing treaties, civilians and combatants remain under the protection and authority of the principles of humanity and the dictates of the public conscience.8 The Martens Clause 6 ICRC, Statement to the Convention on Certain Conventional Weapons (CCW) Meeting of Experts on “Lethal Autonomous Weapons Systems”, 13 April 2015: https://www.icrc.org/en/document/lethalautonomous- weapons-systems-LAWS. 7 For example, among conventional weapons: expanding bullets, anti-personnel mines and cluster munitions. 8 It appears in the preamble to Additional Protocol II and in Article 1(2) of Additional Protocol I: “In cases not covered by this Protocol or by any other international agreements, civilians and combatants remain under the protection and authority of the principles of international law derived from established custom, from the principles of humanity and from dictates of public conscience.” CCW/GGE.1/2018/WP.5 6 prevents the assumption that anything that is not explicitly prohibited by relevant treaties is therefore permitted – it is a safety net for humanity. The provision is recognized as being particularly relevant to assessing new technologies and new means and methods of warfare.9 9. There is debate over whether the Martens Clause constitutes a legally-binding yardstick against which the lawfulness of a weapon must be measured, or rather an ethical guideline. Nevertheless, it is clear that considerations of humanity and public conscience have driven the evolution of international law on weapons, and these notions have triggered the negotiation of specific treaties to prohibit or limit certain weapons, as well as underlying the development and implementation of the rules of international humanitarian law more broadly.10 2.3 The public conscience in practice 10. In the development of international humanitarian law on weapons there is a strong ethical narrative to be found in the words used by States, the ICRC (mandated to uphold international humanitarian law) and civil society in raising concerns about weapons that cause, or have the potential to cause, unacceptable harm. For example, regarding weapons that cause superfluous injury or unnecessary suffering for combatants, in 1918, the ICRC, in calling for a prohibition of chemical weapons, described them as “barbaric weapons”, an “appalling method of waging war”, and appealed to States’ “feeling of humanity”.11 In advocating for a prohibition of blinding laser weapons, the ICRC appealed to the “conscience of humanity” and later welcomed the 1995 Protocol IV to the Convention on Certain Conventional Weapons (CCW) as a “victory of civilization over barbarity”.12 11. Likewise, addressing weapons that strike blindly, indiscriminately affecting civilians, the ICRC expressed an ethical revulsion over the “landmine carnage” and “appalling humanitarian consequences” of anti-personnel mines in debates leading to the prohibition of these weapons in 1997.13 The recent Treaty on the Prohibition of Nuclear Weapons, adopted in July 2017 by a group of 122 States, recognizes that the use of nuclear weapons would be “abhorrent to the principles of humanity and the dictates of public conscience”.14 The ethical underpinnings of restrictions in international humanitarian law on the use of certain weapons are not in dispute. 12. Civil society, medical, scientific and military experts, and the ICRC and other components of the International Red Cross and Red Crescent Movement, have played a key role in raising the attention of States to the unacceptable harm caused by certain weapons, such as anti-personnel mines and cluster munitions, building on evidence collected by those treating victims. Engagement in these endeavours by military veterans and religious figures, appeals to political leaders and parliamentarians, the testimony of victims and communication of concerns to the public were central to securing these prohibitions. In some debates, such as on blinding laser weapons, reflections by the military on the risks for their 9 International Court of Justice, Legality of the Threat or Use of Nuclear Weapons, Advisory Opinion, ICJ Reports, 1996, para. 78. 10 K Lawand and I Robinson, “Development of treaties limiting or prohibiting the use of certain weapons: the role of the International Committee of the Red Cross”, in R Geiss, A Zimmermann and S Haumer (eds.), Humanizing the laws of war: the Red Cross and the development of international humanitarian law, Cambridge University Press, 2017, pp. 141–184; M Veuthey, “Public Conscience in International Humanitarian Law”, in D Fleck (ed.), Crisis Management and Humanitarian Protection, Berliner Wissenschafts-Verlag, Berlin, 2004, pp. 611–642. 11 ICRC, World War I: the ICRC's appeal against the use of poisonous gases, 1918: https://www.icrc.org/eng/resources/documents/statement/57jnqh.htm. 12 L Doswald-Beck, “New Protocol on Blinding Laser Weapons”, International Review of the Red Cross, No. 312, 1996: https://www.icrc.org/eng/resources/documents/article/other/57jn4y.htm. 13 P Herby and K Lawand, “Unacceptable Behaviour: How Norms are Established”, in J Williams, S Goose and M Wareham (eds.), Banning Landmines: Disarmament, Citizen Diplomacy and Human Security, Lanham, MD: Rowman & Littlefield Publishers, 2008, p. 202. 14 UN General Assembly, Treaty on the Prohibition of Nuclear Weapons, preamble, A/CONF.229/2017/8, 7 July 2017. CCW/GGE.1/2018/WP.5 7 own soldiers were critical. All these various activities can be seen, in some way, as a demonstration of the public conscience.15 3. The ethical debate on Autonomous Weapon Systems 13. Ethical questions about autonomous weapon systems have sometimes been viewed as secondary concerns. Many States have tended to be more comfortable discussing whether new weapons can be developed and used in compliance with international law, particularly international humanitarian law, and with the assumption that the primary factors that limit the development and use of autonomous weapon systems are legal and technical. 14. However, for many experts and observers, and for some States, ethics – the “moral principles that govern a person’s behaviour or the conducting of an activity”16 – are at the heart of what autonomous weapon systems mean for the human conduct of warfare, and the use of force more broadly. It is precisely anxiety about the loss of human control over this conduct that goes beyond questions of the compatibility of autonomous weapon systems with our laws to encompass fundamental questions of acceptability to our values. 15. Ethical concerns over delegating life-and-death decisions, and reflections on the importance of the Martens Clause, have been raised in different quarters, including by: more than 30 States during CCW meetings,17 a UN Special Rapporteur at the Human Rights Council,18 Human Rights Watch19 (and the Campaign to Stop Killer Robots), the ICRC,20 the United Nations Institute for Disarmament Research (UNIDIR),21 academics and think-tanks, and, increasingly, among the scientific and technical communities.22 16. Discussions on autonomous weapon systems have generally acknowledged the necessity for some degree of human control over weapons and the use for force, whether for legal, ethical or military operational reasons (States have not always made clear for which reasons, or combination thereof).23 It is clear, however, that the points at which human control is located in the development and deployment, and exercised in the use, of a weapon with autonomy in the critical functions of selecting and attacking targets may be central to 15 K Lawand and I Robinson, op. cit. (footnote 10), 2017. 16 Oxford Dictionary of English: https://en.oxforddictionaries.com/definition/ethics. 17 Including: Algeria, Argentina, Austria, Belarus, Brazil, Cambodia, Costa Rica, Cuba, Ecuador, Egypt, France, Germany, Ghana, Holy See, India, Kazakhstan, Mexico, Morocco, Nicaragua, Norway, Pakistan, Panama, Peru, Republic of Korea, Sierra Leone, South Africa, Sri Lanka, Sweden, Switzerland, Turkey, Venezuela, Zambia and Zimbabwe. 18 Human Rights Council, Report of the Special Rapporteur on extrajudicial, summary or arbitrary executions, Christof Heyns, A/HRC/23/47, 9 April 2013. 19 Human Rights Watch, Losing Humanity: The Case against Killer Robots, 19 November 2012. 20 ICRC, Statement to CCW Meeting of Experts on “Lethal Autonomous Weapons Systems”, 13–17 April 2015: https://www.icrc.org/en/document/lethal-autonomous-weapons-systems-LAWS. 21 UNIDIR, The Weaponization of Increasingly Autonomous Technologies: Considering Ethics and Social Values, 2015. 22 Future of Life Institute, Autonomous Weapons: an Open Letter from AI & Robotics Researchers, 28 July 2015; Future of Life Institute, An Open Letter to the United Nations Convention on Certain Conventional Weapons, 21 August 2017. 23 United Nations, Report of the 2017 Group of Governmental Experts on “Lethal Autonomous Weapons Systems” (LAWS), CCW/GGE.1/2017/CRP.1, 20 November 2017, p.7: “The importance of considering LAWS “Lethal Autonomous Weapon Systems”] in relation to human involvement and the human-machine interface was underlined. The notions that human control over lethal targeting functions must be preserved, and that machines could not replace humans in making decisions and judgements, were promoted. Various related concepts, including, inter alia, meaningful and effective human control, appropriate human judgement, human involvement and human supervision, were discussed.” United Nations, Recommendations to the 2016 Review Conference Submitted by the Chairperson of the Informal Meeting of Experts, November 2016, p. 1: “V]iews on appropriate human involvement with regard to lethal force and the issue of delegation of its use are of critical importance to the further consideration of LAWS amongst the High Contracting Parties and should be the subject of further consideration”. CCW/GGE.1/2018/WP.5 8 determining whether this control is “meaningful”, “effective” or “appropriate” from an ethical perspective (and a legal one). 17. A prominent aspect of the ethical debate has been a focus on “lethal autonomy” or “killer robots” – implying weapon systems that are designed to kill or injure humans, rather than autonomous weapon systems that destroy or damage objects, which are already employed to a limited extent.24 This is despite the fact that some anti-materiel weapons can also result in the death of humans either directly (humans inside objects, such as buildings, vehicles, ships and aircraft) or indirectly (humans in proximity to objects), and that even the use of non-kinetic weapons – such as cyber weapons – can result in kinetic effects and in human casualties. Of course, autonomy in the critical functions of selecting and attacking targets is a feature that could, in theory, be applied to any weapon system. 18. Ethical discussions have also transcended the context-dependent legal bounds of international humanitarian law and international human rights law. Ethical concerns, relevant in all circumstances, have been at the centre of warnings by UN Special Rapporteur Christof Heyns that “allowing LARs Lethal Autonomous Robots] to kill people may denigrate the value of life itself”,25 and by Human Rights Watch that “fully autonomous weapons” would “cross a moral threshold” because of “the lack of human qualities necessary to make a moral decision, the threat to human dignity and the absence of moral agency”.26 3.1 Main ethical argument 19. Nevertheless, ethical arguments have been made both for and against autonomous weapon systems, reflecting, to a certain extent, the different emphases of consequentialist (results-focused) and deontological (process-focused) approaches. The primary argument for these weapons has been an assertion that they might enable better respect for both international law and human ethical values by enabling greater precision and reliability than weapon systems controlled directly by humans, and therefore would result in less adverse humanitarian consequences for civilians.27 This type of argument has been made in the past for other weapon systems, including, most recently, for armed drones, and it is important to recognize that such characteristics are not inherent to a weapon system but depend on both the design-dependent effects and the way the weapon system is used.28 20. Another ethical argument that has been made for autonomous weapon systems is that they help fulfil the duty of militaries to protect their soldiers by removing them from harm’s way. However, since this can equally apply to remote-controlled and remotely-delivered weapons, it is not a convincing argument for autonomy in targeting per se, apart from, perhaps, in scenarios where human soldiers cannot respond quickly enough to an incoming threat, such as in missile and close-in air defence. 24 See footnote 3 on existing autonomous weapon systems. Although the use of anti-materiel systems has not been without its problems and accidents – see, for example: J Hawley, Automation and the Patriot Air and Missile Defense System, Center for a New American Security (CNAS), 25 January 2017. 25 Human Rights Council, op. cit. (footnote 18), 2013, p. 20. 26 Human Rights Watch, Making the Case: The Dangers of Killer Robots and the Need for a Preemptive Ban, 9 December 2016. 27 See, for example on ethical compliance: R Arkin “Lethal Autonomous Systems and the Plight of the Non-combatant”, in AISIB Quarterly, July 2013. And on legal compliance: United States, Autonomy in Weapon Systems, Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on “Lethal Autonomous Weapon Systems”, CCW/GGE.1/2017/WP.6, 10 November 2017, pp. 3–4. 28 For example, remote-controlled armed drones with precision-guided munitions may offer the potential for greater precision and therefore less risk of indiscriminate effects. However, if the information about the target is inaccurate, targeting practices are too generalized, or protected persons or objects are deliberately, or accidentally, attacked, then the potential for precision offers no protection in itself. CCW/GGE.1/2018/WP.5 9 21. Ethical arguments against autonomous weapon systems can generally be divided into two forms: objections based on the limits of technology to function within legal constraints and ethical norms;29 and ethical objections that are independent of technological capability.30 22. Given that technology trajectories are hard to predict, it is the second category of ethical arguments that may be the most interesting for current policy debates. Do autonomous weapon systems raise any universal ethical concerns? Among the main issues in this respect are:  removing human agency from decisions to kill, injure and destroy31 – decisions to use force – leading to a responsibility gap where humans cannot uphold their moral responsibility32  undermining the human dignity of those combatants who are targeted,33 and of civilians who are put at risk of death and injury as a consequence of attacks on legitimate military targets  further increasing human distancing – physically and psychologically – from the battlefield, enhancing existing asymmetries and making the use of violence easier or less controlled.34 3.2 Human agency in decisions to use force 23. In ethical debates, there seems to be wide acknowledgement of the importance of retaining human agency35 – and associated intent – in decisions to use force, particularly in decisions to kill, injure and destroy. In other words, many take the view that “machines must not make life-and-death decisions” and “machines cannot be delegated responsibility for these decisions”.36 24. Machines and computer programs, as inanimate objects, do not think, see and perceive like humans. Therefore, some argue, it is difficult to see how human values can be respected if the “decision” to attack a specific target is functionally delegated to a machine. However, there are differing perspectives on the underlying question: at which point have decisions to use force effectively been delegated to a machine? Or, from another perspective: what limits 29 See, for example: N Sharkey, “The evitability of autonomous robot warfare”, International Review of the Red Cross, No. 886, 2012. 30 See, for example: P Asaro, “On banning autonomous weapon systems: human rights, automation, and the dehumanization of lethal decision-making”, International Review of the Red Cross, No. 886, 2012; R Sparrow, “Robots and respect: Assessing the case against Autonomous Weapon Systems”, Ethics and International Affairs, 30(1), 2016, pp. 93–116; A Leveringhaus, Ethics and Autonomous Weapon Systems, Palgrave Macmillan, UK, 2016. 31 A Leveringhaus, Ethics and Autonomous Weapon Systems, op. cit. (footnote 30), 2016. 32 See, for example: R Sparrow, “Killer robots”, Journal of Applied Philosophy, 24(1), 2007, pp. 62–77; H Roff, “Killing in War: Responsibility, Liability and Lethal Autonomous Robots”, in F Allhoff, N Evans and A Henschke (eds.), Routledge Handbook of Ethics and War: Just War Theory in the 21st Century, Routledge, UK, 2014. 33 See, for example: R Sparrow, op. cit. (footnote 30), 2016; C Heyns, “Autonomous weapons in armed conflict and the right to a dignified life: An African perspective”, South African Journal on Human Rights, Vol. 33, Issue 1, 2017, pp. 46–71. 34 A Leveringhaus, “Distance, weapons technology and humanity in armed conflict”, ICRC Humanitarian Law & Policy Blog, 6 October 2017: http://blogs.icrc.org/law-andpolicy/ 2017/10/06/distance-weapons-technology-and-humanity-in-armed-conflict. 35 N Castree, R Kitchin and A Rogers, A Dictionary of Human Geography, Oxford University Press, Oxford, 2013: “The capacity possessed by people to act of their own volition.” 36 See footnote 17 listing States that have raised core ethical concerns. For example: “Germany will certainly adhere to the principle that it is not acceptable, that the decision to use force, in particular the decision over life and death, is taken solely by an autonomous system without any possibility for a human intervention.” Statement to CCW Meeting of Experts on “Lethal Autonomous Weapon Systems”, 11–15 April 2016. CCW/GGE.1/2018/WP.5 10 on autonomy are required to retain sufficient human agency and intent in these decisions? 25. There is a parallel in this debate with landmines, which have been described as “rudimentary autonomous weapon systems”.37 When humans lay landmines they effectively remove themselves from the decision about subsequent attacks on specific people or vehicles. They may know where the landmines are placed but they do not know who, or what, will trigger them, or when they will be triggered. This could be seen as a primitive form of delegating the decision to kill and injure to a machine. 26. Some argue it is difficult to establish a clear point at which this shift in functional decision-making from human to machine happens, and human agency and intention have been eroded or lost. Rather, it may be more useful, some propose, to agree on the general principle that a minimum level of human control is required in order to retain human agency in these decisions, and then consider the way in which humans must inject themselves into the decision-making process and at what points, to ensure this control is sufficient – for example, through human supervision and the ability to intervene and deactivate; technical requirements for predictability and reliability; and operational constraints on the task the weapon is used for, the type of target, the operating environment, the timeframe of operation and the scope of movement over an area.38 3.3 Human dignity: process and results 27. Closely linked to the issue of human agency, and concerns about the delegation of decisions to use force, is human dignity. The central argument here is that it matters not just if a person is killed and injured but how they are killed and injured. Where a line has been crossed, and machines are effectively making life-and-death “decisions”, the argument is that this undermines the human dignity of those targeted, even if they are lawful targets (for example, under international humanitarian law). As Christof Heyns, then UN Special Rapporteur on extrajudicial, summary or arbitrary executions, put it: “to allow machines to determine when and where to use force against humans is to reduce those humans to objects; they are treated as mere targets. They become zeros and ones in the digital scopes of weapons which are programmed in advance to release force without the ability to consider whether there is no other way out, without a sufficient level of deliberate human choice about the matter.” 39 28. Unlike previous discussions about constraints on weapons (see Section 2.3), which have focused on their effects (whether evidence of unacceptable harm or foreseeable effects), the additional ethical concerns with autonomous weapon systems are about process as well as results. What does this method of using force reveal about the underlying attitude to human life, to human dignity? And, in that sense, these concerns are particularly relevant to the relationship between combatants in armed conflict, although they are also relevant to 37 United States Department of Defense, Department of Defense Law of War Manual, Section 6.5.9.1, Description and Examples of the Use of Autonomy in Weapon Systems, 2015, p. 328: “Some weapons may have autonomous functions. For example, mines may be regarded as rudimentary autonomous weapons because they are designed to explode by the presence, proximity, or contact of a person or vehicle, rather than by the decision of the operator.” There are different views on whether the complexity of the function delegated to a machine affects this ethical assessment. Some distinguish between an “automated function” (activation, or not, of a landmine) and an “autonomous function” with “choice” (e.g. selecting between different targets), but there are no clear lines between automated and autonomous from a technical perspective, and both can enable functional delegation of decisions. See, for example: ICRC, Autonomous weapon systems: Implications of increasing autonomy in the critical functions of weapons, op. cit. (footnote 2), 2016, p. 8. 38 ICRC, Statement to the Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on “Lethal Autonomous Weapon Systems”, op. cit. (footnote 4), 15 November 2017. 39 C Heyns, Autonomous Weapon Systems: Human rights and ethical issues, presentation to the CCW Meeting of Experts on “Lethal Autonomous Weapon Systems”, 14 April 2016. CCW/GGE.1/2018/WP.5 11 civilians, who must not be targeted, but are, nevertheless, exposed to collateral risks of death and injury from attacks on legitimate military targets. 29. For some, autonomous weapon systems conjure up visions of machines being used to kill humans like vermin, and a reduced respect for human life due to a lack of human agency and intention in the specific acts of using force. In this argument, delegating the execution of a task to a machine may be acceptable, but delegating the decision to kill or injure is not, which means applying human intent to each decision. 30. There are strong parallels with the broader societal discussion about algorithmic, and especially artificial intelligence (AI)-driven, decision-making, including military decision-making40 (see also Section 5.1). Through handing over too much of the functional decision-making process to sensors and algorithms, is there a point at which humans are so far removed in time and space from the acts of selecting and attacking targets that human decision-making is effectively substituted by computer-controlled processes? The concern is that, if the connection between the human decision to use force and the eventual consequences is too diffuse, then human agency in that decision is weakened and human dignity eroded. 31. The counter-argument to an emphasis on process is found in the primary argument for autonomous weapons systems (see Section 3.1) that they will offer better results, posing less risk to civilians by enabling the users to exercise greater precision and discrimination than with human-operated systems. However, claims about reduced risks to civilians – which remain contentious in the absence of supporting evidence – are very much context-specific, whereas ethical questions about loss of human dignity present more of a universal concern, independent of context. 4. Responsibility, accountability and transparency 32. Responsibility and accountability for decisions to use force cannot be transferred to a machine or a computer program.41 These are human responsibilities – both legal and ethical – which require human agency in the decision-making process (see Section 3). Therefore, a closely related ethical concern raised by autonomous weapon systems is the risk of erosion – or diffusion – of responsibility and accountability for these decisions. 33. One way to address this concern is to assign responsibility to the operator or commander who authorizes the activation of the autonomous weapon system (or programmers and manufacturers, in case of malfunction). This addresses the issue of legal responsibility to some extent, simply by applying a process for holding an individual accountable for the consequences of their actions.42 And this is how militaries typically address responsibility for operations using existing weapon systems, including, presumably, those with autonomy in their critical functions. 4.1 Implications of autonomy for moral responsibility 34. For the ethical debate, however, responsibility is not only a legal concept but also a moral one. Some argue that, in order for the commander or operator to uphold their moral responsibility in a decision to activate an autonomous weapon system, their intent needs to be directly linked to the eventual outcome of the resulting attack. This requires an understanding of how the weapon will function and the specific consequences of activating 40 D Lewis, G Blum and N Modirzadeh, War-Algorithm Accountability, Harvard Law School Program on International Law and Armed Conflict (HLS PILAC), Harvard University, 31 August 2016: https://pilac.law.harvard.edu/waa. 41 ICRC, Statement to the Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on “Lethal Autonomous Weapon Systems”, op. cit. (footnote 4), 15 November 2017. 42 Although there are still questions around whether a person can be criminally accountable in situations where they lack the required knowledge or intent of how the system will operate once activated, or where there is insufficient evidence to discharge the burden of proof. CCW/GGE.1/2018/WP.5 12 it in those circumstances, which is complicated by the uncertainty introduced by autonomy in targeting. Uncertainty brings a risk that the consequences of activating the weapon will not be those intended – or foreseen – by the operator (see Section 5.2), which raises both ethical and legal concerns. 35. An autonomous weapon system – since it selects and attacks targets independently (after launch or activation) – creates varying degrees of uncertainty as to exactly when, where and/or why the resulting attack will take place. The key difference between a human or remote-controlled weapon and an autonomous weapon system is that the former involves a human choosing a specific target – or group of targets – to be attacked, connecting their moral (and legal) responsibility to the specific consequences of their actions. In contrast, an autonomous weapon system self-initiates an attack: it is given a technical description, or a “signature”, of a target, and a spatial and temporal area of autonomous operation. This description might be general (“an armoured vehicle”) or even quite specific (“a certain type of armoured vehicle”), but the key issue is that the commander or operator activating the weapon is not giving instructions on a specific target to be attacked (“specific armoured vehicle”) at a specific place (“at the corner of that street”) and at a specific point in time (“now”). Rather, when activating the autonomous weapon system, by definition, the user will not know exactly which target will be attacked (“armoured vehicles fitting this technical signature”), in which place (within x square kilometres) or at which point in time (during the next x minutes/hours). Thus, it can be argued, this more generalized nature of the targeting decision means the user is not applying their intent to each specific attack. 36. The potential technical description, or signature, for an enemy combatant is both extremely broad and highly specific (e.g. combatant, fighter or civilian that is directly participating in hostilities but not one that is hors de combat or surrendering) and can vary enormously from one moment to the next. It is therefore highly doubtful that a weapon system could be programmed functionally to identify “enemy combatants”.43 But, assuming this might be possible for the sake of argument, if an anti-personnel autonomous weapon system encountered the signature of an enemy combatant it would attack when the signature matches its programming. A human decision-maker controlling a weapon system in the same circumstances still has a choice. S/he may decide to attack, or s/he may decide not to attack – even if the technical signature fits – including owing to wider ethical considerations in the specific circumstances, which may go beyond whether the combatant is a lawful target.44 (From a legal perspective, it is important to note that the principles of military necessity and humanity already require that the kind and degree of force used against lawful targets must not exceed what is necessary to accomplish a legitimate military purpose in the circumstances.)45 37. In sum, from an ethical perspective, the removal of the human intent from a specific attack weakens moral responsibility by preventing considerations of humanity. There may be a causal explanation for why these combatants were attacked (i.e. they corresponded to the target signature) but we may not be able to offer a reason, an ethical justification, for that attack (i.e. why were they attacked in the specific circumstances?). Since the process of reason-giving and justification establishes moral responsibility, and makes people feel they are treated justly, autonomous technology risks blocking this process and diminishing it. 43 This does not mean it is necessarily simple, functionally, to identify objects (e.g. vehicles, buildings), since they change status over time (between military objective and civilian object), and objects used by civilians and the military can share similar characteristics. 44 A Leveringhaus, Ethics and Autonomous Weapon Systems, op. cit. (footnote 30), 2016, pp. 92–93. 45 N Melzer, Interpretive guidance on the notion of direct participation in hostilities under international humanitarian law, ICRC, Geneva, 2016. Chapter IX: Restraints on the use of force in direct attack, p. 82: “In situations of armed conflict, even the use of force against persons not entitled to protection against direct attack remains subject to legal constraints. In addition to the restraints imposed by international humanitarian law on specific means and methods of warfare, and without prejudice to further restrictions that may arise under other applicable branches of international law, the kind and degree of force which is permissible against persons not entitled to protection against direct attack must not exceed what is actually necessary to accomplish a legitimate military purpose in the prevailing circumstances.” CCW/GGE.1/2018/WP.5 13 4.2 Transparency in human-machine interaction 38. Machine control and human control have different strengths and weaknesses. As currently understood, machines have limited decision-making capacities and limited situational awareness but can respond very quickly, and according to specific parameters (although, of course, this is a fast-developing field, especially with respect to artificial intelligence (AI) – see Section 5.1). In contrast, humans have a limited attention span and field of perception but global situational awareness of their environment, and sophisticated decision-making capacities. This difference gives rise to a number of problems in humanmachine interaction that are relevant to discussions about autonomous weapon systems, including: automation bias – where humans place too much confidence in the operation of an autonomous machine; surprises – where a human is not fully aware of how a machine is functioning at the point s/he needs to take back control; and the “moral buffer” – where the human operator shifts moral responsibility and accountability to the machine as a perceived legitimate authority.46 39. This raises additional questions about how moral responsibility and accountability can be ensured in the use of an autonomous weapon system, including whether there will be sufficient transparency in the way it operates, and its interaction with the environment, to be sufficiently understood by humans. To address this concern, a human operator may need to have continuous situational awareness during the operation of an autonomous weapon system, as well as a two-way communication link to receive information and give updated instructions to the system, if necessary, as well as sufficient time to respond or change the course of action, where necessary. 40. These types of human-machine problems are already evident in existing civilian autonomous systems. One example is the accident that resulted when the pilot of a passenger aircraft had to re-take control following a failure in the autopilot system but was not sufficiently aware of the situation to respond in the correct way.47 Other accidents have happened with car “autopilot” systems, where drivers relied too heavily on a system with limited capacity.48 And there are also parallels with autonomous financial trading systems, causing so-called “flash crashes” in ways not predictable by human traders overseeing them, and not preventable owing to the extremely short time-scales involved.49 46 M Cummings, “Automation and Accountability in Decision Support System Interface Design”, Journal of Technology Studies, Vol. XXXII, No. 1, 2006: “… decision support systems that integrate higher levels of automation can possibly allow users to perceive the computer as a legitimate authority, diminish moral agency, and shift accountability to the computer, thus creating a moral buffering effect”. 47 See, for example: R Charette, “Air France Flight 447 Crash Causes in Part Point to Automation Paradox”, IEEE Spectrum, 2012: https://spectrum.ieee.org/riskfactor/aerospace/aviation/air-franceflight- 447-crash-caused-by-a-combination-of-factors. 48 J Stewart, “People Keep Confusing Their Teslas for Self-Driving Cars”, Wired, 25 January 2018: https://www.wired.com/story/tesla-autopilot-crash-dui. 49 US Securities & Exchange Commission, Findings regarding the market events of 6 May, 2010. Reports of the staffs of the CFTC and SEC to the Joint Advisory Committee on Emerging Regulatory Issues, 30 September 2010. CCW/GGE.1/2018/WP.5 14 5. Predictability, reliability and risk 41. Unpredictability and unreliability have been raised as key issues for any legal assessment of autonomous weapon systems,50 as well as for the risks their use may pose,51 in particular for civilians. However, these factors are also closely connected to ethical questions of human agency and moral responsibility (see Sections 3 and 4). 42. One way to think about predictability and reliability in autonomous (weapon) systems is as means of connecting human agency and intent with the eventual outcome and consequences of the machine’s operation. Predictability is the ability to “say or estimate that (a specified thing) will happen in the future or will be a consequence of something”.52 Applied to an autonomous weapon system, predictability is knowledge of how it will likely function in any given circumstances of use, and the effects that will likely result. Reliability is “the quality of being trustworthy or performing consistently well”.53 In this context, reliability is knowledge of how consistently the system will function as intended, i.e. without failures or unintended effects. 43. Degrees of unpredictability and unreliability in the use of an autonomous weapon system might: be inherent to the technical design of the weapon system; arise from the nature of the environment (e.g. ‘uncluttered’ deep sea versus ‘cluttered’ populated area); and/or be due to the interaction of the weapon system with the environment. Unpredictability and unreliability in the environment may also vary over time and within a given area (depending on the nature of the environment). 44. If one recognizes the argument of the necessity for human agency and intent in decisions to use force (see Section 3) and the difficulties raised by autonomy for moral responsibility and accountability (see Section 4), it follows that the use of weapon systems that lead to unpredictable and unreliable consequences, and therefore heightened risks for civilians, will accentuate these ethical concerns. Unpredictability and unreliability, in that sense, are both legally and ethically problematic. However, predictability and reliability, in themselves, do not necessarily resolve ethical questions. For example, an autonomous weapon system might be highly predictable and reliable in attacking combatants, but it could still raise ethical concerns with respect to human agency and human dignity (see Section 3). 45. Of course, there are only ever degrees of predictability and reliability in complex software-controlled systems. Unpredictable and unreliable operations may result from a variety of factors, including: software errors and system flaws; human cognitive bias in dismissing certain possibilities; in-built algorithmic bias; 54 “normal accidents”, where there is no clear error, but a system still does not function as expected; and deliberate hacking, spoofing or cyber attacks. 46. It is also important to emphasize that nothing is one hundred per cent predictable and reliable, including non-autonomous, human-controlled, weapon systems. Although it is clear that a high degree would be demanded in safety-critical autonomous systems, such as weapon systems, questions remain about the level of predictability and reliability required to satisfy ethical (and legal) considerations. 50 N Davison, Autonomous weapon systems under international humanitarian law, op. cit. (footnote 4), 2017; ICRC, Views of the ICRC on autonomous weapon systems, op. cit. (footnote 4), 11 April 2016; W Wallach, “Predictability and Lethal Autonomous Weapons Systems (LAWS)”, in German Federal Foreign Office, Lethal Autonomous Weapons Systems: Technology, Definition, Ethics, Law & Security, 2016, pp. 295–312. 51 See, for example: P Scharre, Autonomous Weapons and Operational Risk, Center for a New American Security (CNAS), February 2016; UNIDIR, Safety, Unintentional Risk and Accidents in the Weaponization of Increasingly Autonomous Technologies, 2016. 52 Oxford Dictionary of English: https://en.oxforddictionaries.com/definition/predictability. 53 Ibid: https://en.oxforddictionaries.com/definition/reliability. 54 See, for example: A Caliskan, J Bryson and A Narayanan, “Semantics derived automatically from language corpora contain human-like biases”, Science, Vol. 356, Issue 6334, 2017, pp. 183–186; C O’Neil, Weapons of Math Destruction: How big data increases inequality and threatens democracy, Crown, New York, 2016. CCW/GGE.1/2018/WP.5 15 5.1 Artificial Intelligence (AI) and unpredictability 47. For many considering the implications of autonomous weapon systems, the key change in recent years – and a fundamental challenge for predictability – is the further development of artificial intelligence (AI), and especially AI algorithms that incorporate machine learning. In general, machine-learning systems can only be understood at a particular moment in time. The “behaviour” of the learning algorithm is determined not only by initial programming (carried out by a human) but also by the process in which the algorithm itself “learns” and develops by “experience”. This can be offline learning by training (before deployment) and/or online learning by experience (after deployment) while carrying out a task. 48. Deep learning – where an algorithm develops by learning data patterns rather than learning a specific task – further complicates the ability to understand and predict how the algorithm will function, once deployed. It can also add to the problem of biases that can be introduced into an algorithm through limitations in the data sets used to “train” it. Or a learning system may simply have learned in a way that was not intended by the developer. 49. Complicating matters further, humans’ current ability to interrogate machinelearning algorithms is limited. Such systems are often described as “back-boxes”; the inputs and outputs may be known but the process by which a system converts an input to an output is not known. This type of system can be tested to help determine its functioning in different environments. However, there are significant limits in current abilities to verify the functioning of these systems, a task that becomes harder the more actions there are in the repertoire of the system and the more complex the inputs. If a system continues to learn after being tested, then the verification and validation (checks to determine if a system will operate as intended in a given environment) are no longer meaningful. This type of autonomous system would be inherently unpredictable (owing to its technical design) and, if applied to targeting, for example, the link between human intent and eventual outcome would effectively be severed.55 50. Questions about AI and learning algorithms in weapon systems and targeting functions are no longer theoretical. As with civilian digital technology, big data are an increasingly important resource, and the focus of data exploitation and analysis efforts is on AI algorithms. For the military, this promises a capability advantage for decision-making in data-rich conflict environments. And despite the risks of unpredictability, which may conflict with military commanders’ propensity for command and control, there is significant and increasing interest among the major powers in the military applications of AI,56 including projects underway to apply machine learning to automatic target recognition and identification.57 AI systems may not even need to have a physical component to raise ethical 55 From a legal perspective, when considering the obligation of States to review new weapons before their deployment and use under Article 36 of Additional Protocol I to the Geneva Conventions, it is difficult to see how a weapon system that autonomously changes its own functioning could ever be approved, since what had been tested and verified at one point in time would not be valid for the future. See: ICRC, Autonomous weapon systems: Implications of increasing autonomy in the critical functions of weapons, op. cit. (footnote 2), 2016, p. 13. 56 See, for example: United States Department of Defense, Summer Study on Autonomy, Defense Science Board, June 2016; M Cummings, Artificial Intelligence and the Future of Warfare, Chatham House, International Security Department and US and the Americas Programme, January 2017; G Allen and T Chan, Artificial Intelligence and National Security, Harvard Kennedy School, Belfer Center for Science and International Affairs, 2017; E Kania, Battlefield Singularity. Artificial Intelligence, Military Revolution, and China’s Future Military Power, Center for a New American Security (CNAS), 2017; “Artificial Intelligence and Chinese Power”, Associated Press, 2017; “Putin: Leader in artificial intelligence will rule world”, CNBC, 4 September 2017: https://www.cnbc.com/2017/09/04/putin-leader-in-artificial-intelligence-will-rule-world.html. 57 See, for example: J Keller, DARPA TRACE program using advanced algorithms, embedded computing for radar target recognition”, Military & Aerospace Electronics, 2015: http://www.militaryaerospace.com/articles/2015/07/hpec-radar-target-recognition.html; D Lewis, N Modirzadeh and G Blum, “The Pentagon’s New Algorithmic-Warfare Team”, Lawfare, 2017: https://www.lawfareblog.com/pentagons-new-algorithmic-warfare-team. CCW/GGE.1/2018/WP.5 16 (and legal) questions if their outputs, as “decision aids,” are applied to targeting decisions, especially in the absence of cross-checking, or balancing, with other sources of information before human authorization to attack (as over-reliance on algorithmic output would diminish the meaning of the consequent human decision). However, if such AI systems are used directly to control the initiation of an attack by an autonomous weapon system, these concerns would be particularly serious. More broadly, there is growing appreciation of the risks of use, and misuse, of AI across the digital, physical and political domains, and the implications for international security.58 51. The degree of predictability and reliability of autonomous (weapon) systems affects the trust of humans in that system – especially in relation to the link between human intention and the eventual “action”, or operation, of the system – and this trust is also affected by the degree to which the operation of the system can be explained – or explain itself (e.g. with in-built “explainable AI”).59 52. There are now more and more initiatives addressing these ethical questions for AI systems in general, including the Institute of Electrical and Electronics Engineers (IEEE)’s Global Initiative on Ethics of Autonomous and Intelligent Systems, which is working on “ethically aligned design” standards for AI and autonomous systems,60 and for robotic systems, in particular.61 The Asilomar AI Principles recently developed by the Future of Life Institute are interesting in this respect. In warning against an AI arms race,62 they highlight ethical concerns raised by AI systems in general, noting the need for safety, failure transparency, responsibility of developers, alignment with human values and human control over delegation of decisions to AI systems.63 5.2 Ethics and risk 53. Unpredictability and unreliability in autonomous weapon systems also contribute to the level of risk that the use of the weapon will lead to unacceptable consequences, in particular for civilians, which raises ethical (as well as legal) issues. Since assessing risk requires an assessment of probability and consequence, machine-learning systems, for example, present immediate problems. Where there is inherent unpredictability in the functioning of a system it may not be possible to assess the probability of a certain action, and so determining risk becomes problematic. The introduction of this unpredictability into system design is therefore a significant concern in managing risk. From a purely ethical 58 Future of Humanity Institute, University of Oxford; Centre for the Study of Existential Risk, University of Cambridge; Center for a New American Security; Electronic Frontier Foundation; and OpenAI, The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation, 2018: https://maliciousaireport.com. 59 See, for example: DARPA, Explainable Artificial Intelligence (XAI): https://www.darpa.mil/program/explainable-artificial-intelligence. 60 Institute of Electrical and Electronics Engineers (IEEE), The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems, http://standards.ieee.org/develop/indconn/ec/autonomous_systems.html. 61 See, for example: Engineering and Physical Sciences Research Council (EPSRC), Principles of Robotics, https://www.epsrc.ac.uk/research/ourportfolio/themes/engineering/activities/principlesofrobotics/; J Bryson, “The meaning of the EPSRC principles of robotics”, Connection Science, Vol. 29 No. 2, 2017, pp. 130–136. 62 Future of Life Institute, Asilomar AI Principles, 2017: https://futureoflife.org/ai-principles/: “18) AI Arms Race: An arms race in lethal autonomous weapons should be avoided.” 63 Ibid. “6) Safety: AI systems should be safe and secure throughout their operational lifetime, and verifiably so where applicable and feasible. 7) Failure Transparency: If an AI system causes harm, it should be possible to ascertain why. 9) Responsibility: Designers and builders of advanced AI systems are stakeholders in the moral implications of their use, misuse, and actions, with a responsibility and opportunity to shape those implications. 10) Value Alignment: Highly autonomous AI systems should be designed so that their goals and behaviors can be assured to align with human values throughout their operation. 11) Human Values: AI systems should be designed and operated so as to be compatible with ideals of human dignity, rights, freedoms, and cultural diversity. … 16) Human Control: Humans should choose how and whether to delegate decisions to AI systems, to accomplish human-chosen objectives.” CCW/GGE.1/2018/WP.5 17 perspective, some have even argued that creating an unreasonable risk should be considered harm, and ethically wrong, even if that risk does not materialize.64 54. The level of risk also relates to the potential consequences of an unpredicted or unintended action, which will also be determined by the specific type of autonomous weapon system and the context of its use, including uses that were not originally foreseen. Some emphasize that the destructive power of the weapon system – in terms of size of munition or potential destructive effects – is an important factor in determining the level of risk, and therefore for an ethical assessment. For example, few would argue for development of autonomous nuclear weapon systems, even if predictability and reliability could be assured as extremely high. However, others are sceptical of a focus on the destructive power, since relatively low-power weapons – such as an autonomous machine-gun system – could still have serious consequences and be used to kill and injure many people (see also Section 6). In summary, while predictability and reliability may reduce the risks of unintended consequences in the operation of an autonomous weapon system, they do not, in themselves, eliminate risk. 6. Ethical issues in context 55. Another aspect to consider is whether ethical assessments of autonomous weapon systems vary according to context. In particular, do specific characteristics of an autonomous weapon system, and the way it is used, have an influence on its ethical acceptability? For example: the task the weapon is used for, the type of target, the operating environment, the timeframe of operation and the scope of movement over an area. 56. When discussing different types of autonomous weapon systems, in different scenarios and contexts, different views tend to emerge on ethical acceptability. These assessments tend to vary according to the core determinations of human agency in the decision-making process and human dignity (see Section 3), associated moral responsibility (see Section 4) and, especially, the degree of predictability and risk (see Section 5), since contextual factors can have a significant impact on the last of these. 6.1 Constraints in time and space 57. A longer timeframe and/or increased scope of movement over an area are major factors in contributing to uncertainty between the point of activation of an autonomous weapon system and the eventual attack that results. As discussed, an autonomous weapon system – since it selects and attacks targets independently (after launch or activation) – creates varying degrees of uncertainty as to exactly when, where and/or why the resulting attack will take place (see also Section 4.1).65 This is accentuated by wider temporal and spatial boundaries because of greater room for variations in the operational environment over an area, and evolution of that environment over time, both of which may affect the consequences of activation. 58. Uncertainties introduced by autonomy are clearly a problem from a legal perspective, to the extent that they may prevent the commander or operator from making judgements and taking decisions in line with their legal obligations – of distinction, proportionality and precautions – in carrying out attacks in armed conflict. However, uncertainties also raise concerns from an ethical perspective because they can decouple human agency and intent in the decision to use force from the eventual consequences, even if the resulting attack is lawful (see Section 3). 59. There are different dimensions to the issue of temporal constraints. One is the elapsed time between the point of activation of an autonomous weapon system and the point at which a resulting attack takes place. For example, there is a significant difference in the 64 C Finkelstein, “Is Risk a Harm?” University of Pennsylvania Law Review, No. 263, 2003. 65 This is in contrast to a long-range non-autonomous weapon system, such as a cruise missile, which may travel long distances, with a significant delay between launch and impact, but is intended to hit a specific target at a specific point in time. (It may also have the capacity to be manually or automatically deactivated after launch.) CCW/GGE.1/2018/WP.5 18 level of uncertainty in circumstances that may result during a ten-minute flight time versus a two-day loiter time (also depending on the operating environment). There are parallels, here, with mine warfare; a major problem with anti-personnel mines, which contributed to their indiscriminate effects and eventual prohibition, was the lack of control over the period during which they could autonomously operate. Once laid by humans, and unless fitted with self-destruct or self-neutralizing features, landmines remain activated indefinitely, and the initial user has no further control over the eventual attack and the nature of the victim. 60. Mines that stay active indefinitely also raise another time-related concern: the absence of an “off switch”. With autonomous weapon systems, the uncertainty over when, where and/or why an attack takes place could be extended indefinitely if there is no capacity to deactivate the system after launch or activation. Unless the system has an automatic selfdestruct or self-neutralizing feature (the reliability of which can also vary, as was the case with landmines), the ability to deactivate an autonomous weapon system would require a communication link with a human operator to be retained. Since changes in the operational environment may require deactivation at any point following activation, there is a strong argument for enabling constant human supervision and the ability to intervene and deactivate, as is the case with many existing autonomous weapon systems, such as counter- rocket, artillery and mortar weapons.66 61. A further aspect of the temporal issue is human reaction time. Some existing autonomous weapon systems are, by design, intended to initiate an attack quicker than is humanly possible. While speed may create a military advantage – for example, in the case of time-constrained missile and counter rocket, artillery and mortar defence – it also erodes the potential for human intervention to prevent an unlawful, unnecessary or accidental attack. Even with continuous human supervision, it may only be possible to deactivate a weapon system after a problematic attack in order to prevent further negative consequences, and whether or not this is an acceptable risk may depend on the predictability and reliability of the weapon, the operating environment, as well as the task for which it is used and the target against which it is employed. 6.2 Constraints in operating environments, tasks and targets 62. The task for which an autonomous weapon system is used and the environment in which it is used can also be significant for ethical assessments. In situations where there are fewer risks to civilians or civilian objects, some have argued there may also be fewer ethical concerns raised by autonomy – in terms of reduced human agency. For example, it has been suggested that autonomous deep-sea, anti-submarine warfare and autonomous close-in air defence at sea may be more ethically acceptable, owing to the relatively uncluttered and simple nature of the operating environments, and the reduced numbers of civilians and civilian objects, compared with populated areas on the coast or inland – and, therefore, potentially more predictable, in terms of consequences, and lower-risk.67 63. Further, there is the issue of whether an autonomous weapon system is used for defensive or offensive tasks. Some suggest there may be an ethical distinction between a “defensive” weapon system – such as a missile or counter-rocket, artillery and mortar defence weapon, or a “sentry” weapon guarding a border – and an “offensive” system, which actively searches for targets. However, others caution that the distinction between “offensive” and “defensive” is not clear operationally (and legally, the same rules apply to the use of force or conduct of hostilities), and that a weapon system introduced for a “defensive” task may later be used in an “offensive” role. 66 Such a requirement could limit the utility of autonomous weapon systems where constant communication is not feasible, such as underwater. 67 R Sparrow and G Lucas, “When Robots Rule the Waves?” Naval War College Review, 69(4), 2016, pp. 49–78. CCW/GGE.1/2018/WP.5 19 64. Perhaps the most significant contextual factor that gives rise to ethical concerns, however, is the nature of the target, and whether the weapon system only targets objects or attacks humans directly. The fundamental anxiety in the ethical discourse is about antipersonnel autonomous weapon systems, especially, it is argued, with respect to: lack of human agency and intent in decisions to use force; the loss of human dignity on the part of those combatants targeted,68 and of civilians that are put at risk as a consequence of legitimate attacks on military targets; and the implications for moral responsibility (see Sections 3 and 4). 7. Public and military perceptions 65. Although public opinion does not necessarily equal public conscience, and ethics, as a formal mode of criticism, should not be reduced to opinion polls, it is useful to explore the perspectives on autonomous weapon systems from different constituents of society – including the public, the military, and the scientific and technical communities.69 66. Public opinion may not provide evidence-based answers to ethical questions, especially when those surveyed have different understandings of the questions and the concept of an autonomous weapon system. However, opinion polls can spark debate and illustrate a significant interest in and engagement with the topic by different constituents, as well as revealing trends related to public-conscience concerns. 7.1 Opinion surveys 67. There have been several surveys of public opinion in this field.70 Many have contrasted remote-controlled armed drones with autonomous weapon systems, in order to differentiate reactions to autonomy specifically from robotic-weapons platforms in general. In 2011, Moon, Danielson and Van der Loos found greater rejection of autonomous weapon systems (81% against, 10% in favour) than of remote-controlled drones (53% against, 35% in favour) based on three major rationales: preservation of human responsibility and accountability; scepticism about the technology, and therefore risks for civilians; and assertions that humans should always make life-or-death decisions.71 68. In 2015, an Open Roboethics Initiative survey gathered the views of 1000 people from 49 different countries. It, too, found a significant rejection of autonomous weapon systems (67% said all types should be banned) and stronger views based on the type of task (85% should not be used for “offensive purposes”). The rejection of autonomous weapons was also greater in comparison with remote-controlled weapons (71% would prefer their military to use remote-controlled weapons in warfare; 60% would prefer to be attacked by remotecontrolled rather than autonomous weapons).72 69. A 2017 IPSOS poll of 11,500 respondents in 25 countries also found overall opposition to autonomous weapon systems (56% against, 24% in favour), although the poll 68 R Sparrow, “Twenty seconds to comply: Autonomous Weapon Systems and the recognition of surrender”, International Law Studies, 91, 2015, pp. 699–728. 69 R Sparrow, “Ethics as a source of law: The Martens clause and autonomous weapons”, ICRC Humanitarian Law & Policy Blog, 14 November 2017: http://blogs.icrc.org/law-andpolicy/ 2017/11/14/ethics-source-law-martens-clause-autonomous-weapons. 70 Including: L Moshkina and R Arkin, “Lethality and Autonomous Systems: The Roboticist Demographic”, IEEE International Symposium on Technology and Society, 2008; Prof. C Carpenter, US public opinion on autonomous weapons, University of Massachusetts Department of Political Science, 2013; M Horowitz, “Public opinion and the politics of the killer robots debate”, Research and Politics, January–March 2016. 71 A Moon, P Danielson and M Van der Loos, “Survey-based Discussions on Morally Contentious Applications of Interactive Robotics”, International Journal of Social Robotics, Volume 4, Issue 1, 2012, pp 77–96. 72 Open Roboethics Initiative, The Ethics and Governance of Lethal Autonomous Weapons Systems: An International Public Opinion Poll, 9 November 2015. CCW/GGE.1/2018/WP.5 20 also revealed regional variations, with the greatest opposition in Russia (69% against), Peru (67% against), Spain (66% against) and Argentina (66% against), and the least in India (31%), China (36%) and the United States (45%).73 70. While each study has its limitations, these polls reflect trends that are worth exploring further. Why do people tend to prefer attacks to be carried out by remote-controlled rather than autonomous weapon systems? How much significance is placed on reservations about the technology and its consequences, and how much on ethical concerns about human agency, human dignity and the view that machines must not take decisions on the use of force? 7.2 Contrasting military and public perceptions 71. Another 2017 survey contrasted perceptions of remote-controlled armed drones and autonomous weapon systems among the public in the United States, and civilian and military personnel of the Dutch Ministry of Defence.74 The Ministry of Defence personnel had less trust, confidence and support for the “actions” taken by autonomous weapon systems compared with remote-controlled systems but considered them equally “fair”. Respondents were, generally, more anxious about the consequences of using autonomous weapon systems, and concern about a lack of respect for human dignity was one of the main objections, when compared with human-operated drones resulting in the same consequences. In comparisons between military and public perceptions, most notable was the similar level of concern about a loss of human dignity, which may indicate some common ground among different constituents. 8. CONCLUSIONS 72. Ethics, humanity and the dictates of the public conscience are at the heart of the debate about the acceptability of autonomous weapon systems. From the ICRC’s perspective, ethics provides another avenue – alongside legal assessments and technical considerations – to help determine the necessary type and degree of human control that must be retained over weapon systems, and the use of force, and to elucidate where States must establish limits on autonomy in weapon systems. 73. Considerations of humanity and the public conscience provide ethical guidance for discussions, and there is a requirement to connect them to legal assessments via the Martens Clause – a safety net for humanity. These ethical considerations go beyond whether autonomous weapon systems are compatible with our laws to include fundamental questions of whether they are acceptable to our values. And such debates necessarily require the engagement of various constituents of society. 74. Several ethical issues appear central to establishing constraints on autonomy in weapon systems. Perhaps the most powerful ethical concerns are those that transcend context – whether during armed conflict or in peacetime – and transcend technology – whether simple or sophisticated.75 These are concerns about loss of human agency in decisions to use force – decisions to kill, injure and destroy – loss of human dignity in the process of using force, and erosion of moral responsibility for these decisions. 73 IPSOS, Three in ten Americans support using Autonomous Weapons, 7 February 2017. 74 I Verdiesen, Agency perception and moral values related to Autonomous Weapons: An empirical study using the Value-Sensitive Design approach, Masters of Science, Faculty of Technology, Policy and Management, TU Delft, 2017. 75 Although there are different views among experts on the issue of technology. Some make a distinction between “automated” and “autonomous” weapons and focus their concerns on systems controlled by complex AI algorithms rather than simpler software. Others, including the ICRC, note the lack of a clear technical distinction between the two, and argue that “all such weapons raise the same core legal and ethical questions”. See: ICRC, Autonomous weapon systems: Implications of increasing autonomy in the critical functions of weapons, op. cit. (footnote 2), 2016, p. 8. CCW/GGE.1/2018/WP.5 21 75. The importance of retaining human agency – and intent – in these decisions is one of the central ethical arguments for limits on autonomy in weapon systems. Many take the view that decisions to kill, injure and destroy must not be delegated to machines, and that humans must be present in this decision-making process sufficiently to preserve a direct link between the intention of the human and the eventual operation of the weapon system. It is not enough simply to say that “humans have developed, deployed and activated the weapon system”. There must be a direct connection between the human rationale for activation of an autonomous weapon system in the specific circumstances and the consequences of the resulting attack. But questions remain about how close this connection must be, and what form it must take. 76. Human dignity is another core ethical consideration that is linked to concerns about loss of human agency. The central argument is that it matters not just if a person is killed or injured but how they are killed or injured, and the process by which these decisions are made is as important as the results. If human agency is lacking to the extent that machines have effectively, and functionally, been delegated these decisions, then, according to this argument, it undermines the human dignity of those combatants targeted, and of civilians that are put at risk as a consequence of legitimate attacks on military targets. If human agency is retained, on the other hand, it is an acknowledgement of humanity in that decision to use force and the resulting consequences. 77. The need for human agency is also linked to moral responsibility and accountability for decisions to use force. These are human responsibilities (both ethical and legal), which cannot be transferred to inanimate machines, or computer algorithms, since it is humans that have both rights and responsibilities in relation to these decisions. From an ethical perspective, it is not sufficient only to assign legal responsibility to a commander or operator who activates an autonomous weapon system. Humans must uphold their moral responsibility, requiring not only a causal explanation but also a justification for the resulting use of force. Autonomous weapon systems complicate this justification because of the more generalized nature of the targeting decisions, which risks eroding – or diffusing – moral responsibility. 78. Predictability and reliability in using an autonomous weapon system are ways of connecting human agency and intent to the eventual consequences of the resulting attack. A lack of predictability, whether inherent to the weapon system design or due to interaction with the environment, raises serious ethical (and legal) concerns owing to a lack of foreseeability of the consequences and associated risks, in particular for civilians. 79. As weapons that self-initiate attacks, autonomous weapon systems all raise questions about predictability, owing to varying degrees of uncertainty as to exactly when, where and/or why a resulting attack will take place. However, the application of AI and, in particular, machine learning, to targeting functions accentuates this problem, raising fundamental questions of inherent unpredictability by design and heightening concerns about the loss of human agency, moral responsibility and human dignity. 80. Context also affects ethical assessments of autonomous weapon systems, owing to the impact on the predictability of the outcomes of their use, the nature of the consequences and the overall level of risk that results. Constraints on the timeframe of operation and scope of movement over an area are key factors, as are the task for which the weapon is used and the operating environment in which it is activated. 81. However, from an ethical perspective, perhaps the most important contextual factor is the type of target. Core concerns about human agency, human dignity and moral responsibility are most acute in relation to the notion of anti-personnel autonomous weapon systems that target humans directly. These concerns may be one reason – together with legal considerations and technical limitations – why the use of autonomous weapon systems to date has been constrained to anti-materiel systems,76 targeting projectiles, 76 There have been reports that some anti-personnel “sentry” weapon systems have autonomous modes. However, as far as is known to the ICRC, “sentry” weapon systems that have been deployed still CCW/GGE.1/2018/WP.5 22 vehicles, aircraft or other objects, even if these systems pose dangers to humans inside or in proximity to objects.77 8.1 An ethical basis for human control? 82. From the ICRC’s perspective, ethical considerations very much parallel the requirement for a minimum level of human control over weapon systems and the use of force, to ensure compliance with international legal obligations that govern the use of force in armed conflict and in peacetime.78 83. From an ethical viewpoint, “meaningful”, “effective” or “appropriate” human control would be the type and degree of control that preserves human agency and upholds moral responsibility in decisions to use force. This does not necessarily exclude autonomy in weapon systems, but it requires a sufficiently direct and close connection to be maintained between the human intent of the user and the eventual consequences of the operation of the weapon system in a specific attack. This, in turn, will necessitate limits on autonomy. 84. Ethical and legal considerations may demand some similar constraints on autonomy in weapon systems so that meaningful human control is maintained – in particular, with respect to: human supervision and the ability to intervene and deactivate; technical requirements for predictability and reliability (including in the algorithms used); and operational constraints on the task for which the weapon is used, the type of target, the operating environment, the timeframe of operation and the scope of movement over an area.79 85. However, the combined and interconnected ethical concerns about loss of human agency in decisions to use force, diffusion of moral responsibility and loss of human dignity could have the most far-reaching consequences, perhaps precluding the development and use of anti-personnel autonomous weapon systems, and even limiting the applications of anti-materiel systems, depending on the risks that destroying materiel targets present for human life. require human remote authorization to launch an attack (even though they may identify targets autonomously). See also footnote 3. 77 Including through accidents. See, for example, “fratricide” incidents discussed in: J Hawley, Automation and the Patriot Air and Missile Defense System, op. cit. (footnote 24), 2017. 78 N Davison, Autonomous weapon systems under international humanitarian law, op. cit. (footnote 4), 2017; M Brehm, Defending the Boundary: Constraints and Requirements on the Use of Autonomous Weapon Systems Under International Humanitarian and Human Rights Law, Geneva Academy Briefing no. 9, 1 May 2017. 79 ICRC, Statement to the Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on “Lethal Autonomous Weapon Systems”, op. cit. (footnote 4), 15 November 2017. Group of Governmental Experts on Emerging Technologies in the Area of Lethal Autonomous Weapons Systems Geneva, 25–29 March 2019 and 20–21 August 2019 Agenda item 5 Focus of work of the Group of Governmental Experts in 2019 Autonomy, artificial intelligence and robotics: Technical aspects of human control Submitted by International Committee of the Red Cross (ICRC)1 1. The International Committee of the Red Cross (ICRC) has emphasized the need to maintain human control over weapon systems and the use of force, to ensure compliance with international law and to satisfy ethical concerns. This approach has informed the ICRC’s analysis of the legal, ethical, technical and operational questions raised by autonomous weapon systems. 2. In June 2018, the ICRC convened a round-table meeting with independent experts in autonomy, artificial intelligence (AI) and robotics to gain a better understanding of the technical aspects of human control, drawing on experience with civilian autonomous systems. This report combines a summary of the discussions at that meeting with additional research, and highlights the ICRC’s main conclusions, which do not necessarily reflect the views of the participants. Experience in the civilian sector yields insights that can inform efforts to ensure meaningful, effective and appropriate human control over weapon systems and the use of force. 3. Autonomous (robotic) systems operate without human intervention, based on interaction with their environment. These systems raise such questions as “How can one ensure effective human control of their functioning?” and “How can one foresee the consequences of using them?” The greater the complexity of the environment and the task, the greater the need for direct human control and the less one can tolerate autonomy, especially for tasks and in environments that involve risk of death and injury to people or damage to property – in other words safety-critical tasks. 4. Humans can exert some control over autonomous systems – or specific functions – through supervisory control, meaning “human-on-the-loop” supervision and ability to intervene and deactivate. This requires the operator to have: • situational awareness • enough time to intervene • a mechanism through which to intervene (a communication link or physical controls) in order to take back control, or to deactivate the system should circumstances require. 1 To download the full report visit: https://www.icrc.org/en/war-and-law/weapons/ihl-and-new-technologies CCW/GGE.1/2019/WP.7 Convention on Prohibitions or Restrictions on the Use of Certain Conventional Weapons Which May Be Deemed to Be Excessively Injurious or to Have Indiscriminate Effects 20 August 2019 English only CCW/GGE.1/2019/WP.7 2 5. However, human-on-the-loop control is not a panacea, because of such humanmachine interaction problems as automation bias, lack of operator situational awareness and the moral buffer. 6. Predictability and reliability are at the heart of discussions about autonomy in weapon systems, since they are essential to achieving compliance with international humanitarian law and avoiding adverse consequences for civilians. They are also essential for military command and control. 7. It is important to distinguish between: reliability – a measure of how often a system fails; and predictability – a measure of how the system will perform in a particular circumstance. Reliability is a concern in all types of complex system, whereas predictability is a particular problem with autonomous systems. There is a further distinction between predictability in a narrow sense of knowing the process by which the system functions and carries out a task, and predictability in a broad sense of knowing the outcome that will result. 8. It is difficult to ensure and verify the predictability and reliability of an autonomous (robotic) system. Both factors depend not only on technical design but also on the nature of the environment, the interaction of the system with that environment and the complexity of the task. However, setting boundaries or imposing constraints on the operation of an autonomous system – in particular on the task, the environment, the timeframe of operation and the scope of operation over an area – can render the consequences of using such a system more predictable. 9. In a broad sense, all autonomous systems are unpredictable to a degree because they are triggered by their environment. However, developments in the complexity of software control systems – especially those based on AI and machine learning – add unpredictability in the narrow sense that the process by which the system functions is unpredictable. 10. The “black box” manner in which many machine learning systems function makes it difficult – and in many cases impossible – for the user to know how the system reaches its output. Not only are such algorithms unpredictable but they are also subject to bias, whether by design or in use. Furthermore, they do not provide explanations for their outputs, which seriously complicates establishing trust in their use and exacerbates the already significant challenges of testing and verifying the performance of autonomous systems. And the vulnerability of AI and machine learning systems to adversarial tricking or spoofing amplifies the core problems of predictability and reliability. 11. Computer vision and image recognition are important applications of machine learning. These applications use deep neural networks (deep learning), of which the functioning is neither predictable nor explainable, and such networks can be subject to bias. More fundamentally, machines do not see like humans. They have no understanding of meaning or context, which means they make mistakes that a human never would. 12. It is significant that industry standards for civilian safety-critical autonomous robotic systems – such as industrial robots, aircraft autopilot systems and self-driving cars – set stringent requirements regarding: human supervision, intervention and deactivation – or failsafe; predictability and reliability; and operational constraints. Leading developers of AI and machine learning have stressed the need to ensure human control and judgement in sensitive applications – and to address safety and bias – especially where applications can have serious consequences for people’s lives. 13. Civilian experience with autonomous systems reinforces and expands some of the ICRC’s viewpoints and concerns regarding autonomy in the critical functions of weapon systems. The consequences of using autonomous weapon systems are unpredictable because of uncertainty for the user regarding the specific target, and the timing and location of any resulting attack. These problems become more pronounced as the environment or the task become more complex, or freedom of action in time and space increases. Human-on-the-loop supervision, intervention and the ability to deactivate are absolute minimum requirements for countering this risk, but the system must be designed to allow for meaningful, timely, human intervention – and even that is no panacea. 14. All autonomous weapon systems will always display a degree of unpredictability stemming from their interaction with the environment. It might be possible to mitigate this CCW/GGE.1/2019/WP.7 3 to some extent by imposing operational constraints on the task, the timeframe of operation, the scope of operation over an area and the environment. However, the use of software control based on AI – and especially machine learning, including applications in image recognition – brings with it the risk of inherent unpredictability, lack of explainability and bias. This heightens the ICRC’s concerns regarding the consequences of using AI and machine learning to control the critical functions of weapon systems and raises questions about its use in decision-support systems for targeting. 15. This review of technical issues highlights the difficulty of exerting human control over autonomous (weapon) systems and shows how AI and machine learning could exacerbate this problem exponentially. Ultimately it confirms the need for States to work urgently to establish limits on autonomy in weapon systems. 16. It reinforces the ICRC’s view that States should agree on the type and degree of human control required to ensure compliance with international law and to satisfy ethical concerns, while also underlining its doubts that autonomous weapon systems could be used in compliance with international humanitarian law in all but the narrowest of scenarios and the simplest of environments. GE.21-13672(E) Group of Governmental Experts on Emerging Technologies in the Area of Lethal Autonomous Weapons System Geneva, 3-13 August, 24 September-1 October and 2-8 December 2021 Agenda Item 5 Focus of work of the Group of Governmental Experts in 2021 Proposal for consensus recommendations in relation to the clarification, consideration and development of aspects of the normative and operational framework Submitted by the International Committee of the Red Cross I. The ICRC's concerns about autonomous weapon systems 1. Autonomous weapon systems select and apply force to targets without human intervention. After initial activation or launch by a person, an autonomous weapon system self-initiates or triggers a strike in response to information from the environment received through sensors and on the basis of a generalized "target profile". This means that the user does not choose, or even know, the specific target(s) and the precise timing and/or location of the resulting application(s) of force. 2. The use of autonomous weapon systems entails risks due to the difficulties in anticipating and limiting their effects. This loss of human control and judgement in the use of force and weapons raises serious concerns from humanitarian, legal and ethical perspectives. 3. The process by which autonomous weapon systems function: • brings risks of harm for those affected by armed conflict, both civilians and combatants, as well as dangers of conflict escalation • raises challenges for compliance with international law, including international humanitarian law, notably, the rules on the conduct of hostilities for the protection of civilians • raises fundamental ethical concerns for humanity, in effect substituting human decisions about life and death with sensor, software and machine processes. II. The ICRC's recommendations to States for the regulation of autonomous weapon systems 4. The International Committee of the Red Cross (ICRC) has, since 2015, urged States to establish internationally agreed limits on autonomous weapon systems to ensure civilian protection, compliance with international humanitarian law, and ethical acceptability. CCW/GGE.1/2021/WP.6 Convention on Prohibitions or Restrictions on the Use of Certain Conventional Weapons Which May Be Deemed to Be Excessively Injurious or to Have Indiscriminate Effects 27 September 2021 English only CCW/GGE.1/2021/WP.6 2 5. With a view to supporting current efforts to establish international limits on autonomous weapon systems that address the risks they raise, including efforts by the Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Emerging Technologies in the Area of Lethal Autonomous Weapons Systems to develop consensus recommendations in relation to the clarification, consideration and development of aspects of the normative and operational framework, the ICRC recommends that States adopt new legally binding rules.1 In particular: (a) Unpredictable autonomous weapon systems should be expressly ruled out, notably because of their indiscriminate effects. This would best be achieved with a prohibition on autonomous weapon systems that are designed or used in a manner such that their effects cannot be sufficiently understood, predicted and explained. (b) In light of ethical considerations to safeguard humanity, and to uphold international humanitarian law rules for the protection of civilians and combatants hors de combat, use of autonomous weapon systems to target human beings should be ruled out. This would best be achieved through a prohibition on autonomous weapon systems that are designed or used to apply force against persons. (c) In order to protect civilians and civilian objects, uphold the rules of international humanitarian law and safeguard humanity, the design and use of autonomous weapon systems that would not be prohibited should be regulated, including through a combination of: • limits on the types of target, such as constraining them to objects that are military objectives by nature • limits on the duration, geographical scope and scale of use, including to enable human judgement and control in relation to a specific attack • limits on situations of use, such as constraining them to situations where civilians or civilian objects are not present • requirements for human–machine interaction, notably to ensure effective human supervision, and timely intervention and deactivation. 6. The ICRC’s position and its recommendations to States are based on its analyses of associated humanitarian, legal, ethical, technical and military implications of autonomous weapon systems, insights published in a series of reports, and regular engagement with States and experts at the CCW and bilaterally.2 These inform the ICRC’s recommendations on the specific limits on autonomous weapon systems that are needed to ensure civilian protection, compliance with international humanitarian law and ethical acceptability. 7. The normative limits put forward by the ICRC are informed by views expressed by many High Contracting Parties to the CCW, and other stakeholders, on the types of measures that can contribute to ensuring human control, involvement or judgement, and on the need for new legally binding rules on autonomous weapon systems. More specifically, a number of High Contracting Parties, and other stakeholders, support the prohibition of certain autonomous weapon systems and the placement of constraints or requirements on other autonomous weapon systems. 8. The ICRC is convinced that these limits should take the form of new legally binding rules that specifically regulate autonomous weapon systems. These rules should clarify how existing rules of international law, including international humanitarian law, constrain the 1 ICRC, ICRC Position on Autonomous Weapon Systems and Background Paper, Geneva, 12 May 2021, https://www.icrc.org/en/document/icrc-position-autonomous-weapon-systems. 2 Most recently: ICRC, Statement of the ICRC to Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems, Geneva, 21–25 September 2020; ICRC, Commentary on the “Guiding Principles” of the CCW GGE on “Lethal Autonomous Weapons Systems”, Geneva, July 2020; and V. Boulanin, N. Davison, N. Goussac, and M. Peldán Carlsson, Limits on Autonomy in Weapon Systems: Identifying Practical Elements of Human Control, ICRC & SIPRI, June 2020. CCW/GGE.1/2021/WP.6 3 design and use of autonomous weapon systems, and supplement the legal framework where needed, including to address wider humanitarian risks and fundamental ethical concerns raised by autonomous weapon systems. 9. Considering the speed of development in autonomous weapon systems’ technology and use, it is critical that internationally agreed limits be established in a timely manner. Beyond new legal rules, these limits may also include common policy standards and good practice guidance, which can be complementary and mutually reinforcing. To this end, and within the scope of its mandate and expertise, the ICRC stands ready to work in collaboration with States and other stakeholders at international and national levels, including representatives of High Contracting Parties to the CCW and their armed forces, the scientific and technical community, industry and civil society. CHECK AGAINST DELIVERY Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 9–13 April 2018, Geneva Statement of the International Committee of the Red Cross (ICRC) Characterization of the systems under consideration in order to promote a common understanding on concepts and characteristics relevant to the objectives and purposes of the Convention Mr Chair Purpose of characterisation The purpose of discussing characteristics may be to: · to identify a specific category of weapon systems to be regulated, or · to identify a broad category, within which a specific group should be regulated. The ICRC’s view is that the second approach will provide more effective way of identifying “autonomous weapons systems”, and tailoring regulatory or policy responses to address any legal and ethical issues. The advantages of this approach include that it: · acknowledges the reality that some autonomous weapon systems are already in use · enables experience with existing weapons to inform legal and ethical assessments, and determinations of “meaningful” or “effective” human control, and · avoids prejudging regulatory responses at the outset. It is important that discussions are reality-based, and draw on technical, operational and legal evidence from existing weapon systems with autonomy in their critical functions. Autonomy in critical functions is the key characteristic The ICRC has characterised autonomous weapon systems broadly as Any weapon system with autonomy in its critical functions. That is, a weapon system that can select and attack targets without human intervention. After launch or activation by a human operator, the weapon system – though its sensors, programming (software algorithms) and connected weapon(s) – takes on the targeting functions that would normally be controlled by humans. In other words, the weapon system self-initiates the attack. This encompasses any weapon system that can independently select and attack targets, including some existing weapons (see below), as well as potential future systems. 2 It is autonomy in the critical functions that distinguishes autonomous weapon systems from all other weapon systems and that are central to legal obligations, ethical concerns, and humanitarian consequences. Considering only “full system autonomy” is too narrow, since a weapon may have autonomy in targeting functions without having system-level autonomy. Other autonomous functions – take-off and landing, navigation, flying or driving, and control of sensors – can be excluded, as can autonomous robotic systems that are not weaponised. Lethality is not a relevant factor. It is the use of force that triggers legal obligations under international humanitarian law (and international human rights law). Technical sophistication is not a key characteristic Technical sophistication is not the defining characteristic of whether a weapon is autonomous, rather it is whether the weapon system self-initiates the attack. Therefore, notions of “automated” and “autonomous” weapons are interchangeable because they raise the same legal, ethical and humanitarian questions. This is why the ICRC intentionally includes both “dumb” and “intelligent” autonomous weapon systems. A weapon could be very simple and “unintelligent” in its design, but highly autonomous in its critical functions (e.g. a machine-gun that is triggered by a motion or heat sensor). In fact, a “dumb” autonomous weapon systems could even raise greater legal concerns, and lead to worse humanitarian consequences. In addition, predictability in programming of a weapon system does not necessarily equal predictability in consequences. Autonomous weapon systems all raise questions about predictability, owing to varying degrees of uncertainty as to exactly when, where and/or why a resulting attack will take place. Autonomy in existing and future systems Autonomy in the critical functions is found in existing anti-materiel weapons that attack objects (e.g. counter rocket, artillery and mortar systems; vehicle “active protection” systems; and some loitering munitions). However, autonomy in the critical functions is a feature that could, in theory, be applied to any weapon system, including existing remote controlled weapon systems. There is a trend towards increasing autonomy in the wide range of robotic weapon platforms in the air, on land and at sea. Future developments could include shifts: · from anti-materiel to anti-personnel systems · from static (fixed) to mobile systems, that “search” for targets over an area · from use in armed conflict to use in law enforcement operations, and · use of autonomous target identification systems as “decision aids” not directly connected to weapon systems. Thank you. CHECK AGAINST DELIVERY Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 9–13 April 2018, Geneva Statement of the International Committee of the Red Cross (ICRC) The International Committee of the Red Cross (ICRC) is pleased to contribute its views to this second meeting of the Group of Governmental Experts on “Lethal Autonomous Weapon Systems”. The ICRC wishes to acknowledge and thank the Chair for leading these discussions and wish him success in his efforts this week. Building on the valuable work done to date, this meeting provides an opportunity to consider in more detail the complex challenges posed by autonomous weapon systems. We are hopeful that the meeting will advance progress towards a common understanding of the characteristics of these weapons. Setting a broad scope of the discussion will be important in this regard. In the ICRC’s view, the purpose of identifying the characteristics of the systems under consideration should be to identify the features that distinguish autonomous weapon systems from those controlled directly by humans, including remote-­controlled weapons. The objective at the outset need not be to define systems of concern. From the ICRC’s perspective, the focus must be on the functions of weapon systems that are most relevant to legal obligations and ethical concerns within the scope of the Convention on Certain Conventional Weapons, namely autonomy in the critical functions of selecting and attacking targets. Autonomy in other functions (such as movement or navigation) would not in our view be relevant to the discussions. Experiences with existing weapon systems with autonomy in their critical functions should be harnessed for a greater understanding of the legal and ethical issues raised by autonomy in weapon systems more generally, and of the nature of human control over the use of force that must be retained for legal compliance and ethical acceptability. For the ICRC, it is crucial that discussions draw on real-­world technologies and near-­term developments of autonomy in weapon systems. To do otherwise would risk overlooking incremental developments in autonomy that raise concerns and that may require legal and policy responses from the international community. The degree of human control over weapon systems – not their technological sophistication – should be the yardstick in these discussions. It is humans – not machines, computer programs or weapon systems – who apply the law and are obliged to respect it. International humanitarian law (IHL) requires that those who plan, decide upon and carry out attacks make certain judgements in applying the norms when launching an attack. Ethical considerations parallel this requirement – demanding that human agency and intention be retained in decisions to use force. Humans therefore bear responsibilities in the programming, development, activation and operational phases of autonomous weapon systems. Mindful of the potential adverse humanitarian consequences of the loss of human control over weapons and the use of force, the ICRC has posited that a minimum level of human control is necessary from both a legal and ethical perspective. In the view of the ICRC, a weapon system beyond human control would be unlawful by its very nature. But beyond these so-­called “fully autonomous weapon systems”, there is a need consider the full range of risks associated with weapon systems that have autonomy in their critical functions. The ethical concerns around loss of human agency in decisions to use force, diffusion of moral responsibility and loss of human dignity could have far-­reaching consequences, perhaps precluding the development and use of anti-­personnel autonomous weapon systems, and even limiting the application of anti-­matériel systems, depending on the risks that destroying matériel targets present for human life. The ICRC continues to urge all States in this meeting to elaborate what “meaningful” or “effective” human control entails in practice. States must also address fundamental concerns about weapon systems that may introduce inherent unpredictability, such as those employing artificial intelligence (AI) machine-­learning algorithms. The affirmations by CCW States Parties in this Group that IHL is both relevant and applicable to any emerging weapon technology, including autonomous weapon systems, is heartening. The ICRC welcomes efforts to improve implementation of IHL, including through enhancing mechanisms to review the legality of new weapons. Conducting legal reviews of weapon systems with autonomy in their critical functions can raise challenges, in particular regarding standards of predictability and reliability. The ICRC encourages the exchange of information and experiences about these processes, and views such efforts as complementary to the work of the Group of Governmental Experts. This being said, the ICRC remains convinced that the overall purpose of this Group should be to agree limits on autonomy in weapon systems. As noted by the ICRC in previous meetings on this topic, technological developments that remove or reduce direct human control over weapon systems are threatening to outpace international deliberations, and States must therefore approach this task with some urgency. A “human-­centred” approach must guide the identification of limits to autonomy in weapon systems and of possible options to address “autonomous weapon systems of concern”. We look forward to elaborating further on our views during the thematic sessions. Thank you. Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 25-29 March 2019, Geneva Statement of the International Committee of the Red Cross (ICRC) Agenda item 5(a) – An exploration of the potential challenges posed by emerging technologies in the area of lethal autonomous weapon systems to international humanitarian law Thank you, Mr Chair, for bringing this element of our discussions to the fore, particularly in view of the historic link between the Convention on Certain Conventional Weapons and the fundamental rules of international humanitarian law. In the ICRC’s view, the challenges posed to IHL by autonomous weapon systems are inextricable from the issue of the human element in the use of force (which will be discussed this afternoon under agenda item 5(b)). For the ICRC it is clear that the law is addressed to humans, and the relevant legal obligations under international humanitarian law (IHL) – notably the rules of distinction, proportionality and precautions in attack – rest with those who plan, decide on, and carry out attacks. As many of the States who have spoken so far have pointed out, it is humans, not machines, that comply with and implement the law, and it is humans who will be held accountable for any violations of IHL. These legal obligations, and associated judgements, cannot be transferred to a machine, computer program, or weapon system. As a result, combatants will require a level of human control over weapon systems and the use of force so that they can make context-specific legal judgements in specific attacks. It is the loss of human control over the use of force inherent in the use of autonomy in the critical functions of weapon systems that poses a challenge to IHL, particularly the rules regulating the conduct of hostilities. Addressing this challenge necessitates a close examination of the rules to determine exactly what they require, whether existing law is sufficiently clear or whether there is a need to clarify IHL or to develop new rules, or standards. For this reason, the ICRC welcomes the opportunity of this morning’s session to speak about the rules of IHL that demand a level of human control. In this afternoon’s session, we will discuss the key elements of human control required by IHL as we see them. IHL rules demand context-based decisions Mr Chair, the demands by IHL for context-specific legal judgments by those who plan and decide on attacks limits the degree of autonomy that is permitted under exiting IHL. In other words, limitations on autonomy in the critical functions of weapon systems can be deduced from existing IHL rules. The rules of distinction, proportionality and precautions require complex assessments based on the circumstances prevailing at the time of the decision to attack, but also during an attack. 2 For example, the rule on distinction refers to the obligations on parties to an armed conflict to always distinguish between civilians and civilian objects on the one hand, and military objectives on the other, and not to direct attacks against the former. Related is the prohibition on indiscriminate attacks. Both of these rules demand that parties characterise as military objectives the person and/or objects at whom an attack is intended to be directed. Such characterisations are, by their nature, qualitative and variable. By way of illustration, the definition of the term ‘military objective’ – set out in article 52(2) of Additional Protocol I – requires an assessment of whether the object by its nature, location, purpose or use ‘makes an effective contribution to military action and whose partial or total destruction, capture or neutralisation, in the circumstances ruling at the time, offers a definite military advantage’. Assuming that an attack is planned to be directed at a lawful target, the rule of proportionality would prohibit such an attack where it may be expected to cause incidental harm that would be excessive compared to the concrete and direct military advantage anticipated. The assessment of proportionality is generally measured against what a reasonable person would conclude in the circumstances, making use of available information. This rule provides a further example of the qualitative and variable nature of the legal judgments required by IHL. The obligation to take feasible precautions in attack relies on these same qualitative judgements, and generally requires those who plan or decide on attacks to take constant care to spare the civilian population, individual civilians and civilian objects, including when choosing between several possible targets, or when choosing their means and methods of warfare. To conclude, Mr Chair, adherence to IHL rules of distinction, proportionality and precautions clearly requires evaluative assessments based on knowledge of context, including the environment of use and the expected effects of the weapon. These context-based assessments must be made by humans. Military lawyers themselves reject the suggestion that the proportionality calculus, for example, can be reduced to objective indicators, insisting that their evaluations are dependent on the circumstances of particular situations and the good faith of military commanders. Context-based decisions demand human control It’s important to add, Mr Chair, that assessments of distinction, proportionality and precautions made by combatants must be reasonably proximate in time to the attack (or “strike”). Where these assessments form part of planning assumptions, these assumptions must have continuing validity until the execution of the attack in order to comply with IHL. This mean that when using autonomous weapon systems commanders must retain a level of human control over weapon systems sufficient to allow them to make context-specific legal judgments in carrying out attacks in armed conflict. In other words, they must retain a level of human control that is ‘meaningful’ (or ‘substantive’, ‘appropriate’, or ‘effective’, or ‘appropriate levels of human judgment’). 3 Human control at different stages Keeping in mind the IHL requirements for context-based decisions made in reasonable proximity to the attack, and the human control that these requirements demand, we now wish to turn to the different forms that human control can take during the development, activation, and operation of an autonomous weapon system: 1. Development and testing (‘development stage’); 2. Decision by the commander, or operator, to activate the weapon system (‘activation stage’); and 3. the operation of the weapon system during which it independently selects and attacks targets (‘operation stage’). Human control at all three stages, in design (development stage) and in use (activation and operation stages), is essential for compliance with IHL. The International Panel on the Regulation of Autonomous Weapons (iPRAW) has reached similar conclusions on the need for “control by design” and “control in use”. Ensuring human control in use, at the activation and operation stages, is the most important for compliance with IHL rules on the conduct of hostilities. Human control in design, at the development stage, provides a means to set and test control measures that will ensure human control in use. Human control at the development stage alone – control in design – will not be sufficient to ensure compliance with IHL in the use of an autonomous weapon system for attacks in armed conflict given the inherently variable and unpredictable nature of real-world operational environments. Human control, and judgement, for compliance with IHL must be proximate to the use of force in a specific attack, and cannot be substituted with software control. In the view of the ICRC, concepts of “human control in the wider loop” and the use of autonomy to “effectuate the intention of commanders and the operators of weapons systems” do not adequately capture the requirement for human control under IHL. Elements of human control Mr Chair, having addressed the legal basis for human control, this afternoon we will discuss the three key and inter-related elements of human control as we see them, namely: 1. human supervision and the ability to interevent/deactivate 2. predictability and reliability, and 3. operational constraints. To be clear, Mr Chair, and to repeat what we said at the outset of this intervention, the use of autonomous weapon systems brings a loss of human control that entails serious risks for protected persons in armed conflict (both civilians and combatants no longer fighting) and of violations of IHL. This unique characteristic of autonomous weapon systems, raises difficulties in the interpretation and application of IHL rules and, ultimately, it raises the question of whether existing law is sufficiently clear or whether there is a need to clarify IHL or to develop new rules, or standards. This is a question we will return to in this afternoon’s session. Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 25-29 March 2019, Geneva Statement of the International Committee of the Red Cross (ICRC) under agenda item 5(b) further consideration of the human element in the use of lethal force; aspects of human-machine interaction in the development, deployment and use of emerging technologies in the area of lethal autonomous weapon systems In our last intervention, the ICRC explained the legal basis for human control, that is, the limits on autonomy that can be deduced from IHL rules. In the ICRC’s view, ethical considerations also demand human control over weapon systems and the use of force. As you know, the ICRC is convinced that human intention and agency – the human actor – must be sufficiently retained in decisions to use force. Moral responsibility for these decisions cannot be delegated to a machine, no more than legal responsibility can. With the legal and ethical bases of human control in mind, the ICRC wishes to dedicate its intervention this afternoon to the key elements of human control demanded by legal and ethical considerations (noting that ethical concerns might demand additional limits on autonomy – an issue we have spoken about before). These key elements are: 1. Human supervision and the ability to intervene and deactivate 2. Predictability and reliability, and 3. Operational constraints. 1. Human supervision, and ability to intervene and deactivate Human supervision, and the ability to adapt to changing circumstances, is essential to ensure compliance with IHL, including when carrying out an attack with a weapon system with autonomy in its critical functions. This requires supervision of both the weapon system and the target area, in other words situational awareness. Generally speaking, constant human supervision of the weapon system and the target area may be required, so that the operator has sufficient information and understanding about the operation of the weapon system, the environment of use, and the interaction of the two, over the given time period and geographical area. This information is necessary for making the context-based legal judgments required by IHL. A physical and/or communication link that permits adjustment of the engagement criteria and the ability to cancel the attack, as well as sufficient time for such intervention, is necessary if the supervision is to serve its purpose of ensuring compliance with IHL. Without human supervision – and ability to intervene and deactivate – it is difficult to envisage how operators/commanders could take into account changes in the situation and thereby exercise the legal judgements required by IHL in carrying out attacks. IHL requires the attacker to maintain awareness of, and adapt to, continuously changing circumstances, even after the decision to attack. A party to the conflict must do everything feasible to cancel or suspend an attack if it becomes apparent that the target is not a military objective, or that the attack may be expected to cause incidental loss of civilian life, injury to civilians, damage to civilian objects, or a combination thereof which would be excessive in relation to the concrete and direct military advantage anticipated (API article 57(2)(b), CIHL rule 19). The practical effect of this rule is that weapon systems must remain under human supervision, and must permit the user to, where feasible, cancel, suspend or modify attacks, up until the execution of the attack (or halting of the attack). Feasibility means what is practicable or practically possible, taking into account all the circumstances at the time, including humanitarian and military considerations. The existence of autonomy in the critical functions of a weapon system may render precautionary measures unfeasible. Precautions will not always be precluded when using an autonomous weapon system, but the ICRC is interested to hear from States about how they see the interaction between autonomy and feasibility and precautions. In the ICRC’s view, the use of an autonomous weapon system that does not permit the taking of precautions such as cancelling or suspending an attack – in situations where there is a reasonable likelihood that the circumstances will change enough to render an attack unlawful – would likely be unlawful. 2. Predictability and reliability Autonomous weapon systems, since they self-initiate attacks, all raise concerns about unpredictability, owing to varying degrees of uncertainty about location, timing and/or nature of the subsequent machine-initiated attack. Predictability (knowledge of the consequences of use) and reliability (likelihood of failure) are dependent on the: 1 weapon system design (including the software and algorithms that control the system); task it is used for; nature of the environment where it is used; and interaction between the system and the environment. All autonomous weapon systems, which operate based on interaction with their environment, raise questions about human control and predictability. The greater the complexity of the environment and complexity of the task, the greater the need for human control and the less tolerance of autonomy, from both a legal and ethical perspective. 1 Predictability is the ability to “say or estimate that (a specified thing) will happen in the future or will be a consequence of something”. Applied to an autonomous weapon system, predictability is knowledge of how it will function in the circumstances of use, and the effects that will result. Reliability is “the quality of being trustworthy or performing consistently well”. In this context, reliability is knowledge of how consistently the machine will function as intended, e.g. without failures or unintended effects. Humans can exert some control over autonomous systems through “human on the loop” interaction with them. However, this is not a panacea due to human-machine interaction problems, such as automation bias, over-trust in the system, or lack of operator awareness of the system state at the time of intervention. Further, there are challenges in quantifying the level of predictability (and reliability) needed to ensure a level of human control, or judgement, sufficient for IHL compliance (and ethical acceptability). Testing also raises unique challenges since it is not possible to test all possible environmental inputs to an autonomous system. In order to comply with IHL, those who plan or decide on an attack using an autonomous weapon system must understand its capabilities and limitations in the given circumstances, in order to determine whether it will perform lawfully in the given circumstances. This also requires knowledge of the environment over time (see above on human supervision). In general, IHL demands a high level of certainty about the prevailing circumstances and the effect of the chosen means and methods of warfare. One example of this is the rule on the loss of civilian protection in attack. During an attack, doubt as to status of a person must be resolved in favour of treating the individual as a civilian. It is challenging to pinpoint exactly where IHL lines will be crossed with respect to predictability and reliability. The ICRC would be interested to hear from States what levels of predictability/reliability are demanded by IHL. Indeed, the absence of clarity on this issue is one of the reasons that we have called for States to set internationally agreed limits on autonomy in weapon systems. A weapon system that is unpredictable by design would be unlawful by its nature. Let us explain what we mean by that. Unpredictability of a weapon system’s interaction with its environment will inevitably create risks for protected persons and objects in that environment. There are fundamental concerns about autonomy in critical functions controlled by machinelearning algorithms, since these are generally unpredictable in their functioning, not transparent (i.e. they are “black boxes” and their functioning cannot be explained), and their performance therefore cannot be verified during testing. Not only are such algorithms unpredictable, but they can also introduce bias, whether by design or through bias in the data used to “train” (develop) the algorithms. This kind of ‘unpredictability by design’ would raise concerns in all situations, as the person who plans or decides on an attack cannot have reasonable certainty about the effects of the weapon. Any weapon systems with autonomy in critical functions whose software can set its own goals, or learn, change or adapt its functioning after deployment, would be inherently unpredictable, and therefore beyond human control and unlawful under IHL. This is because any assessment that the user has made at the moment of activating the weapon to ensure compliance with IHL would immediately become invalid after deployment. Self-organising swarms used as autonomous weapons could raise similar concerns due to their inherent unpredictability of their behaviour in the environment. 3. Operational constraints Operational parameters and constraints are important for human control in particular the: task the weapon is used for; types of targets it attacks, type of force (and effects) it employs; operating environment; duration of autonomous operation (time-frame); and scope of movement over and area (mobility). An autonomous weapon system must be capable of being used – and must be used – in accordance with existing rules of IHL - notably the rules of distinction, proportionality and precautions in attack, which require complex context-based assessments by commanders based on the circumstances prevailing at the time of the attack. As the ICRC explained in our intervention this morning, these assessments must be reasonably proximate in time to the attack (or “strike”) to comply with these rules. Autonomy in the critical functions of weapons complicates the ability of the commander, to make these context specific judgements since, upon activation, they do not know the timing, location and nature of the subsequent attack(s) that the weapon will self-initiate. The lawfulness of use relies on the continuing validity of IHL assessments and planning assumptions made at the point of activation. Whether an autonomous weapon system will operate within the constraints of IHL once activated will depend on the technical performance of the specific weapon, especially its predictability and reliability (see above), and the specific circumstances and environment of use. Predictability in the consequences of its use will depend not only on the technical design of the system, but on variations in the environment over time and the interaction of the system with that environment, taking into account the task it is used for. The more variable the environment of use, or the longer the timespan between the human decision to activate the weapon and the eventual use of force initiated by the weapon system in response to the environment, the greater the risk of IHL violations. The risk that IHL might be violated can be reduced by manipulating operational parameters like the environment in which the weapon system is to operate, the mobility of the weapon system in space and the time-frame of its operation, in order to increase predictability in the consequences of its use. The more predictable (non-dynamic) the environment, and the more highly constrained the system is in time and space, the greater predictability in the consequences of activating the system. The nature of the spatial/temporal limitations required by IHL is one on which further clarity would be useful. Reaching agreement on the kinds of operational constraints necessary as part of human control for IHL compliance (and ethical acceptability) could be a part of States’ efforts to set limits on autonomy in weapon systems. However, given the high degree of unpredictability of most real world conflict environments, it is likely that operational constraints alone will only help avoid an unacceptable risk of IHL violations in the narrowest of circumstances, and will generally not be sufficient to ensure IHL compliance with IHL in carrying out an attack with an autonomous weapon system. Mr Chair, this intervention has already been rather long, so we will conclude there and look forward to contributing to the discussions in the coming days. Thank you. 1 Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 25-29 March 2019, Geneva Statement of the International Committee of the Red Cross (ICRC) under agenda item 5(e) Possible options for addressing the humanitarian and international security challenges posed by emerging technologies in the area of lethal autonomous weapon systems in the context of the objectives and purposes of the Convention without prejudicing policy outcomes and taking into account past, present and future proposals Thank you Mr. Chairperson, As we have discussed this week, autonomous weapon systems raise concerns about loss of human control over the use of force, which could have serious humanitarian consequences, in terms of adverse consequences for both civilians and combatants in armed conflict. As the ICRC expressed yesterday, these unique characteristics of autonomous weapon systems – namely the loss of the ability of combatants to exercise the context-specific judgments required of international humanitarian law (IHL) rules, and the loss of human agency and diffusion of moral responsibility in decisions to use force -- raise challenges for IHL compliance and for ethical acceptability and, therefore, raise the question of whether new internationally agreed policies, standards or rules are needed. The common ground on which States can build, and have been building in their interventions this week, is their agreement in the 2018 GGE that human responsibility – or control – must be retained over weapon systems and the use of force. As the ICRC has called for in its Working Paper on the “Element of Human Control” submitted to the CCW Meeting of High Contracting Parties last November (CCW/MSP/2018/WP.3), this “human-centred” approach must guide the development of limits on autonomy in weapon systems, and in particular the practical elements of human control over the critical functions of weapon systems needed for legal compliance and ethical acceptability. The Provisional Programme of Work lists a number of options that have been proposed for addressing the humanitarian and international security challenges posed by autonomous weapon systems. These options, which are “not necessarily mutually exclusive”, include: a legally binding instrument; a political declaration; guidelines, principles or codes of conduct; and improving implementation of existing legal requirements, including national legal reviews of new weapons. As the ICRC stated in its Working Paper, all of these approaches share the same need to develop common understandings of the type and degree of human control necessary in practice to ensure compliance with IHL, and ethical acceptability. This effort must be driven by the necessity to preserve human judgement and responsibility in targeting decisions, where human supervision, predictability and context are important factors. 2 Working on the parameters of a positive obligation for human control will also enable States to identify “autonomous weapon systems of concern” that fall outside of effective human control, and would therefore be unlawful and/or ethically unacceptable. Insofar as the sufficiency of existing law – and in particular of IHL – is concerned, it is clear, as the ICRC submitted again yesterday, that existing IHL rules – in particular distinction, proportionality and precautions required in attack – already provide some limits to autonomy in weapon systems, insofar as they establish responsibilities for combatants, who must retain the ability to make the contextspecific assessments required of IHL rules during the operation of the weapon system. We described the IHL limits in more detail in our statement. However, it is also clear that existing IHL rules – while already limiting autonomy in weapons to a certain degree – do not provide all the answers. Key questions include:  What is the type and degree of human control over weapon systems, including the level of human supervision and spatial and temporal constraints on their operation, required to ensure compliance with IHL rules?  What are the minimum levels of predictability (or maximum levels of unpredictability) that are tolerable in the functioning of weapon systems, to ensure compliance with IHL rules? Debates in the GGE this week and in the last two years have shown that there are serious questions whether existing IHL is sufficiently clear, or whether there is a need to clarify IHL or to develop new rules – i.e. new law. This again points to the urgent need to, at minimum, reach common understandings on the practical elements of human control. The ICRC has welcomed the attention that this discussion has brought to improving and developing legal review processes. Effective legal reviews are critical to ensuring that a State’s armed forces comply with IHL in light of rapid technological developments. Greater transparency in how States interpret and apply their obligation to carry out legal reviews of new weapons to new technologies can help to identify good practices and thus assist States seeking to comply with their legal review obligations, particularly when reviewing autonomous weapon systems, whose unique characteristics can make legal reviews more difficult. But the bottom line is that while robust legal reviews remain essential, they are not a substitute for States working towards internationally agreed limits on autonomy in weapon systems. Moreover, the limits dictated by ethical considerations may go beyond those found in existing IHL rules. In particular, ethical concerns have been expressed about the loss of human agency in decisions to use force, diffusion of moral responsibility and loss of human dignity are most acutely felt with autonomous weapon systems that present risks for human life, and especially with the notion of antipersonnel systems, that is, those designed to target humans directly. As a result, public conscience may demand limits or prohibitions on particular types of autonomous weapons – such as anti-personnel systems – and/or their use in certain environments – such as where civilians and civilian objects are present. Perhaps these ethical boundaries are already evident with existing autonomous weapons, which generally only target objects and not humans, and are used in very narrow circumstances for defensive 3 purposes in environments where there are few civilians. And even in these narrow circumstances, human supervision and ability to intervene and deactivate is retained. Regarding a proposed ban on so-called “fully autonomous weapon systems” -- such as weapons that are unsupervised, unpredictable and unconstrained in time and space (that is, beyond any human control) -- the ICRC notes that many States are of the view that such weapons would almost certainly be unlawful by their very nature (a view that it shares), and many have expressly stated they have no intention of developing and using such weapons. Yet the ongoing militarization and weaponization of autonomous technologies demands a standard of human control that is relevant to current and emerging developments as well as to future technological and operational developments. Most States appear to agree that the yardstick against which the legal and ethical acceptability of all existing and future weapon systems with autonomy in their critical functions must be assessed is meaningful (effective or significant) human control, or put another way, ensuring appropriate levels of human judgment in decisions to use force. However, the question remains: what would be the practical elements (or criteria) of this positive obligation of human control? In terms of articulating a standard of human control, and therefore of limits on autonomy in weapon systems and the use of force, could a requirement for human control (responsibility) take one of these two possible approaches?  A requirement for direct or remote human control over all weapons i.e. that would generally exclude autonomous weapon systems, with specific exceptions? Or  A requirement for meaningful / effective / substantive human control or judgment over all weapons with autonomy in their critical functions, with specific prohibitions? In terms of the type and degree of human control that would be needed to comply with IHL and satisfy ethical concerns:  Must there be constant human supervision, with the ability to intervene and de-activate, for the duration of the weapon’s operation? And what would be the exceptions, if any, to this requirement?  Must there be a requirement that the human operator be able to predict, with a high degree of certainty, that the weapon will attack a specific target at a specific point in time, and its effects? And what would be the exceptions, if any, to such requirement?  What are the standards of reliability of weapons with autonomy in their critical functions, also as regards testing?  Assuming that autonomous weapons that are designed to select and attack materiel targets (objects) are acceptable, what operational constraints apply, in particular to the environment in which they operate (e.g. populated or unpopulated area), the duration (time-limit) of their operation? What would be the spatial and temporal limits that would be applied to mobile (as opposed to stationary) systems? The ICRC believes that these key questions can help guide efforts to identify a standard of human control that is clear, robust and practical, and that withstands the test of time. 4 Thank you. CCW Meeting of Experts on “LAWS”, 17 April 2015 Closing statement by the International Committee of the Red Cross (ICRC) Thank you Mr. Chairman, The ICRC very much welcomes broad agreement of the need for continued discussions on autonomous weapon systems at the CCW next year. We remain of the view that incrementally increasing autonomy in weapons systems – specifically in the critical functions of identifying, selecting, and attacking targets – raises fundamental questions about human control over the use of force. In our view, future discussions at the CCW should address: whether these developments may affect the ability of parties to armed conflicts to respect international humanitarian law; and whether these developments are acceptable under the principles of humanity and the dictates of public conscience. Discussions this week have highlighted, once again, broad agreement on the need to retain human control over the critical functions of weapon systems. To move forward, there is now a need for States to address where the limits should lie, in order to ensure that meaningful, adequate and effective human control is maintained over weapon systems. The work of the CCW in the next year should focus on this objective. Mr. Chairman, The ICRC welcomes the wide recognition by States that legal reviews at the domestic level of autonomous weapon systems, as with any new weapon, are required under Article 36 of Additional Protocol I to the Geneva Conventions or under customary international humanitarian law. We wish to recall that this recognition is not new to States party to the CCW. Indeed, past Review Conferences have recalled the importance of legal reviews of new weapons in connection with the CCW’s work. We encourage States to share their experience and outcomes of legal reviews of existing weapons with autonomy in their critical functions. In this respect, we also welcome proposals on sharing good practices for legal review mechanisms in order to improve implementation of the said legal obligations. However, as we have stated previously, these efforts to encourage implementation of national legal reviews are not a substitute for States party to the CCW to consider possible policy and other options at the international level to address the legal and ethical limits to autonomy in weapon systems. Rather these two approaches are very much complimentary. 1 ICRC statement to concluding session CCW Expert Meeting on "Lethal Autonomous Weapon Systems" 16 May 2014 Thank you Mr. President. As the ICRC has previously stated here and elsewhere, the development of weapons with increasingly autonomous functions – i.e. with autonomy in the "critical functions" of identifying and attacking (using force) against targets – has profound implications for the future of warfare and indeed of humanity. We are therefore very pleased that the delegations that have taken the floor in this session seem to agree that discussions on this issue in the CCW should continue through 2015. We would like to highlight a few key issues that have emerged this week, which could help frame future discussions on autonomous weapon systems. Regarding the overall scope of the discussions, we are convinced of the need to ground the discussions in how weapon technology is developing rather than attempting to predict the future. In particular, there is a need to look at increasing autonomy in weapon systems, and specifically at autonomy in the critical functions of acquiring, tracking, selecting and attacking targets. This is an incremental process and not a sudden development, which gives us an opportunity to look carefully at this trend and consider the implications before developments come to fruition. At some point on an incremental process of increasing autonomy in the critical functions of weapon systems, human control may no longer be meaningful. As we mentioned in our opening statement, we believe the crucial aspect is human control over the use of force, and what constitutes meaningful, appropriate and responsible human control over the critical functions of weapon systems. Where humans are so far removed in time and space from control over the weapon system, the human decision-making process on the use of force may in effect be substituted with machine decision-making. Regarding the "Martens Clause", which several delegations have raised in their interventions in relation to the acceptability of autonomous weapons, we believe it is useful to recall that the "Martens Clause" refers to a provision found in several international humanitarian law treaties, starting with the preambles of Hague Conventions II of 1899 and IV of 1907. It's modern incarnation is Article 2(1) of Additional Protocol I of the Geneva Conventions, which reads: In cases not covered by international humanitarian law], civilians and combatants remain under the protection and authority of the principles of international law derived from established custom, from the principles of humanity and from dictates of public conscience. (It is also found in the preamble of Additional Protocol II.) The International Court of Justice, in its 1996 Advisory Opinion on the Legality of the Threat or Use of Nuclear Weapons, affirmed that the applicability of the Martens Clause "is not to be doubted" and that it had "proved to be an effective means of addressing rapid evolution in military technology. 2 The Court also found that the Martens Clause represent customary international law – in other words, it is legally binding on all States, not just those that are party to the treaties I have just mentioned. Therefore, a weapon that is not covered by existing rules of international humanitarian law would be considered contrary to the Martens Clause if it is determined per se to contravene the principles of humanity or the dictates of public conscience. Admittedly, some dispute this interpretation. But it is useful to recall the interpretation of the ICRC Commentary to the Additional Protocols: "the Martens clause prevents the assumption that anything which is not explicitly prohibited by the relevant treaties is therefore permitted". Furthermore, "it should be seen as a dynamic factor proclaiming the applicability of the principles mentioned regardless of subsequent developments of types of situation or technology". In other words, at minimum the Martens Clause provides that the acceptability of new technologies of warfare can be judged against the principles of humanity and the dictates of public conscience. Mr. President, Many delegations this week stressed the importance of legal reviews of autonomous weapons. The ICRC welcomes the wide recognition of the need to carry out thorough legal reviews of the new technologies of warfare they are developing or acquiring, including weapons that have autonomy in their critical functions. Such legal review must determine whether weapons with autonomy in their critical functions are capable of being used, in some or all circumstances (to use the terms of Article 36 of Additional Protocol I) in accordance with international humanitarian law. They must also take into account the compatibility of the weapon with the principles of humanity and the dictates of public conscience (the "Martens Clause"). In the context of discussions in the framework of the CCW, we encourage States to be as transparent as possible in sharing national experiences of legal reviews of weapons with autonomous features. Lessons from the review of autonomy in existing weapons could provide a guiding framework for legal reviews of weapons with increasing levels of autonomy in their critical functions. At the same time, it is clear that the issues and questions raised by autonomous weapon systems cannot be addressed solely through national processes. Multilateral discussions, such as those that have taken place here, must continue in order to address the issues identified this week. Thank you. Convention on Certain Conventional Weapons (CCW) Meeting of Experts on Lethal Autonomous Weapons Systems (LAWS), 13-16 May 2014, Geneva Statement of the International Committee of the Red Cross (ICRC) 13 April 2015 Thank you Mr Chairman. The International Committee of the Red Cross (the ICRC) is pleased to contribute its views to this second CCW Meeting of Experts on “Lethal Autonomous Weapon Systems”. The CCW, which is grounded in international humanitarian law (IHL), provides an important framework to further our understanding of the technical, legal, ethical and policy questions raised by the development and use of autonomous weapon systems in armed conflicts. This week will provide an opportunity to build on last year’s meeting to develop a clearer understanding of the defining characteristics of autonomous weapon systems and their current state of development, so as to identify the best approaches to addressing the legal and ethical concerns raised by this new technology of warfare. We will have the opportunity to comment in more detail during the thematic sessions but at the outset we would like to highlight a few key points on which the ICRC believes attention should be focused this week. We first wish to recall that the ICRC is not at this time calling for a ban, nor a moratorium on “autonomous weapon systems”. However, we are urging States to consider the fundamental legal and ethical issues raised by autonomy in the ‘critical functions’ of weapon systems before these weapons are further developed or deployed in armed conflicts. We also wish to stress that our thinking about this complex subject continues to evolve as we gain a better understanding of current and potential technological capabilities, of the military purposes of autonomy in weapons, and of the resulting legal and ethical issues raised. To ensure a focussed discussion, the ICRC believes that it will be important to have a clearer common understanding of what is the object of the discussion, and in particular of what CHECK AGAINST DELIVERY 2 constitutes an autonomous weapon system. Without engaging in a definition exercise, there is a need to set some boundaries for the discussion. As the ICRC proposed at last year’s CCW Meeting of Experts, an autonomous weapon system is one that has autonomy in its ‘critical functions’, meaning a weapon that can select (i.e. search for or detect, identify, track) and attack (i.e. intercept, use force against, neutralise, damage or destroy) targets without human intervention. We have suggested that it would be useful to focus on how autonomy is developing in these ‘critical functions’ of weapon systems because these are the functions most relevant to ‘targeting decision-making’, and therefore to compliance with international humanitarian law, in particular its rules on distinction, proportionality and precautions in attack. Autonomy in the critical functions of selecting and attacking targets also raise significant ethical questions, notably when force is used autonomously against human targets. The ICRC believes that it would be most helpful to ground discussions on autonomous weapon systems on current and emerging weapon systems that are pushing the boundaries of human control over the critical functions. Hypothetical scenarios about possible developments far off in the future may be inevitable when discussing a new and continuously evolving technology, but there is a risk that by focussing exclusively on such hypothetical scenarios, we will neglect autonomy in the critical functions of weapon systems that actually exist today, or that are currently in development and intended for deployment in the near future. From what we understand, many of the existing autonomous weapon systems have autonomous ‘modes’, and therefore only operate autonomously for short periods. They also tend to be highly constrained in the tasks they are used for, the types of targets they attack, and the circumstances in which they are used. Most existing systems are also overseen in real-time by a human operator. However, future autonomous weapon systems could have more freedom of action to determine their targets, operate outside tightly constrained spatial and temporal limits, and encounter rapidly changing circumstances. The current pace of technological change and military interest in autonomy for weapon systems lend urgency to the international community’s consideration of the legal and ethical implications of these weapons. As the ICRC has stressed in the past, closer examination of existing and emerging autonomous weapon systems may provide useful insights regarding what level of autonomy and human control may be considered acceptable or unacceptable, and under which CHECK AGAINST DELIVERY 3 circumstances, from a legal and ethical standpoint. In our view, this would allow for more informed deliberations. Mr. Chairman, Based on what we have heard this morning and on the discussions that took place last year in the CCW and elsewhere, there appears to be broad agreement among States on the need to retain human control over the critical functions of weapon systems. States should now turn their attention to agreeing a framework for determining what makes human control of a weapon meaningful or adequate. Discussions should focus on the types of controls that are required, in which situations, and at which stages of the process – programming, deployment and/or targeting (selecting and attacking a target). It is also not disputed that autonomous weapons intended for use in armed conflict must be capable of being used in accordance with international humanitarian law (IHL), in particular its rules of distinction, proportionality and precautions in attack. Indeed, weapons with autonomy in their critical functions are not being developed in a ‘legal vacuum’, they must comply with existing law. Based on current and foreseeable robotics technology, it is clear that compliance with the core rules of IHL poses a formidable technological challenge, especially as weapons with autonomy in their critical functions are assigned more complex tasks and deployed in more dynamic environments than has been the case until now. Based on current and foreseeable technology, there are serious doubts about the ability of autonomous weapon systems to comply with IHL in all but the narrowest of scenarios and the simplest of environments. In this respect, it seems evident that overall human control over the selection of targets and use of force against them will continue to be required. In discussions this week, we encourage States that have deployed, or are currently developing, weapon systems with autonomy in their critical functions, to share their experience of how they are ensuring that these weapons can be used in compliance with IHL, and in particular to share the limits and conditions imposed on the use of weapons with autonomous functions, including in terms of the required level of human control. Lessons learned from the legal review of autonomy in the critical functions of existing and emerging weapon systems could help to provide a guiding framework for future discussions. CHECK AGAINST DELIVERY 4 In this respect, the ICRC welcomes the wide recognition of the obligation for States to carry out legal reviews of any new technologies of warfare they are developing or acquiring, including weapons with autonomy in some or all of their critical functions. CCW Meetings of States Parties and Review Conferences have in the past recalled the importance of legal reviews of new weapons, which are a legal requirement for States party to Additional Protocol I to the Geneva Conventions. The ICRC encourages States that have not yet done so to establish weapons review mechanisms and stands ready to advise States in this regard. In this respect, States may wish to refer to the ICRC’s Guide to the Legal Review of New Weapons, Means and Methods of Warfare. Finally Mr. Chairman, the ICRC wishes to again emphasise the concerns raised by autonomous weapon systems under the principles of humanity and the dictates of public conscience. As we have previously stated, there is a sense of deep discomfort with the idea of any weapon system that places the use of force beyond human control. In this respect, we would like this week to hear the views delegations on the following crucial question for the future of warfare, and indeed for humanity: would it be morally acceptable, and if so under what circumstances, for a machine to make life and death decisions on the battlefield without human intervention? We will be pleased to elaborate on our views further during the thematic sessions. Thank you. ICRC statement, CCW GGE “LAWS”, Monday 25 March 2019 CHECK AGAINST DELIVERY 1 Agenda Item 5 (c): CHARACTERISATION. The importance of critical functions • ICRC has characterised autonomous weapon systems broadly as: “Any weapon system with autonomy in its critical functions. That is, a weapon system that can select and attack targets without human intervention.” After initial activation by a human operator, the weapon system – though its sensors, software (programming / algorithms) and connected weapon(s) – takes on the targeting functions that would normally be controlled by humans. • Autonomy in these “critical functions” of selecting and attacking targets is central to humanitarian, legal, and ethical considerations within the scope of the Convention on Certain Conventional Weapons (CCW). It is these functions: that result in injury, damage and destruction to persons or objects in armed conflict; that are governed by international humanitarian law (IHL) rules on the conduct of hostilities; and that raise ethical questions about the role of humans in life and death decisions. • The key distinction, in our view, from non-­‐autonomous weapons is that the machine self-­‐initiates an attack. • With reference to the earlier discussion today of military applications, it may be useful to distinguish between: o Weapon systems where the human choses the specific target at a particular point in time and at a specific location, for example missiles and other munitions with guidance functions; and o (Autonomous) weapon systems, where the exact timing, location and/or nature of the attack not known to the user since it is self-­‐initiated by the weapon, which is – in-­‐turn – triggered by its environment. • As regards the latter, autonomy in critical functions is a feature that could be applied to any weapon system, especially robotic weapon systems (e.g. in the air, on land or at sea), and it is already found to a limited extent in some existing weapons, such as air defence systems, “active protection” systems and some loitering weapons. • In the ICRC’s understanding, notions of “automated” and “autonomous” weapon systems are not easily distinguishable from a technical perspective. But more importantly, they not easily distinguishable from a humanitarian, legal and ethical perspective. • In the ICRC’s view, the core issue is ensuring meaningful/effective/sufficient/appropriate human control over decisions to select and attack targets, independent of the technical sophistication of the weapon system. Agenda Item 5 (d): MILITARY APPLICATIONS. Lessons for human control from autonomy in existing weapon systems • Linking characterisation to discussions this morning on military applications, experiences with existing weapons can highlight: o On one hand, the potential humanitarian consequences, legal questions and ethical concerns introduced by autonomy in critical functions and; o On the other hand, the practical lessons for human control measures that need to be taken to mitigate these risks, and to ensure compliance with IHL, and ethical acceptability. ICRC statement, CCW GGE “LAWS”, Monday 25 March 2019 CHECK AGAINST DELIVERY 2 • The ICRC explored these issues in an international expert meeting it convened with a group of 20 States in 2016, and also proposed some elements of human control in its working paper to the CCW Meeting of High Contracting Parties in November 2018. • In terms of lessons for human control measures, for example with existing air defence weapons, which have autonomous modes, States using such systems have highlighted the need for human control in three key aspects: 1) Human supervision, and ability to intervene and deactivate From what the ICRC understands, and from explanations today and in previous GGE sessions with presentations by military experts, these systems are under constant supervision by a human operator with the ability – through a physical and/or communication link – to intervene and deactivate the system at any time. o As we understand, many such systems retain the ability, even with incoming projectiles, for a human operator to visually verify the projectile on screen and decide to cancel the attack if necessary. 2) Predictability and reliability It would have to be assumed that existing systems have been tested with and meet high standards of predictability and reliability for their intended use, including the intended environment of use. In that sense, it would be beneficial for States to share the standards they use in making such assessments of predictability and reliability, including as part of national legal reviews. 3) Operational constraints Operational constraints are clearly very important for human control, for increasing predictability in the consequences of use, and for compliance with IHL, in particular: • Limits on the task. These systems are used for a single, relatively simple, task to defend against incoming projectiles. • Limits on the targets. These systems are only used against materiel targets, such as projectiles and military aircraft, vehicles and drones. They are not used to target humans (personnel) directly. • Limits on the environment. These systems are only used in highly constrained and relatively simple environments, where there are few or no civilians or civilian objects present. As the ICRC understands, specific measures are also taken to monitor the environment and ensure civilian objects to not enter the area. • Time frame of autonomous operation. In these systems, the autonomous mode is only activated for short periods, and can be switched back to manual mode at any time. • Scope of movement over an area. These systems are fixed in place, at a perimeter, or on a ship or armoured vehicle, thereby limiting their range and effects. • Other existing weapons with automated or autonomous critical functions illustrate some of the difficulties of ensuring human control and ensuring compliance with IHL. In particular, where they are unsupervised, unpredictable, or insufficiently constrained in time and space. • For example, the use of a loitering weapon with autonomy in critical functions creates unpredictability for the user: o Due to the long time scale of autonomous operation (up to several hours) and large area of operation (up to hundreds of square kilometres), there is a high level of unpredictability ICRC statement, CCW GGE “LAWS”, Monday 25 March 2019 CHECK AGAINST DELIVERY 3 for the user as to the timing and location of the subsequent attack, which raises questions for IHL compliance. o This unpredictability is even greater where the system is not supervised by the human operator with the ability to intervene and deactivate, should the attack need to be cancelled during that period. o ICRC would like to hear more from military experts familiar with the operation of these types of systems. • Another example of a much older and simpler technology, where unpredictability and lack of human control has raised concerns, is the mine: o The consequences of use of anti-­‐personnel landmines starkly illustrate the serious dangers to civilians from weapons, which are triggered by their environment, and where the human operator has insufficient knowledge – at the point of use or activation – about the subsequent timing, (location) and nature of the attack that may result. Recognition of the resulting indiscriminate effects on civilians led to the prohibition of anti-­‐personnel landmines by the majority of States. Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 3–13 August 2021, Geneva Statement of the International Committee of the Red Cross The International Committee of the Red Cross (ICRC) welcomes the resumption of work by the Group of Governmental Experts (GGE) at this critical moment in multilateral deliberations on autonomous weapon systems, and with less than five months to go until the Review Conference of the Convention. The ICRC appreciates the efforts of the chair of the GGE, Ambassador Marc Pecsteen de Buytswerve of Belgium, to solicit proposals for consensus recommendations of the Review Conference on the “normative and operational framework”. It is the ICRC’s view that an urgent and effective international response is needed to address the serious risks posed by autonomous weapon systems, as highlighted by many states and civil society organizations over the past decade. These risks stem from the process by which autonomous weapon systems function. It is the ICRC’s understanding that these weapons, after initial activation, select and apply force to targets without human intervention, in the sense that they are triggered by their environment based on a “target profile”, which serves as a generalized approximation of a type of target. The user of an autonomous weapon system does not choose the specific target, nor the precise time or place that force is applied. This process risks the loss of human control over the use of force and it is the source of the humanitarian, legal, and ethical concerns. These concerns are significant when one considers that autonomy in the critical functions of selecting and applying force could be integrated into any weapon system. The central challenge with autonomous weapon systems resides in the difficulty of anticipating and limiting their effects. From a humanitarian perspective, they risk harming those affected by armed conflict, both civilians and combatants hors de combat, and they increase the risk of conflict escalation. From a legal perspective, they challenge the ability of persons who must apply the rules of international humanitarian law (IHL) during the planning, decision and execution of attacks to comply with their obligations. From an ethical perspective, this process of functioning risks effectively substituting human decisions about life and death with sensor, software and machine processes. This raises ethical concerns that are especially acute when autonomous weapon systems are used to target persons directly. Today, autonomous weapon systems are highly constrained in their use; they are mostly used against certain types of military objects, for limited periods of time, in restricted areas where civilians are not present and with close human supervision. However, current trends in the expanded development and use of autonomous weapon systems exacerbate core concerns dramatically. In particular, there is military interest in their use against a wider range of targets, over larger areas and for longer periods of time, in urban areas where civilians would be most at risk and with reduced human supervision and capacity for intervention and deactivation. 2 Worryingly, the use of artificial intelligence and machine learning software to control the critical functions of selecting and applying force is being increasingly explored, which would exacerbate the already difficult task that users have in anticipating and limiting the effects of an autonomous weapon system. Against this background, the High Contracting Parties to the CCW have a responsibility and an opportunity to make progress in clarifying, considering and developing the normative and operational framework for autonomous weapon systems. Given this opportunity, the ICRC offered recommendations to all states on 12 May 2021, which were also submitted to the chair of the GGE on 11 June 2021. The ICRC recommends that states adopt new, legally binding rules to regulate autonomous weapon systems to ensure that sufficient human control and judgement is retained in the use of force. It is the ICRC’s view that this will require prohibiting certain types of autonomous weapon systems and strictly regulating all others. First, unpredictable autonomous weapon systems should be expressly ruled out, notably because of their indiscriminate effects. This would best be achieved with a prohibition on autonomous weapon systems that are designed or used in a manner such that their effects cannot be sufficiently understood, predicted and explained. Secondly, the use of autonomous weapon systems to target human beings should be ruled out. This would best be achieved through a prohibition on autonomous weapon systems that are designed or used to apply force against persons directly as opposed to against objects. Thirdly, the design and use of non-prohibited autonomous weapon systems should be regulated, including through a combination of limits on the types of target, such as constraining them to objects that are military objectives by nature; limits on the duration, geographical scope and scale of use, including to enable human judgement and control in relation to a specific attack; limits on situations of use, such as constraining them to situations where civilians or civilian objects are not present; and imposing a requirement for human–machine interaction, notably to ensure effective human supervision and timely intervention and deactivation. It is the ICRC’s understanding that these proposed prohibitions and restrictions are in line with current military practice in the use of autonomous weapon systems. While these recommendations offer clear, principled and pragmatic guidance on where to draw the boundaries of what is acceptable in light of humanitarian, legal and ethical concerns, they are not ready-made treaty text and the finer details of these limits will require further elaboration by states. It is encouraging, therefore, that there is an increasing convergence of views among states that certain autonomous weapon systems should be prohibited or otherwise excluded from development and use, and that others should be regulated or otherwise limited in their development and use. These proposals reflect widely held views: a recognition of the need to ensure human control and judgement in the use of force; an acknowledgement that ensuring such control and judgement requires effective limits on the design and use of autonomous weapon systems; and an increasing confidence that such limits can be articulated at international level. 3 It is also encouraging that many states have shown a readiness to clarify how IHL already constrains autonomous weapon systems and some have proposed the further sharing of current military practice. The ICRC is convinced that international limits on autonomous weapon systems must take the form of new, legally binding rules. New rules are required because of the seriousness of the risks, the necessity to clarify how existing IHL rules apply and the need to develop and strengthen the legal framework in line with ethical and rule of law issues and humanitarian considerations. International law must continue to evolve in order to uphold and strengthen protections in the face of evolving military technology and practice. The High Contracting Parties to the CCW, which is a framework Convention anchored in IHL, acknowledge the “need to continue the codification and progressive development of the rules of international law applicable in armed conflict”, as they have affirmed in the Convention’s preamble. Considering current military developments in how autonomous weapon systems are being used and deployed, the ICRC urges the High Contracting Parties to the CCW to take action now towards the adoption of new rules. Building on the extensive and in-depth work of the CCW and this GGE over the past eight years, there is an opportunity to shape an international response that will effectively strengthen protections for those affected by armed conflict, uphold the legal obligations and moral responsibilities of persons conducting conflict and safeguard our shared humanity. It is an opportunity that the High Contracting Parties must seize and focus on in their work during this meeting.Contribution by the International Committee of the Red Cross submitted to the Chair of the Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Emerging Technologies in the Area of Lethal Autonomous Weapons Systems as a proposal for consensus recommendations in relation to the clarification, consideration and development of aspects of the normative and operational framework 11 June 2021 The ICRC's concerns about autonomous weapon systems Autonomous weapon systems select and apply force to targets without human intervention. After initial activation or launch by a person, an autonomous weapon system self-initiates or triggers a strike in response to information from the environment received through sensors and on the basis of a generalized "target profile". This means that the user does not choose, or even know, the specific target(s) and the precise timing and/or location of the resulting application(s) of force. The use of autonomous weapon systems entails risks due to the difficulties in anticipating and limiting their effects. This loss of human control and judgement in the use of force and weapons raises serious concerns from humanitarian, legal and ethical perspectives. The process by which autonomous weapon systems function: • brings risks of harm for those affected by armed conflict, both civilians and combatants, as well as dangers of conflict escalation • raises challenges for compliance with international law, including international humanitarian law, notably, the rules on the conduct of hostilities for the protection of civilians • raises fundamental ethical concerns for humanity, in effect substituting human decisions about life and death with sensor, software and machine processes. The ICRC's recommendations to States for the regulation of autonomous weapon systems The International Committee of the Red Cross (ICRC) has, since 2015, urged States to establish internationally agreed limits on autonomous weapon systems to ensure civilian protection, compliance with international humanitarian law, and ethical acceptability. With a view to supporting current efforts to establish international limits on autonomous weapon systems that address the risks they raise, including efforts by the Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Emerging Technologies in the Area of Lethal Autonomous Weapons Systems to develop consensus recommendations in relation to the clarification, consideration and development of aspects of the normative and operational framework, the ICRC recommends that States adopt new legally binding rules.1 In particular: 1. Unpredictable autonomous weapon systems should be expressly ruled out, notably because of their indiscriminate effects. This would best be achieved with a prohibition on autonomous weapon systems that are designed or used in a manner such that their effects cannot be sufficiently understood, predicted and explained. 2. In light of ethical considerations to safeguard humanity, and to uphold international humanitarian law rules for the protection of civilians and combatants hors de combat, use of autonomous weapon systems to target human beings should be ruled out. This would best be achieved through a prohibition on autonomous weapon systems that are designed or used to apply force against persons. 1 ICRC, ICRC Position on Autonomous Weapon Systems and Background Paper, Geneva, 12 May 2021, https://www.icrc.org/en/document/icrc-position-autonomous-weapon-systems. 2 3. In order to protect civilians and civilian objects, uphold the rules of international humanitarian law and safeguard humanity, the design and use of autonomous weapon systems that would not be prohibited should be regulated, including through a combination of: • limits on the types of target, such as constraining them to objects that are military objectives by nature • limits on the duration, geographical scope and scale of use, including to enable human judgement and control in relation to a specific attack • limits on situations of use, such as constraining them to situations where civilians or civilian objects are not present • requirements for human–machine interaction, notably to ensure effective human supervision, and timely intervention and deactivation. The ICRC’s position and its recommendations to States are based on its analyses of associated humanitarian, legal, ethical, technical and military implications of autonomous weapon systems, insights published in a series of reports, and regular engagement with States and experts at the CCW and bilaterally.2 These inform the ICRC’s recommendations on the specific limits on autonomous weapon systems that are needed to ensure civilian protection, compliance with international humanitarian law and ethical acceptability. The normative limits put forward by the ICRC are informed by views expressed by many High Contracting Parties to the CCW, and other stakeholders, on the types of measures that can contribute to ensuring human control, involvement or judgement, and on the need for new legally binding rules on autonomous weapon systems. More specifically, a number of High Contracting Parties, and other stakeholders, support the prohibition of certain autonomous weapon systems and the placement of constraints or requirements on other autonomous weapon systems. The ICRC is convinced that these limits should take the form of new legally binding rules that specifically regulate autonomous weapon systems. These rules should clarify how existing rules of international law, including international humanitarian law, constrain the design and use of autonomous weapon systems, and supplement the legal framework where needed, including to address wider humanitarian risks and fundamental ethical concerns raised by autonomous weapon systems. Considering the speed of development in autonomous weapon systems’ technology and use, it is critical that internationally agreed limits be established in a timely manner. Beyond new legal rules, these limits may also include common policy standards and good practice guidance, which can be complementary and mutually reinforcing. To this end, and within the scope of its mandate and expertise, the ICRC stands ready to work in collaboration with States and other stakeholders at international and national levels, including representatives of High Contracting Parties to the CCW and their armed forces, the scientific and technical community, industry and civil society. 2 Most recently: ICRC, Statement of the ICRC to Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems, Geneva, 21–25 September 2020; ICRC, Commentary on the “Guiding Principles” of the CCW GGE on “Lethal Autonomous Weapons Systems”, Geneva, July 2020; and V. Boulanin, N. Davison, N. Goussac, and M. Peldán Carlsson, Limits on Autonomy in Weapon Systems: Identifying Practical Elements of Human Control, ICRC & SIPRI, June 2020. CHECK AGAINST DELIVERY Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 9–13 April 2018, Geneva Statement of the International Committee of the Red Cross (ICRC) Further consideration of the human element in the use of lethal force;; aspects of human-­ machine interaction in the development, deployment and use of emerging technologies in the area of lethal autonomous weapons systems (Additional remarks) Mr Chair Thank you for the opportunity to add further detail to the statement we made on this topic. Legal basis of human control The ICRC is clear that the law is addressed to humans, and the legal obligations under international humanitarian law rest with combatants who plan, decide upon, and carry out attacks. Machines can never “apply” international humanitarian law, and responsibility and accountability for decisions to use force cannot be transferred to machines, computer programs or weapon systems. Combatants will require a minimum level of human control over weapon systems with autonomy in their critical functions so that they can effectively make legal judgements – of distinction, proportionality and precautions – in specific attacks. The requirement for human control will mean limits to lawful levels of autonomy under international humanitarian law. Legal reviews Mr Chair The ICRC welcomes attention that this issue has brought to developing and improving legal review processes. The ICRC calls on all States to establish a weapon review mechanism or consider strengthening existing mechanisms to ensure that any new weapons that are developed and acquired can comply with IHL in all foreseeable circumstances of their use. When developing or acquiring new weapons, States must determine whether the employment of the new weapon, means or method of warfare would, in some or all circumstances, be prohibited by international law. 2 This is an obligation under article 36 of the First Additional Protocol to the 1949 Geneva Conventions. But, in fact, all States have an interest in assessing the legality of new weapons, whether or not they are party to Additional Protocol I. Such assessments contribute to ensuring that the State’s armed forces can conduct hostilities in accordance with that State’s international obligations, and flows from the obligation to ensure respect for IHL under common article 1 to the Geneva Conventions. A legal review must consider treaty and customary prohibitions and restrictions on specific weapons, as well as the general IHL rules applicable to all weapons, means and methods of warfare. These include the rules aimed at protecting civilians from the indiscriminate effects of weapons and combatants from superfluous injury and unnecessary suffering. Legal reviews can help to improve compliance with IHL for all weapons, in light of rapid technological developments. But legal reviews of AWS are also likely to raise their own challenges. As with all weapons, the lawfulness of a weapon with autonomy in its critical functions depends on its specific characteristics and intended use. The ability to carry out such a review entails fully understanding the weapon’s capabilities and foreseeing its effects, notably through verification and testing. Since the commander or operator must make an assessment of the lawfulness of an attack using an autonomous weapon system at an earlier stage than if the selection and attack of targets were under direct human control, the legal review must demand a very high level of confidence that, once activated, the autonomous weapon system would predictably and reliably operate as intended. This raises unique challenges in ensuring that predictability and reliability are tested and verified for all foreseeable scenarios of use. Predictability is the ability to “say or estimate that (a specified thing) will happen in the future or will be a consequence of something”. Applied to an autonomous weapon system, predictability is knowledge of how it will function in any given circumstances of use, and the effects that will result. Reliability is “the quality of being trustworthy or performing consistently well”. In this context, reliability is knowledge of how consistently the machine will function as intended—e.g., without failures or unintended effects. Foreseeing such effects may become increasingly difficult if autonomous weapon systems were to become more complex or to be given more freedom of action in their operations, and therefore become less predictable. The ICRC continues to urge all States in this meeting to elaborate what “meaningful” or “effective” human control entails in practice. States must also address fundamental concerns about weapon systems that may introduce inherent unpredictability, such as those employing artificial intelligence (AI) machine-­ˇlearning algorithms. Thank you 3CHECK AGAINST DELIVERY Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts on Lethal Autonomous Weapons Systems 9–13 April 2018, Geneva Statement of the International Committee of the Red Cross (ICRC) Further consideration of the human element in the use of lethal force; aspects of humanmachine interaction in the development, deployment and use of emerging technologies in the area of lethal autonomous weapons systems Mr Chair The ICRC advocates a human-centred approach, focused on the obligations and responsibilities of humans in the use of weapons with autonomy in their critical functions. The ICRC’s view is that a minimum level of human control is required to ensure · compliance with international humanitarian law rules that protect civilians and combatants in armed conflict, and · ethical acceptability in terms of the principles of humanity and the public conscience. Legal basis for human control The ICRC is clear that the law is addressed to States and humans, and the legal obligations under international humanitarian law ultimately rest with combatants who plan, decide upon, and carry out attacks.1 Combatants will require a minimum level of human control over weapon systems with autonomy in their critical functions so that they can effectively make legal judgements - of distinction, proportionality and precautions – in specific attacks. Human control can take different forms during the development, activation, and operation of an autonomous weapon system.2 However, these legal judgements are context specific. Therefore, concerns will arise where the design and/or use of the weapon interferes with combatants’ ability to make the necessary legal judgements in carrying out attacks. For example, if an autonomous weapon system is activated over a certain area and/or duration, and the commander/operator who authorises the launch does not know where and when the subsequent attack will take place, the question arises: how can they effectively ensure distinction, judge proportionality, or take precautions should the circumstances on the ground change? 1 Machines can never “apply” international humanitarian law, and responsibility and accountability for decisions to use force cannot be transferred to machines, computer programs or weapon systems. 2 First, the development and testing of the weapon system (‘development stage’); second, the decision by the commander, or operator, to activate the weapon system (‘activation stage’), and third, the operation of the autonomous weapon system during which it independently selects and attacks targets (‘operation stage’). 2 Ethical basis for human control In the ICRC’s view, ethical considerations also demand a minimum level of human control. From an ethical viewpoint, “meaningful” human control would be the type and degree of control that preserves human agency and upholds moral responsibility in decisions to use force. This requires a sufficiently direct and close connection to be maintained between the human intent of the user and the eventual consequences of the operation of the weapon system in a specific attack. Ethical and legal considerations may actually demand some similar constraints to ensure human control is maintained. However, ethical concerns about loss of human agency in decisions to use force, diffusion of moral responsibility and loss of human dignity could have more far reaching implications – potentially preventing the development and use of anti-personnel autonomous weapon systems, and even limiting the applications of anti-materiel systems, depending on the risks for human life. Meaningful human control in practice In the ICRC’s view, the GGE should work to determine – for the broad category of weapons with autonomy in their critical functions – what does “meaningful”, “effective” or “appropriate” human control mean in practice? What type and degree of human control is required? Key elements of human control include · predictability and reliability · human supervision, and the ability to intervene and deactivate, and · operational constraints. For each of these elements there are lessons to be drawn from human control in existing weapons with autonomy in their critical functions. Whether an autonomous weapon system will operate within the constraints of international law after it has been activated will depend on the predictability and reliability3 of · the weapon system itself (including the algorithms used) · the environment of its use, and · the interaction between the two. The level of human supervision, and the ability to intervene and deactivate is also important. Without human supervision is it hard to see how operators/commanders can take into account changes in the situation, and ensure predictability in the effects from the use of the weapon. In many circumstances, constant human supervision of the weapon system and the target area may be required, so that the operator has sufficient information and understanding 3 Predictability is the ability to “say or estimate that (a specified thing) will happen in the future or will be a consequence of something”. Applied to an autonomous weapon system, predictability is knowledge of how it will function in the circumstances of use, and the effects that will result. Reliability is “the quality of being trustworthy or performing consistently well”. In this context, reliability is knowledge of how consistently the machine will function as intended, e.g. without failures or unintended effects. 3 about the operation of the weapon system, the environment of use, and the interaction of the two, over time and in space. Communication links that permit adjustment of the engagement criteria and the ability to cancel the attack, as well as sufficient time for such intervention, may also be necessary. This type of human supervision and potential for deactivation is seen in existing weapons with autonomy in their critical functions.4 Finally, other operational parameters and constraints are also important for human control, and for legal and ethical assessments, including: · the task the weapon is used for – is it used to defend against incoming missile or to search for varied targets over a wide area? · the types of targets it attacks – is it used to attack an incoming rocket or to attack humans directly? · the type of force and munitions it employs (and associated effects); · the operating environment – is it used in an “uncluttered” environment at sea without civilians present or use in a populated area on land? · the duration of autonomous operation (time-frame) – is the autonomous mode activated for a short period or for several hours “loiter” time? · the scope of movement over an area (mobility) – is the weapon stationary (fixed in place) with constrained movement and range or mobile over a wide area? Further work by the GGE on these elements will help determine what “meaningful”, “effective” or “appropriate” human control means in practice, and where limits on autonomy must be placed. Weapon systems falling outside those limits will then be more easily identified as unlawful and/or ethically unacceptable autonomous weapon systems. Thank you For further analysis by the ICRC on the legal and ethical basis for human control please see the following publications: Autonomous weapon systems under international humanitarian law, November 2017: https://www.icrc.org/en/document/autonomous-weapon-systems-under-internationalhumanitarian- law Ethics and autonomous weapon systems: An ethical basis for human control? April 2018: https://www.icrc.org/en/document/ethics-and-autonomous-weapon-systems-ethical-basis human-control 4 For example, existing counter-rocket, artillery and mortar weapons retain the ability, even with incoming projectiles, for a human operator to visually verify the projectile on screen and decide to cancel the attack if necessary. 1 CCW meeting of experts on autonomous weapon systems Session on technical issues, 14 May 2014 Statement by the ICRC 1. On the scope of discussions about autonomous weapon systems General scope • As we have heard from technical experts, and as the ICRC discovered in preparations for its own expert meeting, there is no clear line between automated and autonomous weapon systems. • Rather than search for an unclear line, it may be more useful to focus on the critical functions of weapon systems (i.e. the process of target acquisition, tracking, selection, and attack). • Where these functions are carried out independently by the machine then the weapon system should be within the scope of our discussions at the CCW. Complexity • We also heard from some technical experts that a weapon system that independently selects and attacks targets does not necessarily have to be highly complex. • It could be quite simple in its design but highly autonomous in its functioning. • To explain in another way; after you turn on the weapon– or activate it – the weapon system itself choses its specific target and then to attack it. And it does this without human control or intervention. Human control • This brings us back to the central issue of human control, which ICRC highlighted in its opening statement. • There has been much reference to the concept of ‘meaningful human control’ during discussions. • One way to get a better understanding of this concept is to examine current weapons that have autonomy in ‘critical functions’ to see how meaningful human control is understood and considered to be implemented in practice today. Substituting humans with machines • There have been different views regarding whether machines would ever be ‘taking decisions’ on selecting and attacking targets, or whether they would always be ‘following the instructions’ of humans in some form. • However, it seems that at a certain point – where humans are so far removed in time and space from the process of collecting, analysing, and acting on the information gathered by the weapon system – we are in effect substituting the human decision-making process with machine decision-making process on the use of force. • In such a situation the user of the weapon system would be placing machines in control of the ‘critical functions’ while relying on the machine to have sufficient advance information to make the correct decisions. 2 2. On discussions about the “lethality” of weapons • In the view of the ICRC, which is quite well known on this issue, it is not useful to discuss weapons in terms of their “lethality” or “non-lethality”. • Lethality is not an inherent property of a weapon but depends on the weapon and the context of its use. • That context includes how the weapon is used in practice and the vulnerability of the victim(s). • For example, it is a misconception that conventional weapons are 100% “lethal” (e.g. data since the Second World War shows that around 25% of those wounded on the battlefield by conventional weapons die from their injuries). • Conversely, some weapons described as “non-lethal” have been used in a way that causes comparable outcomes in terms of fatalities (e.g. due to the weapons’ characteristics and context of use; or their use in combination with conventional weapons). • For a simple illustration of why it does not make sense to talk about “lethality” take this example: A rifle fired above a person’s head as a warning shot has a non-fatal outcome, whereas a plastic bullet fired directly at a person’s head can easily kill. • We should also remember, especially given that we are in a CCW meeting, that some weapons promoted at the time of their development as “non-lethal” were eventually banned in 1995 on the grounds that the injuries produced constituted "superfluous injury or unnecessary suffering". Here am referring of course to blinding laser weapons. • In sum, in discussions about autonomous weapon systems we should be referring to the use of weapons and the use of force.
